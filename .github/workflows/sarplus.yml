# This workflow will run tests and do packaging for contrib/sarplus.
#
# Refenreces:
#   * [GitHub Actions doc](https://docs.github.com/en/actions)
#   * GitHub Actions workflow templates
#     + [python package](https://github.com/actions/starter-workflows/blob/main/ci/python-package.yml)
#     + [python publish](https://github.com/actions/starter-workflows/blob/main/ci/python-publish.yml)
#     + [scala](https://github.com/actions/starter-workflows/blob/main/ci/scala.yml)
#   * [GitHub hosted runner - Ubuntu 20.04 LTS](https://github.com/actions/virtual-environments/blob/main/images/linux/Ubuntu2004-README.md)
#   * [Azure Databirkcs runtime releases](https://docs.microsoft.com/en-us/azure/databricks/release-notes/runtime/releases)

name: sarplus package

on:
  push:
    paths:
      - contrib/sarplus/python/**
      - contrib/sarplus/scala/**
      - .github/workflows/sarplus.yml

env:
  PYTHON_ROOT: ${{ github.workspace }}/contrib/sarplus/python
  SCALA_ROOT: ${{ github.workspace }}/contrib/sarplus/scala
  SARPLUS_VERSION: 0.5.0

jobs:
  python:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.8", "3.9", "3.10"]
    steps:
      - uses: actions/checkout@v2

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v2
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          python -m pip install flake8 pybind11 pytest pytest-cov scikit-learn wheel

      - name: Lint with flake8
        run: |
          cd "${PYTHON_ROOT}"
          # See https://flake8.pycqa.org/en/latest/user/index.html
          flake8 .

      - name: Package
        run: |
          cd "${PYTHON_ROOT}"
          VERSION="${SARPLUS_VERSION}" python setup.py bdist_wheel

      - name: Testing
        env:
          ACCESS_TOKEN: ${{ secrets.SARPLUS_TESTDATA_ACCESS_TOKEN }}
        run: |
          cd "${PYTHON_ROOT}"
          python -m pip install --use-feature=2020-resolver dist/*.whl

          cd "${SCALA_ROOT}"
          export SPARK_VERSION=$(python -m pip show pyspark | grep -i version | cut -d ' ' -f 2)
          SPARK_JAR_DIR=$(python -m pip show pyspark | grep -i location | cut -d ' ' -f2)/pyspark/jars
          SCALA_JAR=$(ls ${SPARK_JAR_DIR}/scala-library*)
          HADOOP_JAR=$(ls ${SPARK_JAR_DIR}/hadoop-client-api*)
          SCALA_VERSION=${SCALA_JAR##*-}
          export SCALA_VERSION=${SCALA_VERSION%.*}
          HADOOP_VERSION=${HADOOP_JAR##*-}
          export HADOOP_VERSION=${HADOOP_VERSION%.*}
          export VERSION="${SARPLUS_VERSION}"
          sbt ++"${SCALA_VERSION}"! package

          cd "${PYTHON_ROOT}"
          pytest --token "${ACCESS_TOKEN}" ./tests

      - name: Upload Python package
        uses: actions/upload-artifact@v2
        with:
          name: pysarplus-${{ env.SARPLUS_VERSION }}-cp${{ matrix.python-version }}
          path: ${{ env.PYTHON_ROOT }}/dist/pysarplus-*.whl
#       - name: Publish Python package
#         if: github.ref == 'refs/heads/main'
#         run: 

  scala:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        include:
          - scala-version: "2.12.10"
            spark-version: "3.0.1"
            hadoop-version: "2.7.4"
            databricks-runtime: "ADB 7.3 LTS"

          - scala-version: "2.12.10"
            spark-version: "3.1.2"
            hadoop-version: "2.7.4"
            databricks-runtime: "ADB 9.1 LTS"

          - scala-version: "2.12.14"
            spark-version: "3.2.0"
            hadoop-version: "3.3.1"
            databricks-runtime: "ADB 10.0"

    steps:
      - uses: actions/checkout@v2

      # TODO: Add testing

      - name: Package
        run: |
          cd "${SCALA_ROOT}"
          export VERSION="${SARPLUS_VERSION}"
          export SPARK_VERSION="${{ matrix.spark-version }}"
          export HADOOP_VERSION="${{ matrix.hadoop-version }}"
          sbt ++${{ matrix.scala-version }}! package
          SCALA_VERSION=${{ matrix.scala-version }}
          echo "scala_binary_version=${SCALA_VERSION%.*}" >> $GITHUB_ENV
      - name: Upload Scala package
        uses: actions/upload-artifact@v2
        with:
          name: scala_${{ matrix.scala-version }}_s${{ matrix.spark-version }}_h${{ matrix.hadoop-version }}-${{ env.SARPLUS_VERSION }} (${{ matrix.databricks-runtime }})
          path: ${{ env.SCALA_ROOT }}/target/scala-${{ env.scala_binary_version }}/*.jar
#       - name: Publish Scala package
#         if: github.ref == 'refs/heads/main'
#         run: 
