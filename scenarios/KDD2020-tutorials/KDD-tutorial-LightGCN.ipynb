{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import os\n",
    "import papermill as pm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from reco_utils.common.timer import Timer\n",
    "from reco_utils.recommender.deeprec.models.graphrec.lightgcn import LightGCN\n",
    "from reco_utils.recommender.deeprec.DataModel.ImplicitCF import ImplicitCF\n",
    "from reco_utils.dataset import movielens\n",
    "from reco_utils.dataset.python_splitters import python_stratified_split\n",
    "from reco_utils.evaluation.python_evaluation import map_at_k, ndcg_at_k, precision_at_k, recall_at_k\n",
    "from reco_utils.common.constants import SEED as DEFAULT_SEED\n",
    "from reco_utils.recommender.deeprec.deeprec_utils import prepare_hparams\n",
    "from reco_utils.recommender.deeprec.deeprec_utils import cal_metric\n",
    "from utils.general import *\n",
    "from utils.data_helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightgcn_dir = 'data_folder/my/LightGCN-training-folder'\n",
    "rawdata_dir = 'data_folder/my/DKN-training-folder'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_instance_file(\n",
    "        filename,\n",
    "        target_triples,\n",
    "        label=None\n",
    "    ):\n",
    "    print('load_instance_file: {0}  '.format(os.path.basename(filename)), end=' ')\n",
    "    user_hist_keys = set()\n",
    "    with open(filename, 'r') as rd:\n",
    "        while True:\n",
    "            line = rd.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            words = line.strip().split('%')\n",
    "            tokens = words[0].split(' ')\n",
    "            if label:\n",
    "                target_triples.append((words[1], tokens[2], label))  # (userid, itemid, label)\n",
    "            else:\n",
    "                target_triples.append((words[1], tokens[2], tokens[0]))  #(userid, itemid, label)\n",
    "            user_hist_keys.add(tokens[1])\n",
    "    print('done.')\n",
    "    return user_hist_keys\n",
    "\n",
    "def write_to_file(filename, triples):\n",
    "    with open(filename, 'w') as wt:\n",
    "        for t in triples:\n",
    "            wt.write('{0} {1} {2}\\n'.format(t[0], t[1], t[2]))\n",
    "\n",
    "def load_user_behaviors(\n",
    "        user_behavior_file,\n",
    "        train_triples,\n",
    "        user_behavior_keys=None\n",
    "    ):\n",
    "    with open(user_behavior_file, 'r') as rd:\n",
    "        while True:\n",
    "            line = rd.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            words = line.strip().split(' ')\n",
    "            if user_behavior_keys and not words[0] in user_behavior_keys:\n",
    "                continue\n",
    "            userid = words[0].split('_')[0]\n",
    "            items = words[1].split(',')\n",
    "            for item in items:\n",
    "                train_triples.append((userid, item, '1'))\n",
    "\n",
    "\n",
    "def prepare_dataset(output_folder, input_folder, tag):\n",
    "    train_triples, valid_triples = [], []\n",
    "\n",
    "    training_user_hist_keys = load_instance_file(\n",
    "        os.path.join(input_folder, 'train_{0}.txt'.format(tag)),\n",
    "        train_triples\n",
    "    )\n",
    "    load_instance_file(\n",
    "        os.path.join(input_folder, 'valid_{0}.txt'.format(tag)),\n",
    "        valid_triples\n",
    "    )\n",
    "    load_instance_file(\n",
    "        os.path.join(input_folder, 'test_{0}.txt'.format(tag)),\n",
    "        valid_triples,\n",
    "        label='0'\n",
    "    )\n",
    "\n",
    "    load_user_behaviors(\n",
    "        os.path.join(input_folder, 'user_history_{0}.txt'.format(tag)),\n",
    "        train_triples,\n",
    "        training_user_hist_keys\n",
    "    )\n",
    "\n",
    "    write_to_file(os.path.join(output_folder, 'lightgcn_train_{0}.txt'.format(tag)), train_triples)\n",
    "    write_to_file(os.path.join(output_folder, 'lightgcn_valid_{0}.txt'.format(tag)), valid_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_instance_file: train_small.txt   done.\n",
      "load_instance_file: valid_small.txt   done.\n",
      "load_instance_file: test_small.txt   done.\n"
     ]
    }
   ],
   "source": [
    "tag = 'small'\n",
    "create_dir(lightgcn_dir)\n",
    "prepare_dataset(lightgcn_dir, rawdata_dir, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\n",
    "        os.path.join(lightgcn_dir, 'lightgcn_train_{0}.txt'.format(tag)),\n",
    "        sep=' ',\n",
    "        engine=\"python\",\n",
    "        names=['userID', 'itemID', 'rating'],\n",
    "        header=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2031122664</td>\n",
       "      <td>1966238900</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2031122664</td>\n",
       "      <td>2143004298</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2031122664</td>\n",
       "      <td>2126707939</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2031122664</td>\n",
       "      <td>1976643246</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2100638239</td>\n",
       "      <td>2128200839</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       userID      itemID  rating\n",
       "0  2031122664  1966238900       0\n",
       "1  2031122664  2143004298       0\n",
       "2  2031122664  2126707939       0\n",
       "3  2031122664  1976643246       0\n",
       "4  2100638239  2128200839       1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = pd.read_csv(\n",
    "        os.path.join(lightgcn_dir, 'lightgcn_valid_{0}.txt'.format(tag)),\n",
    "        sep=' ',\n",
    "        engine=\"python\",\n",
    "        names=['userID', 'itemID', 'rating'],\n",
    "        header=0\n",
    "    )\n",
    "# df_test = pd.read_csv(\n",
    "#         os.path.join(path, 'test.txt'),\n",
    "#         sep=' ',\n",
    "#         engine=\"python\",\n",
    "#         names=['userID', 'itemID', 'rating'],\n",
    "#         header=0\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ImplicitCF(\n",
    "    train=df_train, test=df_valid, seed=0,\n",
    "    col_user='userID',\n",
    "    col_item='itemID',\n",
    "    col_rating='rating'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method HParams.values of HParams([('DNN_FIELD_NUM', None), ('EARLY_STOP', 100), ('FEATURE_COUNT', None), ('FIELD_COUNT', None), ('L', None), ('MODEL_DIR', 'data_folder/my/LightGCN-training-folder/saved_models'), ('PAIR_NUM', None), ('SUMMARIES_DIR', None), ('T', None), ('activation', None), ('att_fcn_layer_sizes', None), ('attention_activation', None), ('attention_dropout', 0.0), ('attention_layer_sizes', None), ('attention_size', None), ('batch_size', 1024), ('cate_embedding_dim', None), ('cate_vocab', None), ('contextEmb_file', None), ('cross_activation', 'identity'), ('cross_l1', 0.0), ('cross_l2', 0.0), ('cross_layer_sizes', None), ('cross_layers', None), ('data_format', None), ('decay', 0.0001), ('dilations', None), ('dim', None), ('doc_size', None), ('dropout', [0.0]), ('dtype', 32), ('embed_l1', 0.0), ('embed_l2', 0.0), ('embed_size', 64), ('embedding_dropout', 0.3), ('enable_BN', False), ('entityEmb_file', None), ('entity_dim', None), ('entity_embedding_method', None), ('entity_size', None), ('epochs', 10), ('eval_epoch', 5), ('fast_CIN_d', 0), ('filter_sizes', None), ('hidden_size', None), ('history_size', None), ('init_method', 'tnormal'), ('init_value', 0.01), ('is_clip_norm', 0), ('item_embedding_dim', None), ('item_vocab', None), ('iterator_type', None), ('kernel_size', None), ('kg_file', None), ('kg_training_interval', 5), ('layer_l1', 0.0), ('layer_l2', 0.0), ('layer_sizes', None), ('learning_rate', 0.005), ('load_model_name', None), ('load_saved_model', False), ('loss', None), ('lr_kg', 0.5), ('lr_rs', 1), ('max_grad_norm', 2), ('max_seq_length', None), ('method', None), ('metrics', ['recall', 'ndcg', 'precision', 'map']), ('min_seq_length', 1), ('model_type', 'lightgcn'), ('mu', None), ('n_h', None), ('n_item', None), ('n_item_attr', None), ('n_layers', 3), ('n_user', None), ('n_user_attr', None), ('n_v', None), ('need_sample', True), ('news_feature_file', None), ('num_filters', None), ('optimizer', 'adam'), ('pairwise_metrics', None), ('reg_kg', 0.0), ('save_epoch', 5), ('save_model', True), ('show_step', 1), ('top_k', 10), ('train_num_ngs', 4), ('train_ratio', None), ('transform', None), ('use_CIN_part', False), ('use_DNN_part', False), ('use_FM_part', False), ('use_Linear_part', False), ('use_context', True), ('use_entity', True), ('user_clicks', None), ('user_dropout', False), ('user_embedding_dim', None), ('user_history_file', None), ('user_vocab', None), ('wordEmb_file', None), ('word_size', None), ('write_tfevents', False)])>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yaml_file = os.path.join(lightgcn_dir, r'../../../../../../lightgcn.yaml')\n",
    "\n",
    "\n",
    "hparams = prepare_hparams(yaml_file,                          \n",
    "                          learning_rate=0.005,\n",
    "                          eval_epoch=5,\n",
    "                          top_k=10,\n",
    "                          save_model=True,\n",
    "                          epochs=10,\n",
    "                          save_epoch=5\n",
    "                         )\n",
    "hparams.MODEL_DIR = os.path.join(lightgcn_dir, 'saved_models')\n",
    "hparams.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method HParams.values of HParams([('DNN_FIELD_NUM', None), ('EARLY_STOP', 100), ('FEATURE_COUNT', None), ('FIELD_COUNT', None), ('L', None), ('MODEL_DIR', 'data_folder/my/LightGCN-training-folder/saved_models'), ('PAIR_NUM', None), ('SUMMARIES_DIR', None), ('T', None), ('activation', None), ('att_fcn_layer_sizes', None), ('attention_activation', None), ('attention_dropout', 0.0), ('attention_layer_sizes', None), ('attention_size', None), ('batch_size', 1024), ('cate_embedding_dim', None), ('cate_vocab', None), ('contextEmb_file', None), ('cross_activation', 'identity'), ('cross_l1', 0.0), ('cross_l2', 0.0), ('cross_layer_sizes', None), ('cross_layers', None), ('data_format', None), ('decay', 0.0001), ('dilations', None), ('dim', None), ('doc_size', None), ('dropout', [0.0]), ('dtype', 32), ('embed_l1', 0.0), ('embed_l2', 0.0), ('embed_size', 64), ('embedding_dropout', 0.3), ('enable_BN', False), ('entityEmb_file', None), ('entity_dim', None), ('entity_embedding_method', None), ('entity_size', None), ('epochs', 10), ('eval_epoch', 5), ('fast_CIN_d', 0), ('filter_sizes', None), ('hidden_size', None), ('history_size', None), ('init_method', 'tnormal'), ('init_value', 0.01), ('is_clip_norm', 0), ('item_embedding_dim', None), ('item_vocab', None), ('iterator_type', None), ('kernel_size', None), ('kg_file', None), ('kg_training_interval', 5), ('layer_l1', 0.0), ('layer_l2', 0.0), ('layer_sizes', None), ('learning_rate', 0.005), ('load_model_name', None), ('load_saved_model', False), ('loss', None), ('lr_kg', 0.5), ('lr_rs', 1), ('max_grad_norm', 2), ('max_seq_length', None), ('method', None), ('metrics', ['recall', 'ndcg', 'precision', 'map']), ('min_seq_length', 1), ('model_type', 'lightgcn'), ('mu', None), ('n_h', None), ('n_item', None), ('n_item_attr', None), ('n_layers', 3), ('n_user', None), ('n_user_attr', None), ('n_v', None), ('need_sample', True), ('news_feature_file', None), ('num_filters', None), ('optimizer', 'adam'), ('pairwise_metrics', None), ('reg_kg', 0.0), ('save_epoch', 5), ('save_model', True), ('show_step', 1), ('top_k', 10), ('train_num_ngs', 4), ('train_ratio', None), ('transform', None), ('use_CIN_part', False), ('use_DNN_part', False), ('use_FM_part', False), ('use_Linear_part', False), ('use_context', True), ('use_entity', True), ('user_clicks', None), ('user_dropout', False), ('user_embedding_dim', None), ('user_history_file', None), ('user_vocab', None), ('wordEmb_file', None), ('word_size', None), ('write_tfevents', False)])>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From ../../reco_utils/recommender/deeprec/models/graphrec/lightgcn.py:37: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n",
      "Already create adjacency matrix.\n",
      "Already normalize adjacency matrix.\n",
      "WARNING:tensorflow:From ../../reco_utils/recommender/deeprec/models/graphrec/lightgcn.py:68: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Using xavier initialization.\n",
      "WARNING:tensorflow:From ../../reco_utils/recommender/deeprec/models/graphrec/lightgcn.py:147: The name tf.sparse_tensor_dense_matmul is deprecated. Please use tf.sparse.sparse_dense_matmul instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../../reco_utils/recommender/deeprec/DataModel/ImplicitCF.py:179: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(rowsum, -0.5).flatten()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From ../../reco_utils/recommender/deeprec/models/graphrec/lightgcn.py:104: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From ../../reco_utils/recommender/deeprec/models/graphrec/lightgcn.py:105: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "WARNING:tensorflow:From ../../reco_utils/recommender/deeprec/models/graphrec/lightgcn.py:107: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.\n",
      "\n",
      "WARNING:tensorflow:From ../../reco_utils/recommender/deeprec/models/graphrec/lightgcn.py:108: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From ../../reco_utils/recommender/deeprec/models/graphrec/lightgcn.py:108: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From ../../reco_utils/recommender/deeprec/models/graphrec/lightgcn.py:109: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LightGCN(hparams, data, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 (train)16.2s: train loss = 0.09128 = (mf)0.09031 + (embed)0.00098\n",
      "Epoch 2 (train)15.1s: train loss = 0.02143 = (mf)0.01958 + (embed)0.00186\n",
      "Epoch 3 (train)15.0s: train loss = 0.01274 = (mf)0.01041 + (embed)0.00234\n",
      "Epoch 4 (train)15.0s: train loss = 0.00936 = (mf)0.00676 + (embed)0.00260\n",
      "Save model to path /data/home/jialia/jialia/kdd2020tutorial/formal/recommenders/scenarios/KDD2020-tutorial/data_folder/my/LightGCN-training-folder/saved_models/epoch_5\n",
      "Epoch 5 (train)15.0s + (eval)1.3s: train loss = 0.00752 = (mf)0.00478 + (embed)0.00274, recall = 0.26885, ndcg = 0.15463, precision = 0.02688, map = 0.11988\n",
      "Epoch 6 (train)15.1s: train loss = 0.00671 = (mf)0.00394 + (embed)0.00277\n",
      "Epoch 7 (train)15.0s: train loss = 0.00599 = (mf)0.00326 + (embed)0.00274\n",
      "Epoch 8 (train)15.1s: train loss = 0.00531 = (mf)0.00263 + (embed)0.00268\n",
      "Epoch 9 (train)15.0s: train loss = 0.00488 = (mf)0.00228 + (embed)0.00260\n",
      "WARNING:tensorflow:From /home/jialia/.conda/envs/reco_gpu_tf15/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "Save model to path /data/home/jialia/jialia/kdd2020tutorial/formal/recommenders/scenarios/KDD2020-tutorial/data_folder/my/LightGCN-training-folder/saved_models/epoch_10\n",
      "Epoch 10 (train)14.8s + (eval)1.1s: train loss = 0.00455 = (mf)0.00202 + (embed)0.00252, recall = 0.30065, ndcg = 0.16785, precision = 0.03006, map = 0.12763\n",
      "Took 153.88978648744524 seconds for training.\n"
     ]
    }
   ],
   "source": [
    "with Timer() as train_time:\n",
    "    model.fit()\n",
    "\n",
    "print(\"Took {} seconds for training.\".format(train_time.interval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_emb_file = os.path.join(lightgcn_dir, 'user.emb.txt')\n",
    "item_emb_file = os.path.join(lightgcn_dir, 'item.emb.txt')\n",
    "model.infer_embedding(\n",
    "    user_emb_file,\n",
    "    item_emb_file    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_labels(labels, preds, group_keys):\n",
    "    \"\"\"Devide labels and preds into several group according to values in group keys.\n",
    "    Args:\n",
    "        labels (list): ground truth label list.\n",
    "        preds (list): prediction score list.\n",
    "        group_keys (list): group key list.\n",
    "    Returns:\n",
    "        all_labels: labels after group.\n",
    "        all_preds: preds after group.\n",
    "    \"\"\"\n",
    "    all_keys = list(set(group_keys))\n",
    "    group_labels = {k: [] for k in all_keys}\n",
    "    group_preds = {k: [] for k in all_keys}\n",
    "    for l, p, k in zip(labels, preds, group_keys):\n",
    "        group_labels[k].append(l)\n",
    "        group_preds[k].append(p)\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    for k in all_keys:\n",
    "        all_labels.append(group_labels[k])\n",
    "        all_preds.append(group_preds[k])\n",
    "    return all_labels, all_preds\n",
    "\n",
    "def load_emb_file(emb_file):\n",
    "    res = {}\n",
    "    with open(emb_file, 'r') as rd:\n",
    "        while True:\n",
    "            line = rd.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            words = line.strip().split('\\t')\n",
    "            values = [float(a) for a in words[1].split(' ')]\n",
    "            res[words[0]] = np.asarray(values, dtype=np.float32)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_scores_via_embeddings(test_filename, user_emb_file, item_emb_file):\n",
    "    print('loading embedding file...', end=' ')\n",
    "    user2vec = load_emb_file(user_emb_file)\n",
    "    item2vec = load_emb_file(item_emb_file)\n",
    "    preds, labels, groupids = [], [], []\n",
    "    with open(test_filename, 'r') as rd:\n",
    "        while True:\n",
    "            line = rd.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            words = line.strip().split('%')\n",
    "            tokens = words[0].split(' ')\n",
    "            userid = words[1]\n",
    "            itemid = tokens[2]\n",
    "            pred = user2vec[userid].dot(item2vec[itemid])\n",
    "            preds.append(pred)\n",
    "            labels.append(int(tokens[0]))\n",
    "            groupids.append(userid)\n",
    "    print('done')\n",
    "    return labels, preds, groupids\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading embedding file... done\n"
     ]
    }
   ],
   "source": [
    "test_filename = os.path.join(rawdata_dir, 'test_{}.txt'.format(tag)) \n",
    "labels, preds, group_keys = infer_scores_via_embeddings(test_filename, user_emb_file, item_emb_file)\n",
    "group_labels, group_preds = group_labels(labels, preds, group_keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ndcg@2': 0.3598, 'ndcg@4': 0.458, 'ndcg@6': 0.5016, 'group_auc': 0.799}\n",
      "{'auc': 0.7977}\n"
     ]
    }
   ],
   "source": [
    "res_pairwise = cal_metric(\n",
    "                group_labels, group_preds, ['ndcg@2;4;6', \"group_auc\"]\n",
    "            )\n",
    "print(res_pairwise)\n",
    "res_pointwise = cal_metric(labels, preds, ['auc'])\n",
    "print(res_pointwise)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reco_gpu_tf15",
   "language": "python",
   "name": "reco_gpu_tf15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
