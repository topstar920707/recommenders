{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DKN : Deep Knowledge-Aware Network for News Recommendation\n",
    "DKN \\[1\\] is a deep learning model which incorporates information from knowledge graph for better news recommendation. Specifically, DKN uses TransX \\[2\\] method for knowledge graph representaion learning, then applies a CNN framework, named KCNN, to combine entity embedding with word embedding and generate a final embedding vector for a news article. CTR prediction is made via an attention-based neural scorer. \n",
    "\n",
    "## Properties of DKN:\n",
    "- DKN is a content-based deep model for CTR prediction rather than traditional ID-based collaborative filtering. \n",
    "- It makes use of knowledge entities and common sense in news content via joint learning from semantic-level and knnowledge-level representations of news articles.\n",
    "- DKN uses an attention module to dynamically calculate a user's aggregated historical representaition.\n",
    "\n",
    "## Reference environment \n",
    "we tested this notebook with two environment settings:\n",
    "cpu with tensorflow==1.15.2 and gpu with tensorflow==1.15.2 on July 1, 2020.\n",
    "\n",
    "## Data format:\n",
    "### DKN takes several files as input as follows:\n",
    "- training / validation / test files: each line in these files represents one instance. Impressionid is used to evaluate performance within an impression session, so it is only used when evaluating, you can set it to 0 for training data. The format is : <br> \n",
    "`[label] [userid] [CandidateNews]%[impressionid] `<br> \n",
    "e.g., `1 train_U1 N1%0` <br> \n",
    "- user history file: each line in this file represents a users' click history. You need to set his_size parameter in config file, which is the max number of user's click history we use. We will automatically keep the last his_size number of user click history, if user's click history is more than his_size, and we will automatically padding 0 if user's click history less than his_size. the format is : <br> \n",
    "`[Userid] [newsid1,newsid2...]`<br>\n",
    "e.g., `train_U1 N1,N2` <br> \n",
    "- document feature file:\n",
    "It contains the word and entity features of news. News article is represented by (aligned) title words and title entities. To take a quick example, a news title may be : Trump to deliver State of the Union address next week , then the title words value may be CandidateNews:34,45,334,23,12,987,3456,111,456,432 and the title entitie value may be: entity:45,0,0,0,0,0,0,0,0,0. Only the first value of entity vector is non-zero due to the word Trump. The title value and entity value is hashed from 1 to n(n is the number of distinct words or entities). Each feature length should be fixed at k(doc_size papameter), if the number of words in document is more than k, you should truncate the document to k words, and if the number of words in document is less than k, you should padding 0 to the end. \n",
    "the format is like: <br> \n",
    "`[Newsid] [w1,w2,w3...wk] [e1,e2,e3...ek]`\n",
    "- word embedding/entity embedding/ context embedding files: These are npy files of pretrained embeddings. After loading, each file is a [n+1,k] two-dimensional matrix, n is the number of words(or entities) of their hash dictionary, k is dimension of the embedding, note that we keep embedding 0 for zero padding. \n",
    "In this experiment, we used GloVe\\[4\\] vectors to initialize the word embedding. We trained entity embedding using TransE\\[2\\] on knowledge graph and context embedding is the average of the entity's neighbors in the knowledge graph.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global settings and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jialia/.conda/envs/reco_gpu/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jialia/.conda/envs/reco_gpu/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jialia/.conda/envs/reco_gpu/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jialia/.conda/envs/reco_gpu/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jialia/.conda/envs/reco_gpu/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jialia/.conda/envs/reco_gpu/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "from reco_utils.recommender.deeprec.deeprec_utils import *\n",
    "from reco_utils.recommender.deeprec.models.dkn import *\n",
    "from reco_utils.recommender.deeprec.io.dkn_iterator import *\n",
    "import papermill as pm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "data_path = 'data_folder/my/DKN-training-folder'\n",
    "yaml_file = os.path.join(data_path, r'../../../../../../dkn.yaml')\n",
    "train_file = os.path.join(data_path, r'train_small.txt')\n",
    "valid_file = os.path.join(data_path, r'valid_small.txt')\n",
    "test_file = os.path.join(data_path, r'test_small.txt')\n",
    "user_history_file = os.path.join(data_path, r'user_history_small.txt')\n",
    "news_feature_file = os.path.join(data_path, r'../paper_feature.txt')\n",
    "wordEmb_file = os.path.join(data_path, r'word_embedding.npy')\n",
    "entityEmb_file = os.path.join(data_path, r'entity_embedding.npy')\n",
    "contextEmb_file = os.path.join(data_path, r'context_embedding.npy')\n",
    "infer_embedding_file = os.path.join(data_path, r'infer_embedding.txt')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "epoch=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('DNN_FIELD_NUM', None), ('EARLY_STOP', 100), ('FEATURE_COUNT', None), ('FIELD_COUNT', None), ('L', None), ('MODEL_DIR', 'data_folder/my/DKN-training-folder/save_models'), ('PAIR_NUM', None), ('SUMMARIES_DIR', None), ('T', None), ('activation', ['sigmoid']), ('att_fcn_layer_sizes', None), ('attention_activation', 'relu'), ('attention_dropout', 0.0), ('attention_layer_sizes', 32), ('attention_size', None), ('batch_size', 100), ('cate_embedding_dim', None), ('cate_vocab', None), ('contextEmb_file', 'data_folder/my/DKN-training-folder/context_embedding.npy'), ('cross_activation', 'identity'), ('cross_l1', 0.0), ('cross_l2', 0.0), ('cross_layer_sizes', None), ('cross_layers', None), ('data_format', 'dkn'), ('decay', None), ('dilations', None), ('dim', 32), ('doc_size', 15), ('dropout', [0.0]), ('dtype', 32), ('embed_l1', 0.0), ('embed_l2', 0.0), ('embed_size', None), ('embedding_dropout', 0.3), ('enable_BN', False), ('entityEmb_file', 'data_folder/my/DKN-training-folder/entity_embedding.npy'), ('entity_dim', 32), ('entity_embedding_method', 'TransE'), ('entity_size', 57267), ('epochs', 30), ('eval_epoch', None), ('fast_CIN_d', 0), ('filter_sizes', [1, 2, 3]), ('hidden_size', None), ('history_size', 20), ('init_method', 'uniform'), ('init_value', 0.01), ('is_clip_norm', True), ('item_embedding_dim', None), ('item_vocab', None), ('iterator_type', None), ('kernel_size', None), ('kg_file', None), ('kg_training_interval', 5), ('layer_l1', 0.0), ('layer_l2', 0.0), ('layer_sizes', [300]), ('learning_rate', 0.002), ('load_model_name', None), ('load_saved_model', False), ('loss', 'log_loss'), ('lr_kg', 0.5), ('lr_rs', 1), ('max_grad_norm', 0.5), ('max_seq_length', None), ('method', 'classification'), ('metrics', ['auc']), ('min_seq_length', 1), ('model_type', 'dkn'), ('mu', None), ('n_h', None), ('n_item', None), ('n_item_attr', None), ('n_layers', None), ('n_user', None), ('n_user_attr', None), ('n_v', None), ('need_sample', True), ('news_feature_file', 'data_folder/my/DKN-training-folder/../paper_feature.txt'), ('num_filters', 50), ('optimizer', 'adam'), ('pairwise_metrics', ['group_auc', 'mean_mrr', 'ndcg@2;4;6']), ('reg_kg', 0.0), ('save_epoch', 1), ('save_model', True), ('show_step', 10000), ('top_k', None), ('train_num_ngs', 4), ('train_ratio', None), ('transform', True), ('use_CIN_part', False), ('use_DNN_part', False), ('use_FM_part', False), ('use_Linear_part', False), ('use_context', True), ('use_entity', True), ('user_clicks', None), ('user_dropout', False), ('user_embedding_dim', None), ('user_history_file', 'data_folder/my/DKN-training-folder/user_history_small.txt'), ('user_vocab', None), ('wordEmb_file', 'data_folder/my/DKN-training-folder/word_embedding.npy'), ('word_size', 194755), ('write_tfevents', False)]\n"
     ]
    }
   ],
   "source": [
    "hparams = prepare_hparams(yaml_file,\n",
    "                          news_feature_file = news_feature_file,\n",
    "                          user_history_file = user_history_file,\n",
    "                          wordEmb_file=wordEmb_file,\n",
    "                          entityEmb_file=entityEmb_file,\n",
    "                          contextEmb_file=contextEmb_file,\n",
    "                          epochs=epoch,\n",
    "                          is_clip_norm=True,\n",
    "                          max_grad_norm=0.5,\n",
    "                          history_size=20,\n",
    "                          MODEL_DIR=os.path.join(data_path, 'save_models'),\n",
    "                          learning_rate=0.002,\n",
    "                          embed_l2=0.0,\n",
    "                          layer_l2=0.0,\n",
    "                          use_entity=True,\n",
    "                          use_context=True\n",
    "                         )\n",
    "print(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "input_creator = DKNTextIterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the DKN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "model = DKN(hparams, input_creator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_model(os.path.join(hparams.MODEL_DIR, 'epoch_10'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'auc': 0.5015, 'group_auc': 0.4994, 'mean_mrr': 0.4528, 'ndcg@2': 0.3217, 'ndcg@4': 0.5101, 'ndcg@6': 0.5868}\n",
      "0.22143967946370444\n"
     ]
    }
   ],
   "source": [
    "t01 = time.time()\n",
    "print(model.run_eval(valid_file))\n",
    "t02 = time.time()\n",
    "print((t02-t01)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 1\n",
      "train info: logloss loss:0.38064596181080596\n",
      "eval info: auc:0.8621, group_auc:0.8426, mean_mrr:0.4861, ndcg@2:0.4051, ndcg@4:0.514, ndcg@6:0.5557\n",
      "at epoch 1 , train time: 135.2 eval time: 20.8\n",
      "at epoch 2\n",
      "train info: logloss loss:0.3395406755043066\n",
      "eval info: auc:0.8727, group_auc:0.8564, mean_mrr:0.5263, ndcg@2:0.4547, ndcg@4:0.5556, ndcg@6:0.5928\n",
      "at epoch 2 , train time: 131.5 eval time: 20.9\n",
      "at epoch 3\n",
      "train info: logloss loss:0.3573350878043176\n",
      "eval info: auc:0.8518, group_auc:0.8332, mean_mrr:0.4945, ndcg@2:0.417, ndcg@4:0.5184, ndcg@6:0.5555\n",
      "at epoch 3 , train time: 129.9 eval time: 20.6\n",
      "at epoch 4\n",
      "train info: logloss loss:0.35086342556632566\n",
      "eval info: auc:0.864, group_auc:0.8506, mean_mrr:0.5312, ndcg@2:0.4604, ndcg@4:0.559, ndcg@6:0.592\n",
      "at epoch 4 , train time: 130.9 eval time: 20.8\n",
      "at epoch 5\n",
      "train info: logloss loss:0.38076122232316717\n",
      "eval info: auc:0.4872, group_auc:0.4841, mean_mrr:0.1618, ndcg@2:0.063, ndcg@4:0.1043, ndcg@6:0.1395\n",
      "at epoch 5 , train time: 130.7 eval time: 20.5\n",
      "at epoch 6\n",
      "train info: logloss loss:0.4690256304255407\n",
      "eval info: auc:0.4598, group_auc:0.4558, mean_mrr:0.1477, ndcg@2:0.0536, ndcg@4:0.0867, ndcg@6:0.1175\n",
      "at epoch 6 , train time: 130.2 eval time: 20.5\n",
      "at epoch 7\n",
      "train info: logloss loss:0.43377971975603447\n",
      "eval info: auc:0.858, group_auc:0.8422, mean_mrr:0.505, ndcg@2:0.4256, ndcg@4:0.5308, ndcg@6:0.5701\n",
      "at epoch 7 , train time: 129.9 eval time: 20.6\n",
      "at epoch 8\n",
      "train info: logloss loss:0.347700213594731\n",
      "eval info: auc:0.8549, group_auc:0.8378, mean_mrr:0.4957, ndcg@2:0.4165, ndcg@4:0.5209, ndcg@6:0.5609\n",
      "at epoch 8 , train time: 130.3 eval time: 21.0\n",
      "at epoch 9\n",
      "train info: logloss loss:0.3538586105952918\n",
      "eval info: auc:0.8558, group_auc:0.8401, mean_mrr:0.5125, ndcg@2:0.4337, ndcg@4:0.5359, ndcg@6:0.5752\n",
      "at epoch 9 , train time: 129.7 eval time: 20.5\n",
      "at epoch 10\n",
      "train info: logloss loss:0.3456883578475008\n",
      "eval info: auc:0.8505, group_auc:0.8354, mean_mrr:0.499, ndcg@2:0.4219, ndcg@4:0.5249, ndcg@6:0.5633\n",
      "at epoch 10 , train time: 130.0 eval time: 20.7\n",
      "at epoch 11\n",
      "train info: logloss loss:0.3445496365928611\n",
      "eval info: auc:0.862, group_auc:0.8493, mean_mrr:0.5287, ndcg@2:0.4545, ndcg@4:0.554, ndcg@6:0.592\n",
      "at epoch 11 , train time: 129.8 eval time: 20.6\n",
      "at epoch 12\n",
      "train info: logloss loss:0.3436703368476003\n",
      "eval info: auc:0.8441, group_auc:0.8335, mean_mrr:0.4925, ndcg@2:0.4148, ndcg@4:0.519, ndcg@6:0.5572\n",
      "at epoch 12 , train time: 130.2 eval time: 20.6\n",
      "at epoch 13\n",
      "train info: logloss loss:0.34149652493192145\n",
      "eval info: auc:0.8444, group_auc:0.8285, mean_mrr:0.5022, ndcg@2:0.4253, ndcg@4:0.5254, ndcg@6:0.5626\n",
      "at epoch 13 , train time: 130.6 eval time: 20.6\n",
      "at epoch 14\n",
      "train info: logloss loss:0.3454536630689886\n",
      "eval info: auc:0.8489, group_auc:0.8334, mean_mrr:0.5126, ndcg@2:0.4426, ndcg@4:0.5347, ndcg@6:0.5713\n",
      "at epoch 14 , train time: 132.1 eval time: 21.0\n",
      "at epoch 15\n",
      "train info: logloss loss:0.3845350150201074\n",
      "eval info: auc:0.842, group_auc:0.829, mean_mrr:0.4955, ndcg@2:0.4189, ndcg@4:0.517, ndcg@6:0.5569\n",
      "at epoch 15 , train time: 131.7 eval time: 20.4\n",
      "at epoch 16\n",
      "train info: logloss loss:0.3520999357706697\n",
      "eval info: auc:0.8501, group_auc:0.8389, mean_mrr:0.5067, ndcg@2:0.4293, ndcg@4:0.535, ndcg@6:0.5708\n",
      "at epoch 16 , train time: 129.6 eval time: 20.6\n",
      "at epoch 17\n",
      "train info: logloss loss:0.3457298466284006\n",
      "eval info: auc:0.8474, group_auc:0.8359, mean_mrr:0.5076, ndcg@2:0.4329, ndcg@4:0.5368, ndcg@6:0.573\n",
      "at epoch 17 , train time: 129.9 eval time: 20.6\n",
      "at epoch 18\n",
      "train info: logloss loss:0.35124656255414205\n",
      "eval info: auc:0.8429, group_auc:0.8331, mean_mrr:0.5038, ndcg@2:0.4276, ndcg@4:0.5288, ndcg@6:0.5678\n",
      "at epoch 18 , train time: 129.7 eval time: 20.5\n",
      "at epoch 19\n",
      "train info: logloss loss:0.3486625248944294\n",
      "eval info: auc:0.8414, group_auc:0.824, mean_mrr:0.5025, ndcg@2:0.4246, ndcg@4:0.5253, ndcg@6:0.5608\n",
      "at epoch 19 , train time: 129.6 eval time: 20.6\n",
      "at epoch 20\n",
      "train info: logloss loss:0.3591816548896077\n",
      "eval info: auc:0.8364, group_auc:0.8162, mean_mrr:0.479, ndcg@2:0.3962, ndcg@4:0.4989, ndcg@6:0.5394\n",
      "at epoch 20 , train time: 129.6 eval time: 20.7\n",
      "at epoch 21\n",
      "train info: logloss loss:0.36265919479645525\n",
      "eval info: auc:0.8338, group_auc:0.8111, mean_mrr:0.4852, ndcg@2:0.4071, ndcg@4:0.5062, ndcg@6:0.5424\n",
      "at epoch 21 , train time: 129.9 eval time: 20.5\n",
      "at epoch 22\n",
      "train info: logloss loss:0.3612714559796305\n",
      "eval info: auc:0.8204, group_auc:0.7933, mean_mrr:0.4609, ndcg@2:0.381, ndcg@4:0.4776, ndcg@6:0.5171\n",
      "at epoch 22 , train time: 129.6 eval time: 20.6\n",
      "at epoch 23\n",
      "train info: logloss loss:0.36809092042502145\n",
      "eval info: auc:0.8432, group_auc:0.8224, mean_mrr:0.5009, ndcg@2:0.4241, ndcg@4:0.5193, ndcg@6:0.5587\n",
      "at epoch 23 , train time: 130.2 eval time: 20.6\n",
      "at epoch 24\n",
      "train info: logloss loss:0.3740048301501601\n",
      "eval info: auc:0.8008, group_auc:0.7873, mean_mrr:0.4563, ndcg@2:0.3748, ndcg@4:0.4711, ndcg@6:0.5118\n",
      "at epoch 24 , train time: 129.7 eval time: 20.4\n",
      "at epoch 25\n",
      "train info: logloss loss:0.3752183868882761\n",
      "eval info: auc:0.8278, group_auc:0.8102, mean_mrr:0.4691, ndcg@2:0.3825, ndcg@4:0.4938, ndcg@6:0.5349\n",
      "at epoch 25 , train time: 129.7 eval time: 20.3\n",
      "at epoch 26\n",
      "train info: logloss loss:0.37400691172759803\n",
      "eval info: auc:0.8308, group_auc:0.8164, mean_mrr:0.4907, ndcg@2:0.4079, ndcg@4:0.5082, ndcg@6:0.55\n",
      "at epoch 26 , train time: 129.6 eval time: 20.5\n",
      "at epoch 27\n",
      "train info: logloss loss:0.4024943312443335\n",
      "eval info: auc:0.8057, group_auc:0.7988, mean_mrr:0.4778, ndcg@2:0.3956, ndcg@4:0.4923, ndcg@6:0.5287\n",
      "at epoch 27 , train time: 129.5 eval time: 20.3\n",
      "at epoch 28\n",
      "train info: logloss loss:0.4427552545772358\n",
      "eval info: auc:0.6371, group_auc:0.6579, mean_mrr:0.3305, ndcg@2:0.2505, ndcg@4:0.2976, ndcg@6:0.3351\n",
      "at epoch 28 , train time: 130.4 eval time: 20.4\n",
      "at epoch 30\n",
      "train info: logloss loss:0.49223829677638314\n",
      "eval info: auc:0.6762, group_auc:0.6731, mean_mrr:0.2893, ndcg@2:0.2173, ndcg@4:0.228, ndcg@6:0.2549\n",
      "at epoch 30 , train time: 132.4 eval time: 20.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<reco_utils.recommender.deeprec.models.dkn.DKN at 0x7fc0518397f0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_file, test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test again the performance on valid set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'auc': 0.6499, 'group_auc': 0.646, 'mean_mrr': 0.3429, 'ndcg@2': 0.1743, 'ndcg@4': 0.3469, 'ndcg@6': 0.5014}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jialia/.conda/envs/reco_gpu/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.1). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/papermill.record+json": {
       "res": {
        "auc": 0.6499,
        "group_auc": 0.646,
        "mean_mrr": 0.3429,
        "ndcg@2": 0.1743,
        "ndcg@4": 0.3469,
        "ndcg@6": 0.5014
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = model.run_eval(valid_file)\n",
    "print(res)\n",
    "pm.record(\"res\", res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t01 = time.time()\n",
    "print(model.run_eval(test_file))\n",
    "t02 = time.time()\n",
    "print((t02-t01)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Document embedding inference API\n",
    "After training, you can get document embedding through this document embedding inference API. The input file format is same with document feature file. The output file fomrat is: `[Newsid] [embedding]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.run_get_embedding(news_feature_file, infer_embedding_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running models with large dataset\n",
    "Here are performances using the whole MIND dataset \\[3\\]. \n",
    "\n",
    "MIND dataset is a large-scale English news dataset. It was collected from anonymized behavior logs of Microsoft News website. MIND contains 1,000,000 users, 161,013 news articles and 15,777,377 impression logs. Every news article contains rich textual content including title, abstract, body, category and entities. Each impression log contains the click events, non-clicked events and historical news click behaviors of this user before this impression.\n",
    "\n",
    "| Models | g-AUC | MRR |NDCG@5 | NDCG@10 |\n",
    "| :------| :------: | :------: | :------: | :------ |\n",
    "| LibFM | 0.5993 | 0.2823 | 0.3005 | 0.3574 |\n",
    "| Wide&Deep | 0.6216 | 0.2931 | 0.3138 | 0.3712 |\n",
    "| DeepFM | 0.6030 | 0.2819 | 0.3002 | 0.3571 |\n",
    "| DKN | 0.6436 | 0.3128 | 0.3371 | 0.3908|\n",
    "\n",
    "\n",
    "Note that the results of DKN is using Microsoft recommender and the results of first three models is from MIND paper \\[3\\].\n",
    "We compare the results on the same test dataset. \n",
    "\n",
    "One epoch takes 6381.3s(5066.6s for training, 1314.7s for evaluating) for DKN on GPU. Hardware specification for running the large dataset: <br>\n",
    "GPU: Tesla P100-PCIE-16GB <br>\n",
    "CPU: 6 cores Intel(R) Xeon(R) CPU E5-2690 v4 @ 2.60GHz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\\[1\\] Wang, Hongwei, et al. \"DKN: Deep Knowledge-Aware Network for News Recommendation.\" Proceedings of the 2018 World Wide Web Conference on World Wide Web. International World Wide Web Conferences Steering Committee, 2018.<br>\n",
    "\\[2\\] Knowledge Graph Embeddings including TransE, TransH, TransR and PTransE. https://github.com/thunlp/KB2E <br>\n",
    "\\[3\\] Wu, Fangzhao, et al. \"MIND: A Large-scale Dataset for News Recommendation\" Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. https://msnews.github.io/competition.html <br>\n",
    "\\[4\\] GloVe: Global Vectors for Word Representation. https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(train_file, test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "reco_gpu",
   "language": "python",
   "name": "reco_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
