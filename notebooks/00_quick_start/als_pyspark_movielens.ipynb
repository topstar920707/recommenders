{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running ALS on MovieLens (pySpark)\n",
    "\n",
    "[ALS](https://spark.apache.org/docs/latest/api/python/_modules/pyspark/ml/recommendation.html#ALS) (Alternating Least Squares) is a well-known collaborative filtering algorithm.\n",
    "\n",
    "This notebook provides an example of how to utilize and evaluate ALS pySpark ML (DataFrame-based API) implementation, meant for large-scale distributed datasets. We use a smaller dataset in this example to run ALS efficiently on Data Science Virtual Machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.6.0 | packaged by conda-forge | (default, Feb  9 2017, 14:36:55) \n",
      "[GCC 4.8.2 20140120 (Red Hat 4.8.2-15)]\n",
      "Spark version: 2.3.1\n"
     ]
    }
   ],
   "source": [
    "# set the environment path to find Recommenders\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import os\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "import pyspark\n",
    "from pyspark.ml.recommendation import ALS\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, FloatType, IntegerType, LongType\n",
    "\n",
    "from reco_utils.common.notebook_utils import is_jupyter, is_databricks\n",
    "from reco_utils.dataset.url_utils import maybe_download\n",
    "from reco_utils.dataset.spark_splitters import spark_random_split\n",
    "from reco_utils.evaluation.spark_evaluation import SparkRatingEvaluation, SparkRankingEvaluation\n",
    "\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Spark version: {}\".format(pyspark.__version__))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# top k items to recommend\n",
    "TOP_K = 10\n",
    "\n",
    "# Select Movielens data size: 100k, 1m, 10m, or 20m\n",
    "MOVIELENS_DATA_SIZE = '100k'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Set up Spark context\n",
    "\n",
    "The following settings work well for debugging locally on VM - change when running on a cluster. We set up a giant single executor with many threads and specify memory cap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following settings work well for debugging locally on VM - change when running on a cluster\n",
    "# set up a giant single executor with many threads and specify memory cap\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"ALS pySpark\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\")\\\n",
    "    .config(\"spark.executor.cores\", \"32\")\\\n",
    "    .config(\"spark.executor.memory\", \"8g\")\\\n",
    "    .config(\"spark.memory.fraction\", \"0.9\")\\\n",
    "    .config(\"spark.memory.stageFraction\", \"0.3\")\\\n",
    "    .config(\"spark.executor.instances\", 1)\\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"36000s\")\\\n",
    "    .config(\"spark.network.timeout\", \"10000000s\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"50g\")\\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Download the MovieLens dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MovieLens data have different data-format for each size of dataset\n",
    "data_header = False\n",
    "if MOVIELENS_DATA_SIZE == \"100k\":\n",
    "    separator = \"\\t\"\n",
    "    data_path = \"ml-100k/u.data\"\n",
    "elif MOVIELENS_DATA_SIZE == \"1m\":\n",
    "    separator = \"::\"\n",
    "    data_path = \"ml-1m/ratings.dat\"\n",
    "elif MOVIELENS_DATA_SIZE == \"10m\":\n",
    "    separator = \"::\"\n",
    "    data_path = \"ml-10M100K/ratings.dat\"\n",
    "elif MOVIELENS_DATA_SIZE == \"20m\":\n",
    "    separator = \",\"\n",
    "    data_path = \"ml-20m/ratings.csv\"\n",
    "    data_header = True\n",
    "else:\n",
    "    raise ValueError(\"Invalid data size. Should be one of {100k, 1m, 10m, or 20m}\")\n",
    "\n",
    "# Download dataset zip file and decompress if haven't done yet\n",
    "dest_folder = \".\"\n",
    "dest_file = data_path\n",
    "\n",
    "if is_databricks():\n",
    "    # Handle local file I/O APIs on Databricks\n",
    "    dest_folder = \"/dbfs/tmp\"\n",
    "    dest_file = os.path.join(dest_folder, dest_file)\n",
    "    data_path = \"dbfs:/tmp/\" + data_path\n",
    "    \n",
    "if not os.path.exists(dest_file):\n",
    "    filename = \"ml-\" + MOVIELENS_DATA_SIZE + \".zip\"\n",
    "    filepath = maybe_download(\n",
    "        \"http://files.grouplens.org/datasets/movielens/\" + filename, filename\n",
    "    )\n",
    "\n",
    "    with ZipFile(filepath, \"r\") as zf:\n",
    "        zf.extractall(dest_folder)\n",
    "\n",
    "    # remove zip file we already used\n",
    "    os.remove(filepath)\n",
    "\n",
    "# Force the file to be flushed to persistent storage\n",
    "if is_databricks():\n",
    "    # In Databricks, passing python variable to shell command like \"!sync {dest_file}\" does not work.\n",
    "    if MOVIELENS_DATA_SIZE == \"100k\":\n",
    "        !sync \"/dbfs/tmp/ml-100k/u.data\"\n",
    "    elif MOVIELENS_DATA_SIZE == \"1m\":\n",
    "        !sync \"/dbfs/tmp/ml-1m/ratings.dat\"\n",
    "    elif MOVIELENS_DATA_SIZE == \"10m\":\n",
    "        !sync \"/dbfs/tmp/ml-10M100K/ratings.dat\"\n",
    "    elif MOVIELENS_DATA_SIZE == \"20m\":\n",
    "        !sync \"/dbfs/tmp/ml-20m/ratings.csv\"\n",
    "else:\n",
    "    !sync {dest_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|UserId|MovieId|Rating|Timestamp|\n",
      "+------+-------+------+---------+\n",
      "|   196|    242|   3.0|881250949|\n",
      "|   186|    302|   3.0|891717742|\n",
      "|    22|    377|   1.0|878887116|\n",
      "|   244|     51|   2.0|880606923|\n",
      "|   166|    346|   1.0|886397596|\n",
      "|   298|    474|   4.0|884182806|\n",
      "|   115|    265|   2.0|881171488|\n",
      "|   253|    465|   5.0|891628467|\n",
      "|   305|    451|   3.0|886324817|\n",
      "|     6|     86|   3.0|883603013|\n",
      "|    62|    257|   2.0|879372434|\n",
      "|   286|   1014|   5.0|879781125|\n",
      "|   200|    222|   5.0|876042340|\n",
      "|   210|     40|   3.0|891035994|\n",
      "|   224|     29|   3.0|888104457|\n",
      "|   303|    785|   3.0|879485318|\n",
      "|   122|    387|   5.0|879270459|\n",
      "|   194|    274|   2.0|879539794|\n",
      "|   291|   1042|   4.0|874834944|\n",
      "|   234|   1184|   2.0|892079237|\n",
      "+------+-------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Note: The DataFrame-based API for ALS currently only supports integers for user and item ids.\n",
    "schema = StructType(\n",
    "    (\n",
    "        StructField(\"UserId\", IntegerType()),\n",
    "        StructField(\"MovieId\", IntegerType()),\n",
    "        StructField(\"Rating\", FloatType()),\n",
    "        StructField(\"Timestamp\", LongType()),\n",
    "    )\n",
    ")\n",
    "\n",
    "# pySpark's read csv currently doesn't support multi-character delimiter, thus we manually handle that\n",
    "if len(separator) > 1:\n",
    "    raw_data = spark.sparkContext.textFile(data_path)\n",
    "    # In databricks (or maybe in multi-cluster machines), somehow file \n",
    "    raw_data.take(1)\n",
    "    data_rdd = raw_data.map(lambda l: l.split(separator)) \\\n",
    "        .map(lambda c: [int(c[0]), int(c[1]), float(c[2]), int(c[3])])\n",
    "    data = spark.createDataFrame(data_rdd, schema)\n",
    "else:\n",
    "    data = spark.read.csv(data_path, schema=schema, sep=separator, header=data_header)\n",
    "\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split the data using the Spark random splitter provided in utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N train 75193\n",
      "N test 24807\n"
     ]
    }
   ],
   "source": [
    "train, test = spark_random_split(data, ratio=0.75, seed=123)\n",
    "print (\"N train\", train.cache().count())\n",
    "print (\"N test\", test.cache().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the ALS model on the training data, and get the top-k recommendations for our testing data\n",
    "\n",
    "To predict movie ratings, we use the rating data in the training set as users' explicit feedbacks.\n",
    "\n",
    "When our goal is to recommend top k movies a user is likely to watch, on the other hand, we utilize the ratings as implicit feedbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = {\n",
    "    \"userCol\": \"UserId\",\n",
    "    \"itemCol\": \"MovieId\",\n",
    "    \"ratingCol\": \"Rating\",\n",
    "}\n",
    "\n",
    "\n",
    "# implicitPrefs=True for recommendation, False for rating prediction\n",
    "als = ALS(\n",
    "    rank=40,\n",
    "    maxIter=15,\n",
    "    implicitPrefs=True,\n",
    "    alpha=0.1,\n",
    "    regParam=0.01,\n",
    "    coldStartStrategy='drop',\n",
    "    nonnegative=True,\n",
    "    **header\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = als.fit(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|UserId|     recommendations|\n",
      "+------+--------------------+\n",
      "|   471|[[420, 0.64394724...|\n",
      "|   463|[[13, 1.0953349],...|\n",
      "|   833|[[234, 1.2661667]...|\n",
      "|   496|[[173, 0.75235784...|\n",
      "|   148|[[169, 0.9539694]...|\n",
      "|   540|[[50, 0.88092697]...|\n",
      "|   392|[[50, 0.9353202],...|\n",
      "|   243|[[275, 1.1422337]...|\n",
      "|   623|[[127, 0.927937],...|\n",
      "|   737|[[127, 0.49741036...|\n",
      "|   897|[[174, 0.99030745...|\n",
      "|   858|[[127, 0.9081067]...|\n",
      "|    31|[[269, 0.6397016]...|\n",
      "|   516|[[357, 0.3626451]...|\n",
      "|   580|[[405, 1.0172005]...|\n",
      "|   251|[[121, 1.2434335]...|\n",
      "|   451|[[289, 1.1142702]...|\n",
      "|    85|[[275, 1.0268326]...|\n",
      "|   137|[[117, 1.0683532]...|\n",
      "|   808|[[313, 1.0408531]...|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recommendations = model.recommendForUserSubset(test, TOP_K)\n",
    "recommendations.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+\n",
      "|UserId|MovieId|    rating|\n",
      "+------+-------+----------+\n",
      "|   471|    420|0.64394724|\n",
      "|   471|    501|   0.57905|\n",
      "|   471|    625|0.54104215|\n",
      "|   471|    404|0.53206724|\n",
      "|   471|    465|0.52648234|\n",
      "|   471|    946| 0.4953863|\n",
      "|   471|    140|0.48872247|\n",
      "|   471|    416| 0.4879257|\n",
      "|   471|    102| 0.4875346|\n",
      "|   471|     99|0.47469372|\n",
      "|   463|     13| 1.0953349|\n",
      "|   463|    124| 1.0937009|\n",
      "|   463|    302| 1.0562845|\n",
      "|   463|    286|0.97413903|\n",
      "|   463|    285| 0.9637387|\n",
      "|   463|    242| 0.9326599|\n",
      "|   463|      1| 0.9270849|\n",
      "|   463|     19| 0.9269906|\n",
      "|   463|    303|0.90075296|\n",
      "|   463|    269|0.89880556|\n",
      "+------+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert to reco util's ranking evaluator format\n",
    "top_k = recommendations.select('UserId', F.explode('recommendations').alias('r')) \\\n",
    "    .select('UserId', 'r.*')\n",
    "top_k.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluate how well ALS performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|UserId|MovieId|Rating|Timestamp|\n",
      "+------+-------+------+---------+\n",
      "|     1|      2|   3.0|876893171|\n",
      "|     1|      3|   4.0|878542960|\n",
      "|     1|      4|   3.0|876893119|\n",
      "|     1|      9|   5.0|878543541|\n",
      "|     1|     11|   2.0|875072262|\n",
      "|     1|     17|   3.0|875073198|\n",
      "|     1|     25|   4.0|875071805|\n",
      "|     1|     28|   4.0|875072173|\n",
      "|     1|     30|   3.0|878542515|\n",
      "|     1|     33|   4.0|878542699|\n",
      "|     1|     43|   4.0|878542869|\n",
      "|     1|     48|   5.0|875072520|\n",
      "|     1|     49|   3.0|878542478|\n",
      "|     1|     52|   4.0|875072205|\n",
      "|     1|     59|   5.0|876892817|\n",
      "|     1|     62|   3.0|878542282|\n",
      "|     1|     65|   4.0|875072125|\n",
      "|     1|     66|   4.0|878543030|\n",
      "|     1|     71|   3.0|876892425|\n",
      "|     1|     78|   1.0|878543176|\n",
      "+------+-------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_eval = SparkRankingEvaluation(test, top_k, k = TOP_K, col_user=\"UserId\", col_item=\"MovieId\", \n",
    "                                    col_rating=\"Rating\", col_prediction=\"rating\", \n",
    "                                    relevancy_method=\"top_k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\tALS\n",
      "Top K:\t10\n",
      "MAP:\t0.025732\n",
      "NDCG:\t0.099981\n",
      "Precision@K:\t0.094268\n",
      "Recall@K:\t0.075972\n"
     ]
    }
   ],
   "source": [
    "print(\"Model:\\tALS\",\n",
    "      \"Top K:\\t%d\" % rank_eval.k,\n",
    "      \"MAP:\\t%f\" % rank_eval.map_at_k(),\n",
    "      \"NDCG:\\t%f\" % rank_eval.ndcg_at_k(),\n",
    "      \"Precision@K:\\t%f\" % rank_eval.precision_at_k(),\n",
    "      \"Recall@K:\\t%f\" % rank_eval.recall_at_k(), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluate rating prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+----------+\n",
      "|UserId|MovieId|Rating|Timestamp|prediction|\n",
      "+------+-------+------+---------+----------+\n",
      "|   406|    148|   3.0|879540276|  3.149721|\n",
      "|    27|    148|   3.0|891543129| 3.1062589|\n",
      "|   606|    148|   3.0|878150506|  4.206346|\n",
      "|   916|    148|   2.0|880843892| 2.0281613|\n",
      "|   236|    148|   4.0|890117028| 3.4453466|\n",
      "|   602|    148|   4.0|888638517| 3.7288864|\n",
      "|   663|    148|   4.0|889492989| 3.2095706|\n",
      "|   372|    148|   5.0|876869915| 4.9773664|\n",
      "|   190|    148|   4.0|891033742|  4.093347|\n",
      "|     1|    148|   2.0|875240799| 1.5550745|\n",
      "|   297|    148|   3.0|875239619|  2.721611|\n",
      "|   178|    148|   4.0|882824325|  3.561745|\n",
      "|   308|    148|   3.0|887740788| 3.5785131|\n",
      "|   923|    148|   4.0|880387474|  4.117514|\n",
      "|    54|    148|   3.0|880937490|  3.801107|\n",
      "|   430|    148|   2.0|877226047| 4.4093704|\n",
      "|    92|    148|   2.0|877383934| 2.7917807|\n",
      "|   447|    148|   4.0|878854729| 3.7435863|\n",
      "|   374|    148|   4.0|880392992| 3.5979629|\n",
      "|   891|    148|   5.0|891639793| 2.8613977|\n",
      "+------+-------+------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "als_prediction = ALS(\n",
    "    rank=40,\n",
    "    maxIter=15,\n",
    "    implicitPrefs=False,\n",
    "    regParam=0.01,\n",
    "    coldStartStrategy='drop',\n",
    "    nonnegative=True,\n",
    "    **header\n",
    ")\n",
    "\n",
    "model_prediction = als_prediction.fit(train)\n",
    "\n",
    "prediction = model_prediction.transform(test)\n",
    "prediction.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\tALS rating prediction\n",
      "RMSE:\t1.11\n",
      "MAE:\t0.869224\n",
      "Explained variance:\t0.020389\n",
      "R squared:\t0.016262\n"
     ]
    }
   ],
   "source": [
    "rating_eval = SparkRatingEvaluation(test, prediction, col_user=\"UserId\", col_item=\"MovieId\", \n",
    "                                    col_rating=\"Rating\", col_prediction=\"prediction\")\n",
    "\n",
    "print(\"Model:\\tALS rating prediction\",\n",
    "      \"RMSE:\\t%.2f\" % rating_eval.rmse(),\n",
    "      \"MAE:\\t%f\" % rating_eval.mae(),\n",
    "      \"Explained variance:\\t%f\" % rating_eval.exp_var(),\n",
    "      \"R squared:\\t%f\" % rating_eval.rsquared(), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/papermill.record+json": {
       "map": 0.025732158398746017
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/papermill.record+json": {
       "ndcg": 0.09998145313091748
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/papermill.record+json": {
       "precision": 0.0942675159235669
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/papermill.record+json": {
       "recall": 0.07597175055748426
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/papermill.record+json": {
       "rmse": 1.1118878013375997
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/papermill.record+json": {
       "mae": 0.8692244519564426
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/papermill.record+json": {
       "exp_var": 0.020388681643881745
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/papermill.record+json": {
       "rsquared": 0.016262004074968583
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if is_jupyter():\n",
    "    # Record results with papermill for tests\n",
    "    import papermill as pm\n",
    "    pm.record(\"map\", rank_eval.map_at_k())\n",
    "    pm.record(\"ndcg\", rank_eval.ndcg_at_k())\n",
    "    pm.record(\"precision\", rank_eval.precision_at_k())\n",
    "    pm.record(\"recall\", rank_eval.recall_at_k())\n",
    "    pm.record(\"rmse\", rating_eval.rmse())\n",
    "    pm.record(\"mae\", rating_eval.mae())\n",
    "    pm.record(\"exp_var\", rating_eval.exp_var())\n",
    "    pm.record(\"rsquared\", rating_eval.rsquared())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
