{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running ALS on MovieLens (pySpark)\n",
    "\n",
    "ALS (Alternating Least Squares) is a well-known collaborative filtering algorithm.\n",
    "\n",
    "This notebook provides an example of how to utilize and evaluate ALS pySpark ML implementation, meant for large-scale distributed datasets. We use a smaller dataset in this example to run SAR efficiently on Data Science Virtual Machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.6.0 | packaged by conda-forge | (default, Feb  9 2017, 14:36:55) \n",
      "[GCC 4.8.2 20140120 (Red Hat 4.8.2-15)]\n",
      "Spark version: 2.3.1\n"
     ]
    }
   ],
   "source": [
    "# set the environment path to find Recommenders\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from reco_utils.dataset.url_utils import maybe_download\n",
    "from reco_utils.dataset.spark_splitters import spark_random_split\n",
    "from reco_utils.evaluation.spark_evaluation import SparkRatingEvaluation, SparkRankingEvaluation\n",
    "\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, FloatType, IntegerType\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Spark version: {}\".format(pyspark.__version__))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top k items to recommend\n",
    "TOP_K = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Set up Spark context\n",
    "\n",
    "The following settings work well for debugging locally on VM - change when running on a cluster. We set up a giant single executor with many threads and specify memory cap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following settings work well for debugging locally on VM - change when running on a cluster\n",
    "# set up a giant single executor with many threads and specify memory cap\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"SAR pySpark\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\")\\\n",
    "    .config(\"spark.executor.cores\", \"32\")\\\n",
    "    .config(\"spark.executor.memory\", \"8g\")\\\n",
    "    .config(\"spark.yarn.executor.memoryOverhead\", \"3g\")\\\n",
    "    .config(\"spark.memory.fraction\", \"0.9\")\\\n",
    "    .config(\"spark.memory.stageFraction\", \"0.3\")\\\n",
    "    .config(\"spark.executor.instances\", 1)\\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"36000s\")\\\n",
    "    .config(\"spark.network.timeout\", \"10000000s\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"50g\")\\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Download the MovieLens dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = maybe_download(\"http://files.grouplens.org/datasets/movielens/ml-100k/u.data\", \"ml-100k.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|UserId|MovieId|Rating|Timestamp|\n",
      "+------+-------+------+---------+\n",
      "|   196|    242|   3.0|881250949|\n",
      "|   186|    302|   3.0|891717742|\n",
      "|    22|    377|   1.0|878887116|\n",
      "|   244|     51|   2.0|880606923|\n",
      "|   166|    346|   1.0|886397596|\n",
      "|   298|    474|   4.0|884182806|\n",
      "|   115|    265|   2.0|881171488|\n",
      "|   253|    465|   5.0|891628467|\n",
      "|   305|    451|   3.0|886324817|\n",
      "|     6|     86|   3.0|883603013|\n",
      "|    62|    257|   2.0|879372434|\n",
      "|   286|   1014|   5.0|879781125|\n",
      "|   200|    222|   5.0|876042340|\n",
      "|   210|     40|   3.0|891035994|\n",
      "|   224|     29|   3.0|888104457|\n",
      "|   303|    785|   3.0|879485318|\n",
      "|   122|    387|   5.0|879270459|\n",
      "|   194|    274|   2.0|879539794|\n",
      "|   291|   1042|   4.0|874834944|\n",
      "|   234|   1184|   2.0|892079237|\n",
      "+------+-------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType(\n",
    "    (\n",
    "        StructField(\"UserId\", IntegerType()),\n",
    "        StructField(\"MovieId\", IntegerType()),\n",
    "        StructField(\"Rating\", FloatType()),\n",
    "        StructField(\"Timestamp\", IntegerType()),\n",
    "    )\n",
    ")\n",
    "data = spark.read.csv(filepath, schema=schema, sep=\"\\t\", header=False)\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split the data using the Spark random splitter provided in utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N train 75193\n",
      "N test 24807\n"
     ]
    }
   ],
   "source": [
    "train, test = spark_random_split(data, ratio=0.75, seed=123)\n",
    "print (\"N train\", train.count())\n",
    "print (\"N test\", test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = {\n",
    "    \"userCol\": \"UserId\",\n",
    "    \"itemCol\": \"MovieId\",\n",
    "    \"ratingCol\": \"Rating\",\n",
    "}\n",
    "\n",
    "# Even we uses explicit rating, implicitPrefs=True produces higher accuracy\n",
    "als = ALS(\n",
    "    rank=40,\n",
    "    maxIter=15,\n",
    "    implicitPrefs=True,\n",
    "    alpha=0.1,\n",
    "    coldStartStrategy='drop',\n",
    "    **header\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the ALS model on the training data, and get the top-k recommendations for our testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = als.fit(train)\n",
    "recommendations = model.recommendForUserSubset(test, TOP_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|UserId|     recommendations|\n",
      "+------+--------------------+\n",
      "|   471|[[418, 0.4986122]...|\n",
      "|   463|[[285, 0.70806473...|\n",
      "|   833|[[23, 0.7805631],...|\n",
      "|   496|[[419, 0.5622276]...|\n",
      "|   148|[[169, 0.5968351]...|\n",
      "|   540|[[1, 0.6456575], ...|\n",
      "|   392|[[286, 0.6480617]...|\n",
      "|   243|[[275, 0.54525304...|\n",
      "|   623|[[50, 0.5494961],...|\n",
      "|   737|[[127, 0.35151374...|\n",
      "|   897|[[210, 0.7934636]...|\n",
      "|   858|[[286, 0.5446349]...|\n",
      "|    31|[[303, 0.53303367...|\n",
      "|   516|[[286, 0.2644445]...|\n",
      "|   580|[[405, 0.6236924]...|\n",
      "|   251|[[181, 0.8814149]...|\n",
      "|   451|[[326, 0.80617386...|\n",
      "|    85|[[197, 0.7751863]...|\n",
      "|   137|[[121, 0.68923014...|\n",
      "|   808|[[288, 0.66081864...|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recommendations.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+\n",
      "|UserId|MovieId|    rating|\n",
      "+------+-------+----------+\n",
      "|   471|    418| 0.4986122|\n",
      "|   471|    501|0.46774796|\n",
      "|   471|    946|0.41865447|\n",
      "|   471|    596|0.41041267|\n",
      "|   471|    420| 0.3920477|\n",
      "|   471|     99|0.38924447|\n",
      "|   471|    404|0.38697183|\n",
      "|   471|    477|0.38583365|\n",
      "|   471|     71|0.38505054|\n",
      "|   471|    588|0.36934996|\n",
      "|   463|    285|0.70806473|\n",
      "|   463|     13| 0.6797578|\n",
      "|   463|    124| 0.6626799|\n",
      "|   463|    116|0.63455147|\n",
      "|   463|    475| 0.6337175|\n",
      "|   463|     14|0.61140335|\n",
      "|   463|    286| 0.6070779|\n",
      "|   463|    302|0.60614985|\n",
      "|   463|    111|0.59022146|\n",
      "|   463|    258| 0.5770138|\n",
      "+------+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert to reco util's ranking evaluator format\n",
    "top_k = recommendations.select('UserId', F.explode('recommendations').alias('r')) \\\n",
    "    .select('UserId', 'r.*')\n",
    "top_k.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluate how well SAR performs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|UserId|MovieId|Rating|Timestamp|\n",
      "+------+-------+------+---------+\n",
      "|     1|      2|   3.0|876893171|\n",
      "|     1|      3|   4.0|878542960|\n",
      "|     1|      4|   3.0|876893119|\n",
      "|     1|      9|   5.0|878543541|\n",
      "|     1|     11|   2.0|875072262|\n",
      "|     1|     17|   3.0|875073198|\n",
      "|     1|     25|   4.0|875071805|\n",
      "|     1|     28|   4.0|875072173|\n",
      "|     1|     30|   3.0|878542515|\n",
      "|     1|     33|   4.0|878542699|\n",
      "|     1|     43|   4.0|878542869|\n",
      "|     1|     48|   5.0|875072520|\n",
      "|     1|     49|   3.0|878542478|\n",
      "|     1|     52|   4.0|875072205|\n",
      "|     1|     59|   5.0|876892817|\n",
      "|     1|     62|   3.0|878542282|\n",
      "|     1|     65|   4.0|875072125|\n",
      "|     1|     66|   4.0|878543030|\n",
      "|     1|     71|   3.0|876892425|\n",
      "|     1|     78|   1.0|878543176|\n",
      "+------+-------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_eval = SparkRankingEvaluation(test, top_k, k = TOP_K, col_user=\"UserId\", col_item=\"MovieId\", \n",
    "                                    col_rating=\"Rating\", col_prediction=\"rating\", \n",
    "                                    relevancy_method=\"top_k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\tALS\n",
      "Top K:\t10\n",
      "MAP:\t0.027311\n",
      "NDCG:\t0.110232\n",
      "Precision@K:\t0.105308\n",
      "Recall@K:\t0.082848\n"
     ]
    }
   ],
   "source": [
    "print(\"Model:\\tALS\",\n",
    "      \"Top K:\\t%d\" % rank_eval.k,\n",
    "      \"MAP:\\t%f\" % rank_eval.map_at_k(),\n",
    "      \"NDCG:\\t%f\" % rank_eval.ndcg_at_k(),\n",
    "      \"Precision@K:\\t%f\" % rank_eval.precision_at_k(),\n",
    "      \"Recall@K:\\t%f\" % rank_eval.recall_at_k(), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
