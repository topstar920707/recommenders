{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running SAR on MovieLens (pySpark)\n",
    "\n",
    "SAR is a fast scalable adaptive algorithm for personalized recommendations based on user transaction history and item descriptions. It produces easily explainable / interpretable recommendations and handles \"cold item\" and \"semi-cold user\" scenarios. \n",
    "\n",
    "This notebook provides an example of how to utilize and evaluate SAR's pySpark implementation, meant for large-scale distributed datasets. We use a smaller dataset in this example to run SAR efficiently on Data Science Virtual Machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.5.5 |Anaconda custom (64-bit)| (default, May 13 2018, 21:12:35) \n",
      "[GCC 7.2.0]\n",
      "Spark version: 2.3.1\n"
     ]
    }
   ],
   "source": [
    "# set the environment path to find Recommenders\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from utilities.recommender.sar.sar_pyspark import SARpySparkReference\n",
    "from utilities.dataset.url_utils import maybe_download\n",
    "from utilities.dataset.spark_splitters import spark_random_split\n",
    "from utilities.evaluation.spark_evaluation import SparkRatingEvaluation, SparkRankingEvaluation\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, FloatType, IntegerType\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Spark version: {}\".format(pyspark.__version__))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Set up Spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following settings work well for debugging locally on VM - change when running on a cluster\n",
    "# set up a giant single executor with many threads and specify memory cap\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"SAR pySpark\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\")\\\n",
    "    .config(\"spark.executor.cores\", \"32\")\\\n",
    "    .config(\"spark.executor.memory\", \"8g\")\\\n",
    "    .config(\"spark.yarn.executor.memoryOverhead\", \"3g\")\\\n",
    "    .config(\"spark.memory.fraction\", \"0.9\")\\\n",
    "    .config(\"spark.memory.stageFraction\", \"0.3\")\\\n",
    "    .config(\"spark.executor.instances\", 1)\\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"36000s\")\\\n",
    "    .config(\"spark.network.timeout\", \"10000000s\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"50g\")\\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Download the MovieLens dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = maybe_download(\"http://files.grouplens.org/datasets/movielens/ml-100k/u.data\", \"ml-100k.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|UserId|MovieId|Rating|Timestamp|\n",
      "+------+-------+------+---------+\n",
      "|   196|    242|   3.0|881250949|\n",
      "|   186|    302|   3.0|891717742|\n",
      "|    22|    377|   1.0|878887116|\n",
      "|   244|     51|   2.0|880606923|\n",
      "|   166|    346|   1.0|886397596|\n",
      "|   298|    474|   4.0|884182806|\n",
      "|   115|    265|   2.0|881171488|\n",
      "|   253|    465|   5.0|891628467|\n",
      "|   305|    451|   3.0|886324817|\n",
      "|     6|     86|   3.0|883603013|\n",
      "|    62|    257|   2.0|879372434|\n",
      "|   286|   1014|   5.0|879781125|\n",
      "|   200|    222|   5.0|876042340|\n",
      "|   210|     40|   3.0|891035994|\n",
      "|   224|     29|   3.0|888104457|\n",
      "|   303|    785|   3.0|879485318|\n",
      "|   122|    387|   5.0|879270459|\n",
      "|   194|    274|   2.0|879539794|\n",
      "|   291|   1042|   4.0|874834944|\n",
      "|   234|   1184|   2.0|892079237|\n",
      "+------+-------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType((StructField(\"UserId\", StringType()),\n",
    "                       StructField(\"MovieId\", StringType()),\n",
    "                       StructField(\"Rating\", FloatType()),\n",
    "                       StructField(\"Timestamp\", IntegerType())))\n",
    "data = spark.read.csv(filepath, schema = schema, sep=\"\\t\", header=False)\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split the data using the Spark random splitter provided in utilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N train 75193\n",
      "N test 24807\n"
     ]
    }
   ],
   "source": [
    "train, test = spark_random_split(data)\n",
    "print (\"N train\", train.count())\n",
    "print (\"N test\", test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = {\n",
    "        \"col_user\": \"UserId\",\n",
    "        \"col_item\": \"MovieId\",\n",
    "        \"col_rating\": \"Rating\",\n",
    "        \"col_timestamp\": \"Timestamp\",\n",
    "    }\n",
    "\n",
    "model = SARpySparkReference(spark=spark,\n",
    "                remove_seen=True, similarity_type=\"jaccard\", \n",
    "                time_decay_coefficient=30, time_now=None, timedecay_formula=True, **header\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. In order to use SAR, we need to hash users and items and make sure there are no cold users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N train 75193\n",
      "N test 24807\n"
     ]
    }
   ],
   "source": [
    "# explicitly make sure we don't have cold users\n",
    "train_set_users = set([x[0] for x in train.select(header[\"col_user\"]).distinct().collect()])\n",
    "test_set_users = set([x[0] for x in test.select(header[\"col_user\"]).distinct().collect()])\n",
    "both_sets = train_set_users.intersection(test_set_users)\n",
    "test = test.filter(F.col(header[\"col_user\"]).isin(both_sets))\n",
    "print (\"N train\", train.count())\n",
    "print (\"N test\", test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build uniform index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running query -- select UserId, dense_rank() over(partition by 1 order by UserId) as row_id, MovieId, dense_rank() over(partition by 1 order by MovieId) as col_id, Rating, Timestamp, type from df_all\n"
     ]
    }
   ],
   "source": [
    "# we need to index item IDs which we want to score later, i.e. we need to consider all items\n",
    "train = train.withColumn('type', F.lit(1))\n",
    "test = test.withColumn('type', F.lit(0))\n",
    "df_all = train.union(test)\n",
    "df_all.createOrReplaceTempView(\"df_all\")\n",
    "\n",
    "# create new index for the items\n",
    "query = \"select \" + header[\"col_user\"] + \", \" +\\\n",
    "    \"dense_rank() over(partition by 1 order by \" + header[\"col_user\"] + \") as row_id, \" +\\\n",
    "                    header[\"col_item\"] + \", \" +\\\n",
    "    \"dense_rank() over(partition by 1 order by \" + header[\"col_item\"] + \") as col_id, \" +\\\n",
    "        header[\"col_rating\"] + \", \" + header[\"col_timestamp\"] + \", type from df_all\"\n",
    "print(\"Running query -- \" + query)\n",
    "df_all = spark.sql(query)\n",
    "df_all.createOrReplaceTempView(\"df_all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recover the original data but now with index build-in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtain the indexed dataframes\n",
      "Running query -- select row_id, col_id, Rating, Timestamp from df_all where type=1\n",
      "Running query -- select row_id, col_id, Rating, Timestamp from df_all where type=0\n"
     ]
    }
   ],
   "source": [
    "print(\"Obtain the indexed dataframes\")\n",
    "query = \"select row_id, col_id, \" + header[\"col_rating\"] + \", \" + header[\"col_timestamp\"] + \" from df_all where type=1\"\n",
    "print(\"Running query -- \" + query)\n",
    "train_indexed = spark.sql(query)\n",
    "\n",
    "query = \"select row_id, col_id, \" + header[\"col_rating\"] + \", \" + header[\"col_timestamp\"] + \" from df_all where type=0\"\n",
    "print(\"Running query -- \" + query)\n",
    "test_indexed = spark.sql(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build index mappings: IDs to index and index to IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining all users and items \n",
      "Indexing users and items\n"
     ]
    }
   ],
   "source": [
    "print(\"Obtaining all users and items \")\n",
    "# Obtain all the users and items from both training and test data\n",
    "unique_users =\\\n",
    "    np.array([x[header[\"col_user\"]] for x in df_all.select(header[\"col_user\"]).distinct().toLocalIterator()])\n",
    "unique_items =\\\n",
    "    np.array([x[header[\"col_item\"]] for x in df_all.select(header[\"col_item\"]).distinct().toLocalIterator()])\n",
    "\n",
    "print(\"Indexing users and items\")\n",
    "# index all rows and columns, then split again intro train and test\n",
    "# We perform the reduction on Spark across keys before calling .collect so this is scalable\n",
    "index2user = \\\n",
    "    dict(df_all.select([\"row_id\", header[\"col_user\"]]).rdd.reduceByKey(lambda _, v: v).collect())\n",
    "index2item = \\\n",
    "    dict(df_all.select([\"col_id\", header[\"col_item\"]]).rdd.reduceByKey(lambda _, v: v).collect())\n",
    "\n",
    "# reverse the dictionaries: actual IDs to inner index\n",
    "user_map_dict = {v: k for k, v in index2user.items()}\n",
    "item_map_dict = {v: k for k, v in index2item.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the index values in the model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_index(unique_users, unique_items, user_map_dict, item_map_dict, index2user, index2item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train the SAR model on our training data, and get the top-k recommendations for our testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utilities.recommender.sar.sar_pyspark:Collecting user affinity matrix...\n",
      "INFO:utilities.recommender.sar.sar_pyspark:Calculating time-decayed affinities...\n",
      "INFO:utilities.recommender.sar.sar_pyspark:Running query -- select\n",
      "            row_id, col_id, sum(Rating * exp(-log(2) * (893286638.000000 - Timestamp) / (30.000000 * 3600 * 24))) as Affinity\n",
      "            from df_train group\n",
      "            by\n",
      "            row_id, col_id\n",
      "INFO:utilities.recommender.sar.sar_pyspark:Calculating item cooccurrence...\n",
      "INFO:utilities.recommender.sar.sar_pyspark:Calculating item similarity...\n",
      "INFO:utilities.recommender.sar.sar_pyspark:Running query -- select A.row_item_id, A.col_item_id, (A.value/(B.d+C.d-A.value)) as value from item_cooccurrence as A, diagonal as B, diagonal as C where A.row_item_id = B.i and A.col_item_id=C.i\n",
      "INFO:utilities.recommender.sar.sar_pyspark:Calculating recommendation scores...\n",
      "INFO:utilities.recommender.sar.sar_pyspark:done training\n",
      "INFO:utilities.recommender.sar.sar_pyspark:Removing seen items...\n",
      "INFO:utilities.recommender.sar.sar_pyspark:Getting top K...\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_indexed)\n",
    "top_k = model.recommend_k_items(test_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------------------+\n",
      "|UserId|MovieId|        prediction|\n",
      "+------+-------+------------------+\n",
      "|   796|    204|165.31601890070047|\n",
      "|   796|    186| 155.3215673077529|\n",
      "|   796|    423|154.52656954379694|\n",
      "|   551|    161|154.49870950687432|\n",
      "|   796|     97|153.46520937913883|\n",
      "|   551|    385|149.90269568537045|\n",
      "|   551|    568|149.16227159140976|\n",
      "|   551|     22| 148.6425848821096|\n",
      "|   796|    655| 148.2438372962911|\n",
      "|   551|    655|146.64785742078595|\n",
      "|   796|    403|146.32392308408168|\n",
      "|   796|    550|145.39668926104616|\n",
      "|   796|    176| 145.2127404685524|\n",
      "|   551|     64| 145.0752306738755|\n",
      "|   796|     64| 145.0373327965255|\n",
      "|   796|    195|145.01188955356974|\n",
      "|   551|    173|144.29283951474824|\n",
      "|   416|    204| 144.1174055165611|\n",
      "|   551|    195| 143.7550428908026|\n",
      "|   551|    176|141.62147020334285|\n",
      "+------+-------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_k.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluate how well SAR performs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+----+\n",
      "|UserId|MovieId|Rating|Timestamp|type|\n",
      "+------+-------+------+---------+----+\n",
      "|     1|     10|   3.0|875693118|   0|\n",
      "|     1|    100|   5.0|878543541|   0|\n",
      "|     1|    101|   2.0|878542845|   0|\n",
      "|     1|    106|   4.0|875241390|   0|\n",
      "|     1|    108|   5.0|875240920|   0|\n",
      "|     1|    113|   5.0|878542738|   0|\n",
      "|     1|    120|   1.0|875241637|   0|\n",
      "|     1|    123|   4.0|875071541|   0|\n",
      "|     1|    125|   3.0|878542960|   0|\n",
      "|     1|    128|   4.0|875072573|   0|\n",
      "|     1|    137|   5.0|875071541|   0|\n",
      "|     1|    141|   3.0|878542608|   0|\n",
      "|     1|    142|   2.0|878543238|   0|\n",
      "|     1|    145|   2.0|875073067|   0|\n",
      "|     1|    151|   4.0|875072865|   0|\n",
      "|     1|    154|   5.0|878543541|   0|\n",
      "|     1|    157|   4.0|876892918|   0|\n",
      "|     1|    158|   3.0|878542699|   0|\n",
      "|     1|    162|   4.0|878542420|   0|\n",
      "|     1|    169|   5.0|878543541|   0|\n",
      "+------+-------+------+---------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_eval = SparkRankingEvaluation(test, top_k, col_user=\"UserId\", col_item=\"MovieId\", \n",
    "                                    col_rating=\"Rating\", col_prediction=\"prediction\", \n",
    "                                    relevancy_method=\"top_k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\tsar_pyspark\n",
      "Top K:\t10\n",
      "MAP:\t0.110564\n",
      "NDCG:\t0.378465\n",
      "Precision@K:\t0.330786\n",
      "Recall@K:\t0.185000\n"
     ]
    }
   ],
   "source": [
    "print(\"Model:\\t\" + model.model_str,\n",
    "      \"Top K:\\t%d\" % rank_eval.k,\n",
    "      \"MAP:\\t%f\" % rank_eval.map_at_k(),\n",
    "      \"NDCG:\\t%f\" % rank_eval.ndcg_at_k(),\n",
    "      \"Precision@K:\\t%f\" % rank_eval.precision_at_k(),\n",
    "      \"Recall@K:\\t%f\" % rank_eval.recall_at_k(), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 Spark - local",
   "language": "python",
   "name": "spark-3-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
