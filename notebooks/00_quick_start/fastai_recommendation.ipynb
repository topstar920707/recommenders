{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastAI Recommender\n",
    "\n",
    "This notebook shows how to use the [FastAI](https://fast.ai) recommender which is using [Pytorch](https://pytorch.org/) under the hood. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.6.0 | packaged by conda-forge | (default, Feb  9 2017, 14:36:55) \n",
      "[GCC 4.8.2 20140120 (Red Hat 4.8.2-15)]\n",
      "Pandas version: 0.23.4\n",
      "Fast AI version: 1.0.39\n",
      "Torch version: 1.0.0\n",
      "Cuda Available: True\n",
      "CuDNN Enabled: True\n"
     ]
    }
   ],
   "source": [
    "import torch, fastai\n",
    "from fastai.collab import *\n",
    "from fastai.tabular import *\n",
    "\n",
    "# set the environment path to find Recommenders\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import time\n",
    "import os\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import papermill as pm\n",
    "\n",
    "from reco_utils.dataset import movielens\n",
    "from reco_utils.dataset.python_splitters import python_random_split\n",
    "from reco_utils.evaluation.python_evaluation import map_at_k, ndcg_at_k, precision_at_k, recall_at_k\n",
    "from reco_utils.evaluation.python_evaluation import rmse, mae, rsquared, exp_var\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Pandas version: {}\".format(pd.__version__))\n",
    "print(\"Fast AI version: {}\".format(fastai.__version__))\n",
    "print(\"Torch version: {}\".format(torch.__version__))\n",
    "print(\"Cuda Available: {}\".format(torch.cuda.is_available()))\n",
    "print(\"CuDNN Enabled: {}\".format(torch.backends.cudnn.enabled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining some constants to refer to the different columns of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER,ITEM,RATING,TIMESTAMP,PREDICTION,TITLE = 'UserId','MovieId','Rating','Timestamp','Prediction','Title'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# top k items to recommend\n",
    "TOP_K = 10\n",
    "\n",
    "# Select Movielens data size: 100k, 1m, 10m, or 20m\n",
    "MOVIELENS_DATA_SIZE = '100k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df = movielens.load_pandas_df(\n",
    "    size=MOVIELENS_DATA_SIZE,\n",
    "    header=[USER,ITEM,RATING,TIMESTAMP]\n",
    ")\n",
    "\n",
    "ratings_df.head()\n",
    "\n",
    "# make sure the IDs are loaded as strings to better prevent confusion with embedding ids\n",
    "ratings_df[USER] = ratings_df[USER].astype('str')\n",
    "ratings_df[ITEM] = ratings_df[ITEM].astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds to make sure out runs are reproducible\n",
    "np.random.seed(101)\n",
    "torch.manual_seed(101)\n",
    "torch.cuda.manual_seed_all(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "data = CollabDataBunch.from_df(ratings_df, pct_val=0.25, user_name=USER, item_name=ITEM, rating_name=RATING)\n",
    "\n",
    "preprocess_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <tr>\n",
       "    <th>UserId</th>\n",
       "    <th>MovieId</th>\n",
       "    <th>target</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>785</th>\n",
       "    <th>423</th>\n",
       "    <th>2.0</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>110</th>\n",
       "    <th>230</th>\n",
       "    <th>3.0</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>72</th>\n",
       "    <th>553</th>\n",
       "    <th>5.0</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>871</th>\n",
       "    <th>202</th>\n",
       "    <th>4.0</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>268</th>\n",
       "    <th>558</th>\n",
       "    <th>3.0</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a `collab_learner` for the data. We will be using 40 latent factors. This will create an embedding for the users and the items that will map each of these to 40 floats as can be seen below. Note that the embedding parameters are not predefined, but are learned by the model.\n",
    "\n",
    "Although ratings can only range from 1-5, we are setting the range of possible ratings to a range from 0 to 5.5 -- that will allow the model to predict values around 1 and 5, which improves accuracy. Lastly, we set a value for weight-decay for regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmbeddingDotBias(\n",
       "  (u_weight): Embedding(944, 40)\n",
       "  (i_weight): Embedding(1628, 40)\n",
       "  (u_bias): Embedding(944, 1)\n",
       "  (i_bias): Embedding(1628, 1)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn = collab_learner(data, n_factors=40, y_range=[0,5.5], wd=1e-1)\n",
    "learn.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the model for 5 epochs setting the maximal learning rate. The learner will reduce the learning rate with each epoch using cosine annealing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 00:31 <p><table style='width:300px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>1</th>\n",
       "    <th>0.933093</th>\n",
       "    <th>0.952120</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>2</th>\n",
       "    <th>0.870814</th>\n",
       "    <th>0.879554</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>3</th>\n",
       "    <th>0.744948</th>\n",
       "    <th>0.835027</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>4</th>\n",
       "    <th>0.646840</th>\n",
       "    <th>0.816982</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>5</th>\n",
       "    <th>0.548141</th>\n",
       "    <th>0.816484</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 31.75955605506897 seconds for training.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "learn.fit_one_cycle(5, max_lr=5e-3)\n",
    "\n",
    "train_time = time.time() - start_time + preprocess_time\n",
    "print(\"Took {} seconds for training.\".format(train_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Recommendations\n",
    "\n",
    "Define two helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cartesian_product(*arrays):\n",
    "    la = len(arrays)\n",
    "    dtype = np.result_type(*arrays)\n",
    "    arr = np.empty([len(a) for a in arrays] + [la], dtype=dtype)\n",
    "    for i, a in enumerate(np.ix_(*arrays)):\n",
    "        arr[...,i] = a\n",
    "    return arr.reshape(-1, la)  \n",
    "\n",
    "def score(learner, userIds, movieIds, user_col, item_col, prediction_col, top_k=0):\n",
    "    \"\"\"score all users+movies provided and reduce to top_k items per user if top_k>0\"\"\"\n",
    "    u = learner.get_idx(userIds, is_item=False)\n",
    "    m = learner.get_idx(movieIds, is_item=True)\n",
    "    \n",
    "    pred = learner.model.forward(u, m)\n",
    "    scores = pd.DataFrame({user_col: userIds, item_col:movieIds, prediction_col:pred})\n",
    "    scores =  scores.sort_values([user_col,prediction_col],ascending=[True,False])\n",
    "    if top_k > 0:\n",
    "        top_scores = scores.groupby(user_col).head(top_k).reset_index(drop=True)\n",
    "    else:\n",
    "        top_scores = scores\n",
    "    return top_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the validation and test sets from the learner's data bunch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = pd.DataFrame({USER:[row.classes[USER][row.cats[0]] for row in learn.data.valid_ds.x], \n",
    "                        ITEM:[row.classes[ITEM][row.cats[1]] for row in learn.data.valid_ds.x], \n",
    "                        RATING: [row.obj for row in data.valid_ds.y]})\n",
    "\n",
    "train_df = pd.DataFrame({USER:[row.classes[USER][row.cats[0]] for row in learn.data.train_ds.x], \n",
    "                        ITEM:[row.classes[ITEM][row.cats[1]] for row in learn.data.train_ds.x], \n",
    "                        RATING: [row.obj for row in data.train_ds.y]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all users from the validation set and all items from the test and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_users = valid_df[USER].unique()\n",
    "_, total_items = data.classes.values()\n",
    "total_items = np.array(total_items[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the cartesian product of users and items to score all items for all users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_items = cartesian_product(np.array(valid_users),np.array(total_items))\n",
    "users_items = pd.DataFrame(users_items, columns=[USER,ITEM])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "\n",
    "Lastly, remove the user/items combinations that are in the training set -- we don't want to propose a movie that the user has already watched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_removed = pd.concat([users_items, train_df[[USER,ITEM]], train_df[[USER,ITEM]]]).drop_duplicates(keep=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score the model to find the top K recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 1.7312288284301758 seconds for 1459261 predictions.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "top_k_scores = score(learn, training_removed[USER], training_removed[ITEM], \n",
    "                     user_col=USER, item_col=ITEM, prediction_col=PREDICTION, top_k=TOP_K)\n",
    "\n",
    "test_time = time.time() - start_time\n",
    "print(\"Took {} seconds for {} predictions.\".format(test_time, len(training_removed)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate some metrics for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_map = map_at_k(valid_df, top_k_scores, col_user=USER, col_item=ITEM, \n",
    "                    col_rating=RATING, col_prediction=PREDICTION, \n",
    "                    relevancy_method=\"top_k\", k=TOP_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_ndcg = ndcg_at_k(valid_df, top_k_scores, col_user=USER, col_item=ITEM, \n",
    "                      col_rating=RATING, col_prediction=PREDICTION, \n",
    "                      relevancy_method=\"top_k\", k=TOP_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_precision = precision_at_k(valid_df, top_k_scores, col_user=USER, col_item=ITEM, \n",
    "                                col_rating=RATING, col_prediction=PREDICTION, \n",
    "                                relevancy_method=\"top_k\", k=TOP_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_recall = recall_at_k(valid_df, top_k_scores, col_user=USER, col_item=ITEM, \n",
    "                          col_rating=RATING, col_prediction=PREDICTION, \n",
    "                          relevancy_method=\"top_k\", k=TOP_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\tCollabLearner\n",
      "Top K:\t10\n",
      "MAP:\t0.024682\n",
      "NDCG:\t0.153864\n",
      "Precision@K:\t0.139236\n",
      "Recall@K:\t0.054513\n"
     ]
    }
   ],
   "source": [
    "print(\"Model:\\t\" + learn.__class__.__name__,\n",
    "      \"Top K:\\t%d\" % TOP_K,\n",
    "      \"MAP:\\t%f\" % eval_map,\n",
    "      \"NDCG:\\t%f\" % eval_ndcg,\n",
    "      \"Precision@K:\\t%f\" % eval_precision,\n",
    "      \"Recall@K:\\t%f\" % eval_recall, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above numbers are lower than SAR, but expected, since the model is explicitly trying to generalize the users and items to the latent factors. Next look at how well the model predicts how the user would rate the movie. Need to score `training_removed` again, but this time don't ask for top_k, but keep all instead. Then merge the validation set that contains the rating with the predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = score(learn, training_removed[USER], training_removed[ITEM], \n",
    "               user_col=USER, item_col=ITEM, prediction_col=PREDICTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculate some regression metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RMSE</td>\n",
       "      <td>0.902846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MAE</td>\n",
       "      <td>0.713283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.363257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ExpVar</td>\n",
       "      <td>0.363808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Metric     Value\n",
       "0    RMSE  0.902846\n",
       "1     MAE  0.713283\n",
       "2      R2  0.363257\n",
       "3  ExpVar  0.363808"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_r2 = rsquared(valid_df, scores, col_user=USER, col_item=ITEM, col_rating=RATING, col_prediction=PREDICTION)\n",
    "eval_rmse = rmse(valid_df, scores, col_user=USER, col_item=ITEM, col_rating=RATING, col_prediction=PREDICTION)\n",
    "eval_mae = mae(valid_df, scores, col_user=USER, col_item=ITEM, col_rating=RATING, col_prediction=PREDICTION)\n",
    "eval_exp_var = exp_var(valid_df, scores, col_user=USER, col_item=ITEM, col_rating=RATING, col_prediction=PREDICTION)\n",
    "\n",
    "pd.DataFrame([\n",
    "    [\"RMSE\",eval_rmse],\n",
    "    [\"MAE\",eval_mae],\n",
    "    [\"R2\",eval_r2],\n",
    "    [\"ExpVar\",eval_exp_var]], \n",
    "    columns=['Metric', 'Value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That RMSE is actually quite good when compared to these benchmarks: https://www.librec.net/release/v1.3/example.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/papermill.record+json": {
       "map": 0.024682329778020824
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/papermill.record+json": {
       "ndcg": 0.1538638096682185
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/papermill.record+json": {
       "precision": 0.13923647932131497
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/papermill.record+json": {
       "recall": 0.05451313193266867
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/papermill.record+json": {
       "rmse": 0.9028457366673833
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/papermill.record+json": {
       "mae": 0.7132825352687198
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/papermill.record+json": {
       "exp_var": 0.36380808012398935
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/papermill.record+json": {
       "rsquared": 0.36325670187345405
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/papermill.record+json": {
       "train_time": 31.75955605506897
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/papermill.record+json": {
       "test_time": 1.7312288284301758
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Record results with papermill for tests\n",
    "pm.record(\"map\", eval_map)\n",
    "pm.record(\"ndcg\", eval_ndcg)\n",
    "pm.record(\"precision\", eval_precision)\n",
    "pm.record(\"recall\", eval_recall)\n",
    "pm.record(\"rmse\", eval_rmse)\n",
    "pm.record(\"mae\", eval_mae)\n",
    "pm.record(\"exp_var\", eval_exp_var)\n",
    "pm.record(\"rsquared\", eval_r2)\n",
    "pm.record(\"train_time\", train_time)\n",
    "pm.record(\"test_time\", test_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python (bare)",
   "language": "python",
   "name": "bar"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
