{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM: A Highly Efficient Gradient Boosting Decision Tree\n",
    "This notebook will give you a quick example of how to train LightGBM model in a recommendation scenario. \n",
    "LightGBM \\[1\\] is a gradient boosting framework that uses tree-based learning algorithms. It is designed to be distributed and efficient with the following advantages:\n",
    "* Faster training speed and higher efficiency.\n",
    "* Lower memory usage.\n",
    "* Better accuracy.\n",
    "* Support of parallel and GPU learning.\n",
    "* Capable of handling large-scale data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Settings and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34) \n",
      "[GCC 7.3.0]\n",
      "LightGBM version: 2.2.1\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(\"../../\")\n",
    "import lightgbm as lgb\n",
    "import papermill as pm\n",
    "import pandas as pd\n",
    "import category_encoders as ce\n",
    "import reco_utils.recommender.lightgbm.lightgbm_utils as lgb_utils\n",
    "import reco_utils.dataset.criteo_dac as criteo_dac\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"LightGBM version: {}\".format(lgb.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Setting\n",
    "Let's set the main related parameters for LightGBM now. Basically, the task is a binary classification (predicting click or no click), so the objective function is set to binary logloss, and 'AUC' metric, is used as a metric which is less effected by imbalance in the classes of the dataset.\n",
    "\n",
    "Generally, we can adjust the number of leaves (MAX_LEAF), the minimum number of data in each leaf (MIN_DATA), maximum number of trees (NUM_OF_TREES), the learning rate of trees (TREE_LEARINING_RATE) and EARLY_STOPPING_ROUNDS (to avoid overfitting) in the model to get better performance.\n",
    "\n",
    "Besides, we can also adjust some other listed parameters in the following to optimize the results, which are shown in [5] concretely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "MAX_LEAF = 64\n",
    "MIN_DATA = 20\n",
    "NUM_OF_TREES = 100\n",
    "TREE_LEARNING_RATE = 0.15\n",
    "EARLY_STOPPING_ROUNDS = 20\n",
    "METRIC = \"auc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_class': 1,\n",
    "    'objective': \"binary\",\n",
    "    'metric': METRIC,\n",
    "    'num_leaves': MAX_LEAF,\n",
    "    'min_data': MIN_DATA,\n",
    "    'boost_from_average': True,\n",
    "    #set it according to your cpu cores.\n",
    "    'num_threads': 20,\n",
    "    'feature_fraction': 0.8,\n",
    "    'learning_rate': TREE_LEARNING_RATE,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "Here we use CSV format as the example data input. Our example data is a sample (about 100 thousand samples) from Criteo dataset [2]. The Criteo dataset is a well-known industry benchmarking dataset for developing CTR prediction models, and it's frequently adopted as evaluation dataset by research papers. The original dataset is too large for a lightweight demo, so we sample a small portion from it as a demo dataset. <br>\n",
    "Specifically, there are 39 columns of features in Criteo, where 13 columns are numerical features (I1-I13) and the other 26 columns are categorical features (C1-C26)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we cut three sets (train_data (first 80%), valid_data (middle 10%) and test_data (last 10%)), cut from the original all data. <br>\n",
    "Notably, considering the Criteo is a kind of time-series streaming data, which is also very common in recommendation scenario, we split the data by its order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting component files from /tmp/tmpz0rodvbn/dac_sample.tar.gz.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>I1</th>\n",
       "      <th>I2</th>\n",
       "      <th>I3</th>\n",
       "      <th>I4</th>\n",
       "      <th>I5</th>\n",
       "      <th>I6</th>\n",
       "      <th>I7</th>\n",
       "      <th>I8</th>\n",
       "      <th>I9</th>\n",
       "      <th>...</th>\n",
       "      <th>C17</th>\n",
       "      <th>C18</th>\n",
       "      <th>C19</th>\n",
       "      <th>C20</th>\n",
       "      <th>C21</th>\n",
       "      <th>C22</th>\n",
       "      <th>C23</th>\n",
       "      <th>C24</th>\n",
       "      <th>C25</th>\n",
       "      <th>C26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>90000</th>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2</td>\n",
       "      <td>47.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>...</td>\n",
       "      <td>e5ba7672</td>\n",
       "      <td>cbae5931</td>\n",
       "      <td>3014a4b1</td>\n",
       "      <td>5840adea</td>\n",
       "      <td>b5f914e8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32c7478e</td>\n",
       "      <td>b2f178a3</td>\n",
       "      <td>001f3601</td>\n",
       "      <td>938732a0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90001</th>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>537.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>...</td>\n",
       "      <td>e5ba7672</td>\n",
       "      <td>124c6b00</td>\n",
       "      <td>21ddcdc9</td>\n",
       "      <td>5840adea</td>\n",
       "      <td>99c09e97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32c7478e</td>\n",
       "      <td>335a6a1e</td>\n",
       "      <td>ea9a246c</td>\n",
       "      <td>68a2a837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90002</th>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>800.0</td>\n",
       "      <td>...</td>\n",
       "      <td>e5ba7672</td>\n",
       "      <td>449d6705</td>\n",
       "      <td>21ddcdc9</td>\n",
       "      <td>a458ea53</td>\n",
       "      <td>15415fcf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>423fab69</td>\n",
       "      <td>3fdb382b</td>\n",
       "      <td>e8b83407</td>\n",
       "      <td>49d68486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90003</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58</td>\n",
       "      <td>17.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>33040.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>e5ba7672</td>\n",
       "      <td>1cdbd1c5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d9d9202f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32c7478e</td>\n",
       "      <td>8fc66e78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90004</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>d4bb7bd8</td>\n",
       "      <td>e4ca448c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d7c5ddf2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32c7478e</td>\n",
       "      <td>9117a34a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Label    I1  I2    I3    I4       I5    I6    I7    I8     I9  ...  \\\n",
       "90000      0   9.0   2  47.0  32.0      5.0  32.0   9.0  27.0   32.0  ...   \n",
       "90001      0  10.0   1  10.0  20.0    537.0  41.0  10.0   8.0   41.0  ...   \n",
       "90002      0   3.0   1   5.0  21.0     10.0   8.0   9.0   0.0  800.0  ...   \n",
       "90003      0   NaN  58  17.0   8.0  33040.0   NaN   0.0  12.0    9.0  ...   \n",
       "90004      0   1.0  18   5.0   0.0     92.0  21.0   2.0   1.0   15.0  ...   \n",
       "\n",
       "            C17       C18       C19       C20       C21  C22       C23  \\\n",
       "90000  e5ba7672  cbae5931  3014a4b1  5840adea  b5f914e8  NaN  32c7478e   \n",
       "90001  e5ba7672  124c6b00  21ddcdc9  5840adea  99c09e97  NaN  32c7478e   \n",
       "90002  e5ba7672  449d6705  21ddcdc9  a458ea53  15415fcf  NaN  423fab69   \n",
       "90003  e5ba7672  1cdbd1c5       NaN       NaN  d9d9202f  NaN  32c7478e   \n",
       "90004  d4bb7bd8  e4ca448c       NaN       NaN  d7c5ddf2  NaN  32c7478e   \n",
       "\n",
       "            C24       C25       C26  \n",
       "90000  b2f178a3  001f3601  938732a0  \n",
       "90001  335a6a1e  ea9a246c  68a2a837  \n",
       "90002  3fdb382b  e8b83407  49d68486  \n",
       "90003  8fc66e78       NaN       NaN  \n",
       "90004  9117a34a       NaN       NaN  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nume_cols = ['I'+str(i) for i in range(1, 14)]\n",
    "cate_cols = ['C'+str(i) for i in range(1, 27)]\n",
    "label_col = 'Label'\n",
    "\n",
    "from tempfile import TemporaryDirectory\n",
    "with TemporaryDirectory() as tmp:\n",
    "    file_path = os.path.join(tmp, r'dac_sample.tar.gz')\n",
    "    all_data = criteo_dac.load_pandas_df(file_path, label_col, nume_cols, cate_cols)\n",
    "# split data to 3 sets    \n",
    "length = len(all_data)\n",
    "train_data = all_data.loc[:0.8*length-1]\n",
    "valid_data = all_data.loc[0.8*length:0.9*length-1]\n",
    "test_data = all_data.loc[0.9*length:]\n",
    "display(test_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Usage\n",
    "### Ordinal Encoding\n",
    "Considering LightGBM could handle the low-frequency features and missing value by itself, for basic usage, we only encode the string-like categorical features by an ordinal encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape: X: (80000, 39); Y: (80000,).\n",
      "Valid Data Shape: X: (10000, 39); Y: (10000,).\n",
      "Test Data Shape: X: (10000, 39); Y: (10000,).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_col = 'Label'\n",
    "ord_encoder = ce.ordinal.OrdinalEncoder(cols=cate_cols)\n",
    "\n",
    "def encode_csv(df, encoder, label_col, typ='fit'):\n",
    "    if typ == 'fit':\n",
    "        df = encoder.fit_transform(df)\n",
    "    else:\n",
    "        df = encoder.transform(df)\n",
    "    y = df[label_col].values\n",
    "    del df[label_col]\n",
    "    return df, y\n",
    "\n",
    "train_x, train_y = encode_csv(train_data, ord_encoder, label_col)\n",
    "valid_x, valid_y = encode_csv(valid_data, ord_encoder, label_col, 'transform')\n",
    "test_x, test_y = encode_csv(test_data, ord_encoder, label_col, 'transform')\n",
    "\n",
    "print('Train Data Shape: X: {trn_x_shape}; Y: {trn_y_shape}.\\nValid Data Shape: X: {vld_x_shape}; Y: {vld_y_shape}.\\nTest Data Shape: X: {tst_x_shape}; Y: {tst_y_shape}.\\n'\n",
    "      .format(trn_x_shape=train_x.shape,\n",
    "              trn_y_shape=train_y.shape,\n",
    "              vld_x_shape=valid_x.shape,\n",
    "              vld_y_shape=valid_y.shape,\n",
    "              tst_x_shape=test_x.shape,\n",
    "              tst_y_shape=test_y.shape,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model\n",
    "When both hyper-parameters and data are ready, we can create a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's auc: 0.728695\n",
      "Training until validation scores don't improve for 20 rounds.\n",
      "[2]\tvalid_0's auc: 0.742373\n",
      "[3]\tvalid_0's auc: 0.747298\n",
      "[4]\tvalid_0's auc: 0.747969\n",
      "[5]\tvalid_0's auc: 0.751102\n",
      "[6]\tvalid_0's auc: 0.753734\n",
      "[7]\tvalid_0's auc: 0.755335\n",
      "[8]\tvalid_0's auc: 0.75658\n",
      "[9]\tvalid_0's auc: 0.757071\n",
      "[10]\tvalid_0's auc: 0.758572\n",
      "[11]\tvalid_0's auc: 0.759742\n",
      "[12]\tvalid_0's auc: 0.760415\n",
      "[13]\tvalid_0's auc: 0.760602\n",
      "[14]\tvalid_0's auc: 0.761192\n",
      "[15]\tvalid_0's auc: 0.7616\n",
      "[16]\tvalid_0's auc: 0.761697\n",
      "[17]\tvalid_0's auc: 0.762255\n",
      "[18]\tvalid_0's auc: 0.76253\n",
      "[19]\tvalid_0's auc: 0.763092\n",
      "[20]\tvalid_0's auc: 0.762172\n",
      "[21]\tvalid_0's auc: 0.762066\n",
      "[22]\tvalid_0's auc: 0.761866\n",
      "[23]\tvalid_0's auc: 0.761433\n",
      "[24]\tvalid_0's auc: 0.761588\n",
      "[25]\tvalid_0's auc: 0.761017\n",
      "[26]\tvalid_0's auc: 0.761086\n",
      "[27]\tvalid_0's auc: 0.761177\n",
      "[28]\tvalid_0's auc: 0.760893\n",
      "[29]\tvalid_0's auc: 0.760635\n",
      "[30]\tvalid_0's auc: 0.760104\n",
      "[31]\tvalid_0's auc: 0.759298\n",
      "[32]\tvalid_0's auc: 0.759176\n",
      "[33]\tvalid_0's auc: 0.758384\n",
      "[34]\tvalid_0's auc: 0.758168\n",
      "[35]\tvalid_0's auc: 0.757902\n",
      "[36]\tvalid_0's auc: 0.758005\n",
      "[37]\tvalid_0's auc: 0.757782\n",
      "[38]\tvalid_0's auc: 0.757542\n",
      "[39]\tvalid_0's auc: 0.756966\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid_0's auc: 0.763092\n"
     ]
    }
   ],
   "source": [
    "lgb_train = lgb.Dataset(train_x, train_y.reshape(-1), params=params, categorical_feature=cate_cols)\n",
    "lgb_valid = lgb.Dataset(valid_x, valid_y.reshape(-1), reference=lgb_train, categorical_feature=cate_cols)\n",
    "lgb_test = lgb.Dataset(test_x, test_y.reshape(-1), reference=lgb_train, categorical_feature=cate_cols)\n",
    "lgb_model = lgb.train(params,\n",
    "                      lgb_train,\n",
    "                      num_boost_round=NUM_OF_TREES,\n",
    "                      early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "                      valid_sets=lgb_valid,\n",
    "                      categorical_feature=cate_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what is the model's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'auc': 0.7674, 'logloss': 0.4669}\n"
     ]
    },
    {
     "data": {
      "application/papermill.record+json": {
       "res_basic": {
        "auc": 0.7674,
        "logloss": 0.4669
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_preds = lgb_model.predict(test_x)\n",
    "res_basic = lgb_utils.cal_metric(test_y.reshape(-1), test_preds, ['auc','logloss'])\n",
    "print(res_basic)\n",
    "pm.record(\"res_basic\", res_basic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n",
    "## Optimized Usage\n",
    "### Label-encoding and Binary-encoding\n",
    "Next, since LightGBM has a better capability in handling dense numerical features effectively, we try to convert all the categorical features in original data into numerical ones, by label-encoding [3] and binary-encoding [4]. Also due to the sequence property of Criteo, the label-encoding we adopted is executed one-by-one, which means we encode the samples in order, by the information of the previous samples before each sample (sequential label-encoding and sequential count-encoding). Besides, we also filter the low-frequency categorical features and fill the missing values by the mean of corresponding columns for the numerical features. (consulting `lgb_utils.NumEncoder`)\n",
    "\n",
    "Specifically, in `lgb_utils.NumEncoder`, the main steps are as follows.\n",
    "* Firstly, we convert the low-frequency categorical features to \"LESS\" and the missing categorical features to \"UNK\". \n",
    "* Secondly, we convert the missing numerical features into the mean of corresponding columns. \n",
    "* Thirdly, the string-like categorical features are ordinal encoded like the example shown in basic usage. \n",
    "* And then, we target encode the categorical features in the samples order one-by-one. For each sample, we add the label and count information of its former samples into the data and produce new features. Formally, for $i=1,2,...,n$, we add $\\frac{\\sum\\nolimits_{j=1}^{i-1} I(x_j=c) \\cdot y}{\\sum\\nolimits_{j=1}^{i-1} I(x_j=c)}$ as a new label feature for current sample $x_i$, where $c$ is a category to encode in current sample, so $(i-1)$ is the number of former samples, and $I(\\cdot)$ is the indicator function that check the former samples contain $c$ (whether $x_j=c$) or not. At the meantime, we also add the count frequency of $c$, which is $\\frac{\\sum\\nolimits_{j=1}^{i-1} I(x_j=c)}{i-1}$, as a new count feature. \n",
    "* Finally, based on the results of ordinal encoding, we add the binary encoding results as new columns into the data.\n",
    "\n",
    "Note that the statistics used in the above process only updates when fitting the training set, while maintaining static when transforming the testing set because the label of test data should be considered as unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-12 07:55:56,415 [INFO] Filtering and fillna features\n",
      "100%|██████████| 26/26 [00:02<00:00, 12.67it/s]\n",
      "100%|██████████| 13/13 [00:00<00:00, 604.84it/s]\n",
      "2019-03-12 07:55:58,494 [INFO] Ordinal encoding cate features\n",
      "2019-03-12 07:55:59,943 [INFO] Target encoding cate features\n",
      "100%|██████████| 26/26 [00:03<00:00,  6.48it/s]\n",
      "2019-03-12 07:56:03,878 [INFO] Start manual binary encoding\n",
      "100%|██████████| 65/65 [00:03<00:00, 16.50it/s]\n",
      "100%|██████████| 26/26 [00:02<00:00,  7.86it/s]\n",
      "2019-03-12 07:56:10,790 [INFO] Filtering and fillna features\n",
      "100%|██████████| 26/26 [00:00<00:00, 167.89it/s]\n",
      "100%|██████████| 13/13 [00:00<00:00, 1874.97it/s]\n",
      "2019-03-12 07:56:10,956 [INFO] Ordinal encoding cate features\n",
      "2019-03-12 07:56:11,120 [INFO] Target encoding cate features\n",
      "100%|██████████| 26/26 [00:00<00:00, 52.55it/s]\n",
      "2019-03-12 07:56:11,618 [INFO] Start manual binary encoding\n",
      "100%|██████████| 65/65 [00:03<00:00, 21.43it/s]\n",
      "100%|██████████| 26/26 [00:01<00:00, 18.60it/s]\n",
      "2019-03-12 07:56:16,094 [INFO] Filtering and fillna features\n",
      "100%|██████████| 26/26 [00:00<00:00, 151.00it/s]\n",
      "100%|██████████| 13/13 [00:00<00:00, 2127.93it/s]\n",
      "2019-03-12 07:56:16,288 [INFO] Ordinal encoding cate features\n",
      "2019-03-12 07:56:16,453 [INFO] Target encoding cate features\n",
      "100%|██████████| 26/26 [00:00<00:00, 52.20it/s]\n",
      "2019-03-12 07:56:16,953 [INFO] Start manual binary encoding\n",
      "100%|██████████| 65/65 [00:03<00:00, 21.40it/s]\n",
      "100%|██████████| 26/26 [00:01<00:00, 18.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape: X: (80000, 268); Y: (80000, 1).\n",
      "Valid Data Shape: X: (10000, 268); Y: (10000, 1).\n",
      "Test Data Shape: X: (10000, 268); Y: (10000, 1).\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "label_col = 'Label'\n",
    "num_encoder = lgb_utils.NumEncoder(cate_cols, nume_cols, label_col)\n",
    "train_x, train_y = num_encoder.fit_transform(train_data)\n",
    "valid_x, valid_y = num_encoder.transform(valid_data)\n",
    "test_x, test_y = num_encoder.transform(test_data)\n",
    "del num_encoder\n",
    "print('Train Data Shape: X: {trn_x_shape}; Y: {trn_y_shape}.\\nValid Data Shape: X: {vld_x_shape}; Y: {vld_y_shape}.\\nTest Data Shape: X: {tst_x_shape}; Y: {tst_y_shape}.\\n'\n",
    "      .format(trn_x_shape=train_x.shape,\n",
    "              trn_y_shape=train_y.shape,\n",
    "              vld_x_shape=valid_x.shape,\n",
    "              vld_y_shape=valid_y.shape,\n",
    "              tst_x_shape=test_x.shape,\n",
    "              tst_y_shape=test_y.shape,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's auc: 0.731759\n",
      "Training until validation scores don't improve for 20 rounds.\n",
      "[2]\tvalid_0's auc: 0.747705\n",
      "[3]\tvalid_0's auc: 0.751667\n",
      "[4]\tvalid_0's auc: 0.75589\n",
      "[5]\tvalid_0's auc: 0.758054\n",
      "[6]\tvalid_0's auc: 0.758094\n",
      "[7]\tvalid_0's auc: 0.759904\n",
      "[8]\tvalid_0's auc: 0.761098\n",
      "[9]\tvalid_0's auc: 0.761744\n",
      "[10]\tvalid_0's auc: 0.762308\n",
      "[11]\tvalid_0's auc: 0.762473\n",
      "[12]\tvalid_0's auc: 0.763606\n",
      "[13]\tvalid_0's auc: 0.764222\n",
      "[14]\tvalid_0's auc: 0.765004\n",
      "[15]\tvalid_0's auc: 0.765933\n",
      "[16]\tvalid_0's auc: 0.766507\n",
      "[17]\tvalid_0's auc: 0.767192\n",
      "[18]\tvalid_0's auc: 0.767284\n",
      "[19]\tvalid_0's auc: 0.767859\n",
      "[20]\tvalid_0's auc: 0.768619\n",
      "[21]\tvalid_0's auc: 0.769045\n",
      "[22]\tvalid_0's auc: 0.768987\n",
      "[23]\tvalid_0's auc: 0.769601\n",
      "[24]\tvalid_0's auc: 0.77011\n",
      "[25]\tvalid_0's auc: 0.770183\n",
      "[26]\tvalid_0's auc: 0.770539\n",
      "[27]\tvalid_0's auc: 0.77096\n",
      "[28]\tvalid_0's auc: 0.771164\n",
      "[29]\tvalid_0's auc: 0.771296\n",
      "[30]\tvalid_0's auc: 0.771402\n",
      "[31]\tvalid_0's auc: 0.771596\n",
      "[32]\tvalid_0's auc: 0.771476\n",
      "[33]\tvalid_0's auc: 0.771697\n",
      "[34]\tvalid_0's auc: 0.77169\n",
      "[35]\tvalid_0's auc: 0.771836\n",
      "[36]\tvalid_0's auc: 0.771832\n",
      "[37]\tvalid_0's auc: 0.771948\n",
      "[38]\tvalid_0's auc: 0.772098\n",
      "[39]\tvalid_0's auc: 0.772136\n",
      "[40]\tvalid_0's auc: 0.771748\n",
      "[41]\tvalid_0's auc: 0.771748\n",
      "[42]\tvalid_0's auc: 0.771724\n",
      "[43]\tvalid_0's auc: 0.771676\n",
      "[44]\tvalid_0's auc: 0.77169\n",
      "[45]\tvalid_0's auc: 0.771916\n",
      "[46]\tvalid_0's auc: 0.771864\n",
      "[47]\tvalid_0's auc: 0.771851\n",
      "[48]\tvalid_0's auc: 0.771891\n",
      "[49]\tvalid_0's auc: 0.771629\n",
      "[50]\tvalid_0's auc: 0.771984\n",
      "[51]\tvalid_0's auc: 0.772085\n",
      "[52]\tvalid_0's auc: 0.771988\n",
      "[53]\tvalid_0's auc: 0.771778\n",
      "[54]\tvalid_0's auc: 0.771485\n",
      "[55]\tvalid_0's auc: 0.7715\n",
      "[56]\tvalid_0's auc: 0.770778\n",
      "[57]\tvalid_0's auc: 0.770816\n",
      "[58]\tvalid_0's auc: 0.770408\n",
      "[59]\tvalid_0's auc: 0.770489\n",
      "Early stopping, best iteration is:\n",
      "[39]\tvalid_0's auc: 0.772136\n"
     ]
    }
   ],
   "source": [
    "lgb_train = lgb.Dataset(train_x, train_y.reshape(-1), params=params)\n",
    "lgb_valid = lgb.Dataset(valid_x, valid_y.reshape(-1), reference=lgb_train)\n",
    "lgb_model = lgb.train(params,\n",
    "                      lgb_train,\n",
    "                      num_boost_round=NUM_OF_TREES,\n",
    "                      early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "                      valid_sets=lgb_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'auc': 0.7757, 'logloss': 0.4607}\n"
     ]
    },
    {
     "data": {
      "application/papermill.record+json": {
       "res_optim": {
        "auc": 0.7757,
        "logloss": 0.4607
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_preds = lgb_model.predict(test_x)\n",
    "res_optim = lgb_utils.cal_metric(test_y.reshape(-1), test_preds, ['auc','logloss'])\n",
    "print(res_optim)\n",
    "pm.record(\"res_optim\", res_optim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model saving and loading\n",
    "Now we finish the basic training and testing for LightGBM, next let's try to save and reload the model, and then evaluate it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'auc': 0.7757, 'logloss': 0.4607}\n"
     ]
    }
   ],
   "source": [
    "from tempfile import TemporaryDirectory\n",
    "with TemporaryDirectory() as tmp:\n",
    "    save_file = os.path.join(tmp, r'finished.model')\n",
    "    lgb_model.save_model(save_file)\n",
    "    loaded_model = lgb.Booster(model_file=save_file)\n",
    "\n",
    "# eval the performance again\n",
    "test_preds = loaded_model.predict(test_x)\n",
    "print(lgb_utils.cal_metric(test_y.reshape(-1), test_preds, ['auc','logloss']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\\[1\\] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. 2017. LightGBM: A highly efficient gradient boosting decision tree. In Advances in Neural Information Processing Systems. 3146–3154.<br>\n",
    "\\[2\\] The Criteo datasets: http://labs.criteo.com/wp-content/uploads/2015/04/dac_sample.tar.gz .<br>\n",
    "\\[3\\] Anna Veronika Dorogush, Vasily Ershov, and Andrey Gulin. 2018. CatBoost: gradient boosting with categorical features support. arXiv preprint arXiv:1810.11363 (2018).<br>\n",
    "\\[4\\] Scikit-learn. 2018. categorical_encoding. https://github.com/scikit-learn-contrib/categorical-encoding .<br>\n",
    "\\[5\\] The parameters of LightGBM: https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters.rst ."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python (reco)",
   "language": "python",
   "name": "reco_base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
