{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running SAR on MovieLens (Python, CPU)\n",
    "\n",
    "SAR is a fast scalable adaptive algorithm for personalized recommendations based on user transaction history and item descriptions. It produces easily explainable / interpretable recommendations and handles \"cold item\" and \"semi-cold user\" scenarios. \n",
    "\n",
    "SAR recommends items that are most ***similar*** to the ones that the user already has an existing ***affinity*** for. Two items are ***similar*** if the users who have interacted with one item are also likely to have interacted with another. A user has an ***affinity*** to an item if they have interacted with it in the past.\n",
    "\n",
    "### SAR has the following advantages:\n",
    "- High accuracy for an easy to train and deploy algorithm\n",
    "- Fast training, only requiring simple counting to construct matrices used at prediction time. \n",
    "- Fast scoring, only involving multiplication of the similarity matric with an affinity vector\n",
    "\n",
    "### And the following drawbacks:\n",
    "- Since it does not use item or user features, it can be at a disadvantage against algorithms that do.\n",
    "- It's memory-hungry, requiring the creation of an $m√óm$ square matrix (where $m$ is the number of items).\n",
    "- Only outputs a SAR score (rather than an estimated rating).\n",
    "\n",
    "This notebook provides an example of how to utilize and evaluate SAR in Python on a CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)]\n",
      "Pandas version: 0.23.0\n"
     ]
    }
   ],
   "source": [
    "# set the environment path to find Recommenders\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from utilities.recommender.sar.sar_singlenode import SARSingleNodeReference\n",
    "from utilities.dataset.url_utils import maybe_download\n",
    "from utilities.dataset.python_splitters import python_random_split\n",
    "from utilities.evaluation.python_evaluation import PythonRatingEvaluation, PythonRankingEvaluation\n",
    "\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Pandas version: {}\".format(pd.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Download the MovieLens dataset\n",
    "\n",
    "SAR is intended to be used on interactions with the following schema:\n",
    "`<User ID>, <Item ID>,<Time>,[<Event Type>], [<Event Weight>]`. \n",
    "\n",
    "Each row represents a single interaction between a user and an item. The MovieLens dataset is well formatted interactions of Users providing Ratings to Movies - we will use it for the rest of the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = maybe_download(\"http://files.grouplens.org/datasets/movielens/ml-100k/u.data\", \"ml-100k.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserId</th>\n",
       "      <th>MovieId</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>881250949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>878887116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>886397596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserId  MovieId  Rating  Timestamp\n",
       "0     196      242       3  881250949\n",
       "1     186      302       3  891717742\n",
       "2      22      377       1  878887116\n",
       "3     244       51       2  880606923\n",
       "4     166      346       1  886397596"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(filepath, sep=\"\\t\", names=[\"UserId\", \"MovieId\", \"Rating\", \"Timestamp\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split the data using the python random splitter provided in utilities:\n",
    "\n",
    "We utilize the provided `python_random_split` function to split into `train` and `test` datasets randomly at a 80/20 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = python_random_split(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Instantiate the SAR algorithm and set the index\n",
    "\n",
    "In order to use SAR, we need to hash users and items and set an index in order for SAR matrix operations to work. First we instantiate SAR using the reference implementation provided in `SARSingleNodeReference`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = {\n",
    "        \"col_user\": \"UserId\",\n",
    "        \"col_item\": \"MovieId\",\n",
    "        \"col_rating\": \"Rating\",\n",
    "        \"col_timestamp\": \"Timestamp\",\n",
    "    }\n",
    "\n",
    "model = SARSingleNodeReference(\n",
    "                remove_seen=True, similarity_type=\"jaccard\", \n",
    "                time_decay_coefficient=30, time_now=None, timedecay_formula=True, **header\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_users = data[\"UserId\"].unique()\n",
    "unique_items = data[\"MovieId\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will hash users and items to smaller continuous space.\n",
    "This is an ordered set - it's discrete, but contiguous.\n",
    "This helps keep the matrices we keep in memory as small as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "enumerate_items_1, enumerate_items_2 = itertools.tee(enumerate(unique_items))\n",
    "enumerate_users_1, enumerate_users_2 = itertools.tee(enumerate(unique_users))\n",
    "item_map_dict = {x: i for i, x in enumerate_items_1}\n",
    "user_map_dict = {x: i for i, x in enumerate_users_1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reverse of the dictionary above - array index to actual ID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2user = dict(enumerate_users_2)\n",
    "index2item = dict(enumerate_items_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to index the train and test sets for SAR matrix operations to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_index(unique_users, unique_items, user_map_dict, item_map_dict, index2user, index2item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train the SAR model on our training data, and get the top-k recommendations for our testing data\n",
    "\n",
    "SAR first computes an item-to-item ***co-occurence matrix***. Co-occurence represents the number of times two items appear together for any given user. Once we have the co-occurence matrix, we compute an ***item similarity matrix*** by rescaling the cooccurences by a given metric (Jaccard similarity in this example). \n",
    "\n",
    "We also compute an ***affinity matrix*** to capture the strength of the relationship between each user and each item. Affinity is driven by different types (like *rating* or *viewing* a movie), and by the time of the event. \n",
    "\n",
    "Personalized recommendations are achieved by multiplying the affinity matrix $A$ and the similarity matrix $S$. The result is a ***recommendation score matrix*** $R$. We do not compute the full matrix $R$ to reduce memory requirements: we compute the ***top-k*** results for each user in the `recommend_k_items` function seen below.\n",
    "\n",
    "A full walkthrough of the SAR algorithm can be found [here](../02_modeling/sar_educational_walkthrough.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utilities.recommender.sar.sar_singlenode:Collecting user affinity matrix...\n",
      "INFO:utilities.recommender.sar.sar_singlenode:Calculating time-decayed affinities...\n",
      "../..\\utilities\\recommender\\sar\\sar_singlenode.py:214: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df[\"exponential\"] = expo_fun(df[self.col_timestamp].values)\n",
      "../..\\utilities\\recommender\\sar\\sar_singlenode.py:216: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df[\"rating_exponential\"] = df[self.col_rating] * df[\"exponential\"]\n",
      "INFO:utilities.recommender.sar.sar_singlenode:Creating index columns...\n",
      "../..\\utilities\\recommender\\sar\\sar_singlenode.py:278: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  self.index = df.as_matrix([self._col_hashed_users, self._col_hashed_items])\n",
      "INFO:utilities.recommender.sar.sar_singlenode:Building user affinity sparse matrix...\n",
      "INFO:utilities.recommender.sar.sar_singlenode:Calculating item cooccurrence...\n",
      "INFO:utilities.recommender.sar.sar_singlenode:Calculating item similarity...\n",
      "INFO:utilities.recommender.sar.sar_singlenode:Calculating jaccard...\n",
      "C:\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py:594: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.true_divide(self.todense(), other)\n",
      "INFO:utilities.recommender.sar.sar_singlenode:Calculating recommendation scores...\n",
      "INFO:utilities.recommender.sar.sar_singlenode:done training\n",
      "INFO:utilities.recommender.sar.sar_singlenode:Converting to dense matrix...\n",
      "../..\\utilities\\recommender\\sar\\sar_singlenode.py:414: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  test[self._col_hashed_users] = test[self.col_user].map(self.user_map_dict)\n",
      "INFO:utilities.recommender.sar.sar_singlenode:Removing seen items...\n",
      "INFO:utilities.recommender.sar.sar_singlenode:Getting top K...\n",
      "INFO:utilities.recommender.sar.sar_singlenode:Select users from the test set\n",
      "INFO:utilities.recommender.sar.sar_singlenode:Creating output dataframe...\n",
      "INFO:utilities.recommender.sar.sar_singlenode:Formatting output\n"
     ]
    }
   ],
   "source": [
    "model.fit(train)\n",
    "top_k = model.recommend_k_items(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: remove this call when the model returns same type as input\n",
    "top_k['UserId'] = pd.to_numeric(top_k['UserId'])\n",
    "top_k['MovieId'] = pd.to_numeric(top_k['MovieId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserId</th>\n",
       "      <th>MovieId</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5787</th>\n",
       "      <td>796</td>\n",
       "      <td>234</td>\n",
       "      <td>155.419422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5786</th>\n",
       "      <td>796</td>\n",
       "      <td>216</td>\n",
       "      <td>154.755182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5789</th>\n",
       "      <td>796</td>\n",
       "      <td>174</td>\n",
       "      <td>154.093210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5788</th>\n",
       "      <td>796</td>\n",
       "      <td>566</td>\n",
       "      <td>153.102244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5785</th>\n",
       "      <td>796</td>\n",
       "      <td>79</td>\n",
       "      <td>152.824891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      UserId  MovieId  prediction\n",
       "5787     796      234  155.419422\n",
       "5786     796      216  154.755182\n",
       "5789     796      174  154.093210\n",
       "5788     796      566  153.102244\n",
       "5785     796       79  152.824891"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(top_k.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluate how well SAR performs \n",
    "\n",
    "We evaluate how well SAR performs with for a few common ranking metrics provided in the `PythonRankingEvaluation` classs in utilities. We will consider the Mean Average Precision (MAP), Normalized Discounted Cumalative Gain (NDCG), Precision, and Recall for the top-k items per user we computed with SAR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_eval = PythonRankingEvaluation(test, top_k, col_user=\"UserId\", col_item=\"MovieId\", \n",
    "                                    col_rating=\"Rating\", col_prediction=\"prediction\", \n",
    "                                    relevancy_method=\"top_k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\tsar_ref\n",
      "Top K:\t10\n",
      "MAP@k:\t0.105815\n",
      "NDCG@k:\t0.373197\n",
      "Precision@k:\t0.326617\n",
      "Recall@k:\t0.175957\n"
     ]
    }
   ],
   "source": [
    "print(\"Model:\\t\" + model.model_str,\n",
    "      \"Top K:\\t%d\" % rank_eval.top_k,\n",
    "      \"MAP@k:\\t%f\" % rank_eval.map_at_k(),\n",
    "      \"NDCG@k:\\t%f\" % rank_eval.ndcg_at_k(),\n",
    "      \"Precision@k:\\t%f\" % rank_eval.precision_at_k(),\n",
    "      \"Recall@k:\\t%f\" % rank_eval.recall_at_k(), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (reco)",
   "language": "python",
   "name": "reco"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
