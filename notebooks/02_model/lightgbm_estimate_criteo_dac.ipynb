{"cells":[{"cell_type":"code","source":["## parameters:\n\n#################################\n## data parameters:\n#################################\n\n## number of rows to process\nnum_rows = 10**7\n## number of categorical variables; will process categorical variables 0:n_sparse_features\nn_sparse_features = 26\n## number of numeric features; numeric variables 0:n_num_features \nn_num_features = 13\n\n#################################\n## Feature Engineering parameters:\n#################################\n\n# categorical_var_strategy = 'targ-enc' ## options are 'string-index' or 'targ-enc'\n# replace_small_levels = False\ncategorical_var_strategy = 'string-index' ## options are 'string-index' or 'targ-enc'\nreplace_small_levels = True\nsmall_level_freq_thresh = 10 ## same threshold used by winners: https://www.csie.ntu.edu.tw/~r01922136/kaggle-2014-criteo.pdf\n## constructed variables:\nfile_tail = 'try2'\ntable_to_save = 'criteo_dac_proc_{}sparse_{}num_{}freqthresh_{}rows_{}catstrat+{}'.format(n_sparse_features,n_num_features, small_level_freq_thresh, num_rows, categorical_var_strategy, file_tail)\n\noutput_dir = 'dbfs:/mnt/adlsgen2'\n\n#################################\n## LightGBM params\n#################################\n\nclassifier_lightgbm_iterations = 3  \nn_folds = 4                         \nnum_leaves_grid = [32,64]\n\n#################################\n## Control and Verbosity Parameters\n#################################\n\n## run describe on the input table\ndescribe = False\n## save the result of the preprocessing pipeline as a table.\nsave_as_table = False\nrun_crossvalidation = False\n\nimport os\n\noutfile = os.path.join(output_dir,table_to_save+'.parquet')\n\nprint(outfile)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">dbfs:/mnt/adlsgen2/criteo_dac_proc_26sparse_13num_10freqthresh_10000000rows_string-indexcatstrat+try2.parquet\n</div>"]}}],"execution_count":1},{"cell_type":"markdown","source":["## load data loader"],"metadata":{}},{"cell_type":"code","source":["# from reco_utils.dataset.criteo_dac import load_spark_df"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["## read in the data - this takes some time...8-10 minutes\n# df = load_spark_df(spark=spark, dbutils=dbutils)\n## print('writing to parquet...')\n## df.write.parquet('dbfs:/FileStore/dac_train.parquet')\n# df = sqlContext.read.parquet(\"/FileStore/dac_train.parquet\")\ndf = sqlContext.read.parquet('dbfs:/mnt/adlsgen2/dac_train_nocatna.parquet')\n# Could ADLS be causing issues?\n# df = sqlContext.read.parquet(\"/mnt/adlsgen2/dac_train.parquet\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"markdown","source":["## Get number of rows"],"metadata":{}},{"cell_type":"code","source":["total_rows = df.count()\nprint('{} rows in raw data file. Limiting this to {} ({}\\%)'.format(total_rows, num_rows, num_rows/total_rows))\nif num_rows > 0:\n  df = df.limit(num_rows)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">10000000 rows in raw data file. Limiting this to 10000000 (1.0\\%)\n</div>"]}}],"execution_count":6},{"cell_type":"markdown","source":["## Describe the data"],"metadata":{}},{"cell_type":"code","source":["if describe:\n  ## This can take quite a bit of time...\n  cur_descr = df.describe()\n  display(cur_descr)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"code","source":["## boundary check n_sparse_features\nif n_sparse_features < 0 or n_sparse_features > 26:\n  raise ValueError('n_sparse_features must be between 0 and 26...')\nelse:\n  print('Running with {} sparse (i.e. categorical) features.'.format(n_sparse_features))\n  \n## boundary check n_num_features\nif n_num_features < 0 or n_num_features > 13:\n  raise ValueError('n_num_features must be between 0 and 13...')\nelse:\n  print('Running with {} numeric features.'.format(n_num_features))\n  \nif n_num_features+n_sparse_features < 1:\n  raise ValueError('total number of features is less than 1.')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Running with 26 sparse (i.e. categorical) features.\nRunning with 13 numeric features.\n</div>"]}}],"execution_count":9},{"cell_type":"markdown","source":["## Imports"],"metadata":{}},{"cell_type":"code","source":["## for feature engineering:\nfrom pyspark.sql.functions import col, when, count, isnan\nfrom pyspark.ml.feature import (Imputer,StringIndexer,VectorAssembler)\nfrom pyspark.ml.pipeline import Pipeline\n\n## for modeling:\nfrom mmlspark import LightGBMClassifier\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":11},{"cell_type":"markdown","source":["## Define what features to process\n\n- `features` maps to numeric features that need to have missing values replaced\n- `sparse_features` maps to the first `n_sparse_features` categorical / string variables"],"metadata":{}},{"cell_type":"code","source":["\n## features are int features (does median imputation)\nfeatures = [x for x in df.columns if x[0:3] == 'int'][0:n_num_features]\n## sparse_features are str features \nsparse_features = [x for x in df.columns if x[0:3] == 'cat'][0:n_sparse_features]\n\nprint(sparse_features)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[&apos;cat00&apos;, &apos;cat01&apos;, &apos;cat02&apos;, &apos;cat03&apos;, &apos;cat04&apos;, &apos;cat05&apos;, &apos;cat06&apos;, &apos;cat07&apos;, &apos;cat08&apos;, &apos;cat09&apos;, &apos;cat10&apos;, &apos;cat11&apos;, &apos;cat12&apos;, &apos;cat13&apos;, &apos;cat14&apos;, &apos;cat15&apos;, &apos;cat16&apos;, &apos;cat17&apos;, &apos;cat18&apos;, &apos;cat19&apos;, &apos;cat20&apos;, &apos;cat21&apos;, &apos;cat22&apos;, &apos;cat23&apos;, &apos;cat24&apos;, &apos;cat25&apos;]\n</div>"]}}],"execution_count":13},{"cell_type":"markdown","source":["## Fill Missing Values in String Vars"],"metadata":{}},{"cell_type":"code","source":["## fill  missing values in string variables\n## done already and saved above - something weird was going on, where this transformation wasn't propagating to fcut...\n## df = df.na.fill('M', subset = sparse_features)\n## df.write.mode('overwrite').parquet('dbfs:/mnt/adlsgen2/dac_train_nocatna.parquet')\n\nprint(sparse_features)\nprint(df.count())\nfor i in sparse_features:\n  display(df.select([count(when(col(c).isNull(), c)).alias(c) for c in sparse_features]))\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[&apos;cat00&apos;, &apos;cat01&apos;, &apos;cat02&apos;, &apos;cat03&apos;, &apos;cat04&apos;, &apos;cat05&apos;, &apos;cat06&apos;, &apos;cat07&apos;, &apos;cat08&apos;, &apos;cat09&apos;, &apos;cat10&apos;, &apos;cat11&apos;, &apos;cat12&apos;, &apos;cat13&apos;, &apos;cat14&apos;, &apos;cat15&apos;, &apos;cat16&apos;, &apos;cat17&apos;, &apos;cat18&apos;, &apos;cat19&apos;, &apos;cat20&apos;, &apos;cat21&apos;, &apos;cat22&apos;, &apos;cat23&apos;, &apos;cat24&apos;, &apos;cat25&apos;]\n10000000\n</div>"]}}],"execution_count":15},{"cell_type":"markdown","source":["## Replace infrequent levels with a special value"],"metadata":{}},{"cell_type":"code","source":["## only do it if replace_small_levels is true AND small_level_freq_thresh\nif small_level_freq_thresh > 0 and replace_small_levels:\n  ## count frequency of levels, and repalce if F <= threshold for each categorical variable\n  print('Assigning Rare levels a special value')\n  fcut_cat_levels_dict = {i: df.groupby(i).count().\n   select(i, when(col('count') > small_level_freq_thresh, col(i)).otherwise(\"RARE\").alias(i+'_fcut')) for i in sparse_features}\n  ## now join them back...\n  for i in sparse_features:\n    df = df.join(fcut_cat_levels_dict[i], i, how = 'left')\n  ## update the variables we're using as sparse variables:\n  sparse_features = [f + '_fcut' for f in sparse_features]\n  print('Categorical Features updated to be: {}'.format(' '.join(sparse_features)))"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["## do additional work if doing targ-enc\nif categorical_var_strategy == 'targ-enc':\n  ## have to do additional work to compute p(label|level)\n  ## this should be done only on training data then applied to testing. Note for later.\n  labelbylevel_dict = {i: df.groupby(i).mean('label').select([i, col(\"avg(label)\").alias(i+\"_trgt\")]) for i in sparse_features}  \n  ## set up the graph to do all the joins:\n  for i in sparse_features:\n    df = df.join(labelbylevel_dict[i], i, how = \"left\")\n  ## update the variables we're using as sparse:\n  sparse_features = [f + '_trgt' for f in sparse_features]\n  print('Categorical Features updated to be: {}'.format(' '.join(sparse_features)))"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["## Recast `int` variables to `float`\n\n`Imputer()` only works with `float` or `double` type. We could import the data as floats, or run directly on ints using the `df.na.fill()` method.\n\nCurrently using this approach to keep the work in the pipeline."],"metadata":{}},{"cell_type":"code","source":["## cast ints to floats, because Imputer only works with floats\n## and only pull out the strings with the frequency cutoff\nsql_lst = ['cast({} as float) {}'.format(x, x) for x in features] + sparse_features + ['label']\nrecast_df = df.selectExpr(*[sql_lst])"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["## persist to disk to trigger transforms\n## about 25 minutes on l16s x 4\n## time depends on steps and parameters above.\n# recast_df.write.mode('overwrite').parquet(outfile)\n# del recast_df\nrecast_df = sqlContext.read.parquet(outfile)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["num_imputer = Imputer(strategy='median',\n              inputCols=features,\n              outputCols=[f + '_imp' for f in features])\n\nif categorical_var_strategy == 'string-index':\n  print('Using StringIndexer for categorical variables.')\n  pipeline = Pipeline(stages=[\n    num_imputer,\n    # LightGBM can handle categoricals directly if StringIndexer is used through meta-data\n    *[StringIndexer(inputCol=f, outputCol=f+'_vec') for f in sparse_features],\n    VectorAssembler(inputCols= [f + '_imp' for f in features] +\n                    [f + '_vec' for f in sparse_features],\n                    outputCol='features')\n  ])\nelif categorical_var_strategy == 'targ-enc':\n  print('Using target-encoding for categorical variables.')\n  ## build the pipeline\n  pipeline = Pipeline(stages=[\n    num_imputer,\n    VectorAssembler(inputCols= [f + '_imp' for f in features] +\n                    [f for f in sparse_features],\n                    outputCol='features')\n  ])  \nelse:\n  raise ValueError('Unknown strategy for categorical_var_strategy. Should be either \"string-index\" or \"targ-enc\"')"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["## run the pipeline:\ntrain_proc_df = pipeline.fit(recast_df).transform(recast_df)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["from pyspark.sql.functions import isnan, when, count, col\n\nprint(sparse_features)\nfor i in sparse_features:\n  display(train_proc_df.select([count(when(col(c).isNull(), c)).alias(c) for c in sparse_features]))\n#  display(train_proc_df.select([count(when(isnan(c), c)).alias(c) for c in df.columns]))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-3239304803589860&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      1</span> <span class=\"ansigreen\">for</span> i <span class=\"ansigreen\">in</span> sparse_features<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 2</span><span class=\"ansiyellow\">   </span>display<span class=\"ansiyellow\">(</span>train_proc_df<span class=\"ansiyellow\">.</span>select<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">[</span>count<span class=\"ansiyellow\">(</span>when<span class=\"ansiyellow\">(</span>isnan<span class=\"ansiyellow\">(</span>c<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> c<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>alias<span class=\"ansiyellow\">(</span>c<span class=\"ansiyellow\">)</span> <span class=\"ansigreen\">for</span> c <span class=\"ansigreen\">in</span> df<span class=\"ansiyellow\">.</span>columns<span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">&lt;command-3239304803589860&gt;</span> in <span class=\"ansicyan\">&lt;listcomp&gt;</span><span class=\"ansiblue\">(.0)</span>\n<span class=\"ansigreen\">      1</span> <span class=\"ansigreen\">for</span> i <span class=\"ansigreen\">in</span> sparse_features<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 2</span><span class=\"ansiyellow\">   </span>display<span class=\"ansiyellow\">(</span>train_proc_df<span class=\"ansiyellow\">.</span>select<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">[</span>count<span class=\"ansiyellow\">(</span>when<span class=\"ansiyellow\">(</span>isnan<span class=\"ansiyellow\">(</span>c<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> c<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>alias<span class=\"ansiyellow\">(</span>c<span class=\"ansiyellow\">)</span> <span class=\"ansigreen\">for</span> c <span class=\"ansigreen\">in</span> df<span class=\"ansiyellow\">.</span>columns<span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">NameError</span>: name &apos;count&apos; is not defined</div>"]}}],"execution_count":24},{"cell_type":"code","source":["## save after pipeline? This can be an issue...\n## train_proc_df.write.mode('overwrite').parquet(outfile)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["## describe label to see balance\nif describe:\n  display(train_proc_df.select(['label']).describe())"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["if describe:\n  display(train_proc_df.select('features').limit(2))"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["## Set up the Classifier:"],"metadata":{}},{"cell_type":"code","source":["model = LightGBMClassifier(featuresCol='features',\n                           labelCol='label',\n                           numIterations=classifier_lightgbm_iterations,\n                           numLeaves=31,\n                           maxDepth=10,\n                           isUnbalance=True)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["## Fit the model.\n\nto see if simple use-case works"],"metadata":{}},{"cell_type":"code","source":["## try just fitting the model, not with CV\n## model fit works, sometimes.\n## failed I think with full data and 26 cat features l16s x 4\n## 10M rows and full columns: 7.5 minutes\nmodel_fit = model.fit(train_proc_df)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-1706216560221836&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      3</span> <span class=\"ansired\">## failed I think with full data and 26 cat features l16s x 4</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      4</span> <span class=\"ansired\">## 10M rows and full columns: 7.5 minutes</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 5</span><span class=\"ansiyellow\"> </span>model_fit <span class=\"ansiyellow\">=</span> model<span class=\"ansiyellow\">.</span>fit<span class=\"ansiyellow\">(</span>train_proc_df<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/ml/base.py</span> in <span class=\"ansicyan\">fit</span><span class=\"ansiblue\">(self, dataset, params)</span>\n<span class=\"ansigreen\">    130</span>                 <span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>copy<span class=\"ansiyellow\">(</span>params<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>_fit<span class=\"ansiyellow\">(</span>dataset<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    131</span>             <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 132</span><span class=\"ansiyellow\">                 </span><span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>_fit<span class=\"ansiyellow\">(</span>dataset<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    133</span>         <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    134</span>             raise ValueError(&quot;Params must be either a param map or a list/tuple of param maps, &quot;\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/ml/wrapper.py</span> in <span class=\"ansicyan\">_fit</span><span class=\"ansiblue\">(self, dataset)</span>\n<span class=\"ansigreen\">    293</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    294</span>     <span class=\"ansigreen\">def</span> _fit<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> dataset<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 295</span><span class=\"ansiyellow\">         </span>java_model <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_fit_java<span class=\"ansiyellow\">(</span>dataset<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    296</span>         model <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_create_model<span class=\"ansiyellow\">(</span>java_model<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    297</span>         <span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>_copyValues<span class=\"ansiyellow\">(</span>model<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/ml/wrapper.py</span> in <span class=\"ansicyan\">_fit_java</span><span class=\"ansiblue\">(self, dataset)</span>\n<span class=\"ansigreen\">    290</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">    291</span>         self<span class=\"ansiyellow\">.</span>_transfer_params_to_java<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 292</span><span class=\"ansiyellow\">         </span><span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>_java_obj<span class=\"ansiyellow\">.</span>fit<span class=\"ansiyellow\">(</span>dataset<span class=\"ansiyellow\">.</span>_jdf<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    293</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    294</span>     <span class=\"ansigreen\">def</span> _fit<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> dataset<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansicyan\">__call__</span><span class=\"ansiblue\">(self, *args)</span>\n<span class=\"ansigreen\">   1255</span>         answer <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>gateway_client<span class=\"ansiyellow\">.</span>send_command<span class=\"ansiyellow\">(</span>command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1256</span>         return_value = get_return_value(\n<span class=\"ansigreen\">-&gt; 1257</span><span class=\"ansiyellow\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansigreen\">   1258</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1259</span>         <span class=\"ansigreen\">for</span> temp_arg <span class=\"ansigreen\">in</span> temp_args<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansicyan\">deco</span><span class=\"ansiblue\">(*a, **kw)</span>\n<span class=\"ansigreen\">     61</span>     <span class=\"ansigreen\">def</span> deco<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     62</span>         <span class=\"ansigreen\">try</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 63</span><span class=\"ansiyellow\">             </span><span class=\"ansigreen\">return</span> f<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     64</span>         <span class=\"ansigreen\">except</span> py4j<span class=\"ansiyellow\">.</span>protocol<span class=\"ansiyellow\">.</span>Py4JJavaError <span class=\"ansigreen\">as</span> e<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     65</span>             s <span class=\"ansiyellow\">=</span> e<span class=\"ansiyellow\">.</span>java_exception<span class=\"ansiyellow\">.</span>toString<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansicyan\">get_return_value</span><span class=\"ansiblue\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansigreen\">    326</span>                 raise Py4JJavaError(\n<span class=\"ansigreen\">    327</span>                     <span class=\"ansiblue\">&quot;An error occurred while calling {0}{1}{2}.\\n&quot;</span><span class=\"ansiyellow\">.</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 328</span><span class=\"ansiyellow\">                     format(target_id, &quot;.&quot;, name), value)\n</span><span class=\"ansigreen\">    329</span>             <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    330</span>                 raise Py4JError(\n\n<span class=\"ansired\">Py4JJavaError</span>: An error occurred while calling o5548.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 459.0 failed 4 times, most recent failure: Lost task 1.3 in stage 459.0 (TID 9365, 10.139.64.6, executor 1): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$9: (string) =&gt; double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:622)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anonfun$1$$anon$1.hasNext(InMemoryRelation.scala:133)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1170)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1161)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1096)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1161)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:883)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:351)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:302)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:340)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:304)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:340)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:304)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:340)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:304)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:340)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:304)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:340)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:304)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:139)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:112)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1432)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:503)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:251)\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:246)\n\t... 38 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2100)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2088)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2087)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2087)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1076)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1076)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1076)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2319)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2267)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2255)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:873)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2252)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2350)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1051)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:379)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1033)\n\tat org.apache.spark.sql.Dataset$$anonfun$reduce$1.apply(Dataset.scala:1649)\n\tat org.apache.spark.sql.Dataset$$anonfun$withNewRDDExecutionId$1.apply(Dataset.scala:3408)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1.apply(SQLExecution.scala:99)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:228)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:85)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:158)\n\tat org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:3404)\n\tat org.apache.spark.sql.Dataset.reduce(Dataset.scala:1648)\n\tat com.microsoft.ml.spark.LightGBMClassifier.train(LightGBMClassifier.scala:85)\n\tat com.microsoft.ml.spark.LightGBMClassifier.train(LightGBMClassifier.scala:27)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n\tat sun.reflect.GeneratedMethodAccessor518.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$9: (string) =&gt; double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:622)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anonfun$1$$anon$1.hasNext(InMemoryRelation.scala:133)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1170)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1161)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1096)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1161)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:883)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:351)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:302)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:340)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:304)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:340)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:304)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:340)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:304)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:340)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:304)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:340)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:304)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:139)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:112)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1432)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:503)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:251)\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:246)\n\t... 38 more\n</div>"]}}],"execution_count":31},{"cell_type":"code","source":["if run_crossvalidation:\n  grid = (ParamGridBuilder()\n          .addGrid(model.numLeaves, num_leaves_grid) \n          .build())\n  evaluator = BinaryClassificationEvaluator(labelCol='label')\n  cv = CrossValidator(estimator=model, estimatorParamMaps=grid, evaluator=evaluator, numFolds=n_folds)\n  cv_fit = cv.fit(train_proc_df)\n  print(cv_fit)"],"metadata":{},"outputs":[],"execution_count":32}],"metadata":{"name":"lightgbm_estimate_criteo_dac","notebookId":1706216560221809},"nbformat":4,"nbformat_minor":0}
