{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wide and Deep Model for Movie Recommendation\n",
    "\n",
    "### Prerequisite\n",
    "* `tensorflow` (or setup gpu venv -- TODO: link to SETUP guide)\n",
    "\n",
    "In this example, we utilize TensorFlow's higher level Estimator API to build [wide-and-deep model](https://ai.googleblog.com/2016/06/wide-deep-learning-better-together-with.html) for movie recommendation. For more details about how wide-and-deep learning works, see linked [paper](https://arxiv.org/abs/1606.07792)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "import itertools\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "from reco_utils.dataset import movielens\n",
    "from reco_utils.dataset.python_splitters import python_random_split\n",
    "from reco_utils.evaluation.python_evaluation import (\n",
    "    rmse, mae, rsquared, exp_var,\n",
    "    map_at_k, ndcg_at_k, precision_at_k, recall_at_k\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/device:CPU:0', '/device:GPU:0']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "devices = device_lib.list_local_devices()\n",
    "[x.name for x in devices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "MOVIELENS_DATA_SIZE = '100k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserId</th>\n",
       "      <th>MovieId</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3.0</td>\n",
       "      <td>881250949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3.0</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1.0</td>\n",
       "      <td>878887116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2.0</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1.0</td>\n",
       "      <td>886397596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserId  MovieId  Rating  Timestamp\n",
       "0     196      242     3.0  881250949\n",
       "1     186      302     3.0  891717742\n",
       "2      22      377     1.0  878887116\n",
       "3     244       51     2.0  880606923\n",
       "4     166      346     1.0  886397596"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = movielens.load_pandas_df(\n",
    "    size=MOVIELENS_DATA_SIZE,\n",
    "    header=['UserId','MovieId','Rating','Timestamp'],\n",
    "    # TODO For now, not using genres YET\n",
    "    load_genres=False\n",
    ")\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature embedding\n",
    "\n",
    "Wide and deep model utilizes two different types of feature set: 1) a wide set of cross-producted features to capture how the co-occurrence of a query-item feature pair correlates with the target label or rating, and 2) a deep, lower-dimensional embedding vectors for every query and item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Wide columns \"\"\"\n",
    "\n",
    "_HASH_BUCKET_SIZE = 1000\n",
    "\n",
    "# Distinct users and items\n",
    "user_list = data['UserId'].unique()\n",
    "item_list = data['MovieId'].unique()\n",
    "\n",
    "user_id = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    'UserId', user_list)\n",
    "item_id = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    'MovieId', item_list)\n",
    "\n",
    "# Wide columns and deep columns.\n",
    "base_columns = [user_id, item_id]\n",
    "crossed_columns = [\n",
    "    tf.feature_column.crossed_column(\n",
    "          ['UserId', 'MovieId'], hash_bucket_size=_HASH_BUCKET_SIZE),\n",
    "    # TODO maybe add genre here too\n",
    "#       tf.feature_column.crossed_column(\n",
    "#           [age_buckets, 'education', 'occupation'],\n",
    "#           hash_bucket_size=_HASH_BUCKET_SIZE),\n",
    "]\n",
    "\n",
    "# TODO try w/o base_columns\n",
    "wide_columns = base_columns + crossed_columns\n",
    "\n",
    "# TODO check features by:\n",
    "# features = tf.parse_example(..., features=make_parse_example_spec(columns))\n",
    "# dense_tensor = input_layer(features, columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 943 users to 5-dim vector\n",
      "Embedding 1682 items to 6-dim vector\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Deep columns \"\"\"\n",
    "\n",
    "# Rule of thumb for embedding_dimensions =  number_of_categories ** 0.25\n",
    "USER_EMBEDDING_DIM = int(len(user_list) ** 0.25) # or 16\n",
    "ITEM_EMBEDDING_DIM = int(len(item_list) ** 0.25) # or 64\n",
    "print(\"Embedding {} users to {}-dim vector\".format(len(user_list), USER_EMBEDDING_DIM))\n",
    "print(\"Embedding {} items to {}-dim vector\".format(len(item_list), ITEM_EMBEDDING_DIM))\n",
    "\n",
    "# Convert a categorical feature, e.g. UserId or MovieId, into a lower-dimensional embedding vectors\n",
    "user_embedding = tf.feature_column.embedding_column(\n",
    "    categorical_column=user_id,\n",
    "    dimension=USER_EMBEDDING_DIM,\n",
    "    max_norm=USER_EMBEDDING_DIM**.5)\n",
    "\n",
    "item_embedding = tf.feature_column.embedding_column(\n",
    "    categorical_column=item_id,\n",
    "    dimension=ITEM_EMBEDDING_DIM,\n",
    "    max_norm=ITEM_EMBEDDING_DIM**.5)\n",
    "\n",
    "timestamp = tf.feature_column.numeric_column('Timestamp')\n",
    "\n",
    "# TODO numeric_column (w/ shape)\n",
    "# genres = tf.feature_column.numeric_column(\n",
    "#     'Genre', shape=(NUM_GENRES,), dtype=tf.uint8)\n",
    "\n",
    "deep_columns = [user_embedding, item_embedding, timestamp]  # TODO , genres]\n",
    "wide_columns = []  # TODO cross product transformation of user and item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tran and test data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       UserId  MovieId  Timestamp\n",
      "31450     496      136  876066424\n",
      "42809      64      101  889740225\n",
      "52419     158      471  880132513\n",
      "45663     198      652  884209569\n",
      "50696     749      121  878847645\n",
      "\n",
      "Labels:\n",
      "31450    1.0\n",
      "42809    2.0\n",
      "52419    4.0\n",
      "45663    3.0\n",
      "50696    3.0\n",
      "Name: Rating, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "train, test = python_random_split(data, ratio=0.75, seed=123)\n",
    "\n",
    "train_x = train.copy()\n",
    "train_y = train_x.pop('Rating')\n",
    "test_x = test.copy()\n",
    "test_y = test_x.pop('Rating')\n",
    "\n",
    "print(train_x.head())\n",
    "print(\"\\nLabels:\")\n",
    "print(train_y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model selection\n",
    "* `wide` - Linear model\n",
    "* `deep` - DNN model\n",
    "* `wide_deep` - Linear combination of the linear and DNN models\n",
    "\n",
    "(TODO)Model type: `regressor` or `classifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'wide', 'deep', or 'wide_deep' \n",
    "MODEL_TYPE = 'wide'\n",
    "# DNN hidden nodes\n",
    "HIDDEN_UNITS = [256, 256, 256, 128]\n",
    "# Model checkpoints folder\n",
    "MODEL_DIR = './models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': './models', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fc9dddf7d68>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "# TODO set run config if needed\n",
    "if MODEL_TYPE == 'wide':\n",
    "    if len(wide_columns) == 0:\n",
    "        raise ValueError(\"No features have defined for the 'wide' model\")\n",
    "    model = tf.estimator.LinearRegressor(  # LinearClassifier(\n",
    "        model_dir=MODEL_DIR,\n",
    "        feature_columns=wide_columns,\n",
    "    )\n",
    "elif MODEL_TYPE == 'deep':\n",
    "    if len(deep_columns) == 0:\n",
    "        raise ValueError(\"No features have defined for the 'deep' model\")\n",
    "    model = tf.estimator.DNNRegressor(  # DNNClassifier(\n",
    "        model_dir=MODEL_DIR,\n",
    "        feature_columns=deep_columns,\n",
    "        hidden_units=HIDDEN_UNITS,\n",
    "        optimizer=tf.train.AdamOptimizer(),\n",
    "#         activation_fn=tf.nn.sigmoid,\n",
    "#         dropout=0.3,\n",
    "#         loss_reduction=tf.losses.Reduction.MEAN,\n",
    "#         batch_norm=False\n",
    "    )\n",
    "elif MODEL_TYPE == 'wide_deep':\n",
    "    if len(wide_columns) == 0 and len(deep_columns) == 0:\n",
    "        raise ValueError(\"No features have defined for the 'wide_deep' model\")\n",
    "    model = tf.estimator.DNNLinearCombinedRegressor(  # DNNLinearCombinedClassifier(\n",
    "        model_dir=MODEL_DIR,\n",
    "        # wide settings\n",
    "        linear_feature_columns=wide_columns,\n",
    "        # deep settings\n",
    "        dnn_feature_columns=deep_columns,\n",
    "        dnn_hidden_units=HIDDEN_UNITS,\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\"Model type should be either 'wide', 'deep', or 'wide_deep'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into ./models/model.ckpt.\n",
      "INFO:tensorflow:loss = 3695.0, step = 1\n",
      "INFO:tensorflow:global_step/sec: 142.574\n",
      "INFO:tensorflow:loss = 251.51472, step = 101 (0.703 sec)\n",
      "INFO:tensorflow:global_step/sec: 198.346\n",
      "INFO:tensorflow:loss = 207.60385, step = 201 (0.504 sec)\n",
      "INFO:tensorflow:global_step/sec: 206.948\n",
      "INFO:tensorflow:loss = 219.08542, step = 301 (0.483 sec)\n",
      "INFO:tensorflow:global_step/sec: 156.129\n",
      "INFO:tensorflow:loss = 212.32654, step = 401 (0.645 sec)\n",
      "INFO:tensorflow:global_step/sec: 120.729\n",
      "INFO:tensorflow:loss = 184.71173, step = 501 (0.824 sec)\n",
      "INFO:tensorflow:global_step/sec: 187.374\n",
      "INFO:tensorflow:loss = 171.0518, step = 601 (0.535 sec)\n",
      "INFO:tensorflow:global_step/sec: 203.944\n",
      "INFO:tensorflow:loss = 216.3405, step = 701 (0.489 sec)\n",
      "INFO:tensorflow:global_step/sec: 200.185\n",
      "INFO:tensorflow:loss = 210.99043, step = 801 (0.499 sec)\n",
      "INFO:tensorflow:global_step/sec: 194.016\n",
      "INFO:tensorflow:loss = 205.07655, step = 901 (0.515 sec)\n",
      "INFO:tensorflow:global_step/sec: 203.233\n",
      "INFO:tensorflow:loss = 240.52121, step = 1001 (0.494 sec)\n",
      "INFO:tensorflow:global_step/sec: 204.791\n",
      "INFO:tensorflow:loss = 190.12425, step = 1101 (0.488 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.614\n",
      "INFO:tensorflow:loss = 197.20547, step = 1201 (0.492 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.807\n",
      "INFO:tensorflow:loss = 220.03217, step = 1301 (0.481 sec)\n",
      "INFO:tensorflow:global_step/sec: 194.19\n",
      "INFO:tensorflow:loss = 225.79073, step = 1401 (0.515 sec)\n",
      "INFO:tensorflow:global_step/sec: 209.251\n",
      "INFO:tensorflow:loss = 246.01707, step = 1501 (0.478 sec)\n",
      "INFO:tensorflow:global_step/sec: 169.931\n",
      "INFO:tensorflow:loss = 198.87914, step = 1601 (0.589 sec)\n",
      "INFO:tensorflow:global_step/sec: 122.653\n",
      "INFO:tensorflow:loss = 209.33836, step = 1701 (0.815 sec)\n",
      "INFO:tensorflow:global_step/sec: 178.227\n",
      "INFO:tensorflow:loss = 203.65607, step = 1801 (0.563 sec)\n",
      "INFO:tensorflow:global_step/sec: 190.542\n",
      "INFO:tensorflow:loss = 217.39037, step = 1901 (0.523 sec)\n",
      "INFO:tensorflow:global_step/sec: 203.873\n",
      "INFO:tensorflow:loss = 236.23883, step = 2001 (0.492 sec)\n",
      "INFO:tensorflow:global_step/sec: 196.893\n",
      "INFO:tensorflow:loss = 217.36278, step = 2101 (0.506 sec)\n",
      "INFO:tensorflow:global_step/sec: 201.929\n",
      "INFO:tensorflow:loss = 237.31236, step = 2201 (0.495 sec)\n",
      "INFO:tensorflow:global_step/sec: 203.17\n",
      "INFO:tensorflow:loss = 231.26093, step = 2301 (0.492 sec)\n",
      "INFO:tensorflow:global_step/sec: 197.587\n",
      "INFO:tensorflow:loss = 219.92978, step = 2401 (0.507 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.453\n",
      "INFO:tensorflow:loss = 233.32982, step = 2501 (0.493 sec)\n",
      "INFO:tensorflow:global_step/sec: 195.489\n",
      "INFO:tensorflow:loss = 229.60133, step = 2601 (0.512 sec)\n",
      "INFO:tensorflow:global_step/sec: 198.048\n",
      "INFO:tensorflow:loss = 199.49063, step = 2701 (0.505 sec)\n",
      "INFO:tensorflow:global_step/sec: 169.272\n",
      "INFO:tensorflow:loss = 224.68927, step = 2801 (0.591 sec)\n",
      "INFO:tensorflow:global_step/sec: 124.149\n",
      "INFO:tensorflow:loss = 252.84773, step = 2901 (0.805 sec)\n",
      "INFO:tensorflow:global_step/sec: 177.315\n",
      "INFO:tensorflow:loss = 227.57431, step = 3001 (0.564 sec)\n",
      "INFO:tensorflow:global_step/sec: 200.174\n",
      "INFO:tensorflow:loss = 189.13943, step = 3101 (0.500 sec)\n",
      "INFO:tensorflow:global_step/sec: 203.587\n",
      "INFO:tensorflow:loss = 225.40479, step = 3201 (0.491 sec)\n",
      "INFO:tensorflow:global_step/sec: 195.997\n",
      "INFO:tensorflow:loss = 202.06508, step = 3301 (0.510 sec)\n",
      "INFO:tensorflow:global_step/sec: 203.247\n",
      "INFO:tensorflow:loss = 236.09402, step = 3401 (0.492 sec)\n",
      "INFO:tensorflow:global_step/sec: 205.224\n",
      "INFO:tensorflow:loss = 207.21103, step = 3501 (0.487 sec)\n",
      "INFO:tensorflow:global_step/sec: 203.796\n",
      "INFO:tensorflow:loss = 215.78499, step = 3601 (0.491 sec)\n",
      "INFO:tensorflow:global_step/sec: 197.768\n",
      "INFO:tensorflow:loss = 224.1203, step = 3701 (0.506 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.427\n",
      "INFO:tensorflow:loss = 178.5386, step = 3801 (0.494 sec)\n",
      "INFO:tensorflow:global_step/sec: 203.472\n",
      "INFO:tensorflow:loss = 229.90948, step = 3901 (0.491 sec)\n",
      "INFO:tensorflow:global_step/sec: 179.272\n",
      "INFO:tensorflow:loss = 173.40282, step = 4001 (0.558 sec)\n",
      "INFO:tensorflow:global_step/sec: 119.648\n",
      "INFO:tensorflow:loss = 251.13297, step = 4101 (0.837 sec)\n",
      "INFO:tensorflow:global_step/sec: 165.237\n",
      "INFO:tensorflow:loss = 182.25612, step = 4201 (0.605 sec)\n",
      "INFO:tensorflow:global_step/sec: 199.394\n",
      "INFO:tensorflow:loss = 228.22356, step = 4301 (0.500 sec)\n",
      "INFO:tensorflow:global_step/sec: 195.989\n",
      "INFO:tensorflow:loss = 216.44247, step = 4401 (0.510 sec)\n",
      "INFO:tensorflow:global_step/sec: 190.877\n",
      "INFO:tensorflow:loss = 238.28708, step = 4501 (0.524 sec)\n",
      "INFO:tensorflow:global_step/sec: 201.715\n",
      "INFO:tensorflow:loss = 207.83244, step = 4601 (0.496 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.549\n",
      "INFO:tensorflow:loss = 202.38416, step = 4701 (0.494 sec)\n",
      "INFO:tensorflow:global_step/sec: 200.989\n",
      "INFO:tensorflow:loss = 224.79185, step = 4801 (0.498 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.207\n",
      "INFO:tensorflow:loss = 207.7974, step = 4901 (0.494 sec)\n",
      "INFO:tensorflow:global_step/sec: 198.701\n",
      "INFO:tensorflow:loss = 206.65152, step = 5001 (0.504 sec)\n",
      "INFO:tensorflow:global_step/sec: 199.309\n",
      "INFO:tensorflow:loss = 232.3003, step = 5101 (0.501 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.427\n",
      "INFO:tensorflow:loss = 205.2135, step = 5201 (0.553 sec)\n",
      "INFO:tensorflow:global_step/sec: 122.447\n",
      "INFO:tensorflow:loss = 196.36789, step = 5301 (0.815 sec)\n",
      "INFO:tensorflow:global_step/sec: 156.894\n",
      "INFO:tensorflow:loss = 224.0588, step = 5401 (0.638 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.409\n",
      "INFO:tensorflow:loss = 189.18442, step = 5501 (0.483 sec)\n",
      "INFO:tensorflow:global_step/sec: 204.019\n",
      "INFO:tensorflow:loss = 218.1141, step = 5601 (0.489 sec)\n",
      "INFO:tensorflow:global_step/sec: 196.855\n",
      "INFO:tensorflow:loss = 221.86896, step = 5701 (0.508 sec)\n",
      "INFO:tensorflow:global_step/sec: 179.758\n",
      "INFO:tensorflow:loss = 206.23444, step = 5801 (0.556 sec)\n",
      "INFO:tensorflow:global_step/sec: 205.943\n",
      "INFO:tensorflow:loss = 180.42525, step = 5901 (0.485 sec)\n",
      "INFO:tensorflow:global_step/sec: 198.078\n",
      "INFO:tensorflow:loss = 211.48271, step = 6001 (0.505 sec)\n",
      "INFO:tensorflow:global_step/sec: 200.133\n",
      "INFO:tensorflow:loss = 198.12454, step = 6101 (0.499 sec)\n",
      "INFO:tensorflow:global_step/sec: 199.398\n",
      "INFO:tensorflow:loss = 213.62605, step = 6201 (0.502 sec)\n",
      "INFO:tensorflow:global_step/sec: 205.21\n",
      "INFO:tensorflow:loss = 206.93279, step = 6301 (0.489 sec)\n",
      "INFO:tensorflow:global_step/sec: 160.542\n",
      "INFO:tensorflow:loss = 190.32036, step = 6401 (0.622 sec)\n",
      "INFO:tensorflow:global_step/sec: 126.429\n",
      "INFO:tensorflow:loss = 219.80965, step = 6501 (0.791 sec)\n",
      "INFO:tensorflow:global_step/sec: 180.654\n",
      "INFO:tensorflow:loss = 227.56126, step = 6601 (0.555 sec)\n",
      "INFO:tensorflow:global_step/sec: 198.917\n",
      "INFO:tensorflow:loss = 224.8937, step = 6701 (0.501 sec)\n",
      "INFO:tensorflow:global_step/sec: 198.278\n",
      "INFO:tensorflow:loss = 198.0737, step = 6801 (0.505 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.678\n",
      "INFO:tensorflow:loss = 194.10445, step = 6901 (0.516 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.638\n",
      "INFO:tensorflow:loss = 234.67255, step = 7001 (0.493 sec)\n",
      "INFO:tensorflow:global_step/sec: 191.58\n",
      "INFO:tensorflow:loss = 198.76527, step = 7101 (0.523 sec)\n",
      "INFO:tensorflow:global_step/sec: 201.891\n",
      "INFO:tensorflow:loss = 255.1464, step = 7201 (0.494 sec)\n",
      "INFO:tensorflow:global_step/sec: 197.512\n",
      "INFO:tensorflow:loss = 202.42766, step = 7301 (0.506 sec)\n",
      "INFO:tensorflow:global_step/sec: 201.644\n",
      "INFO:tensorflow:loss = 177.60461, step = 7401 (0.496 sec)\n",
      "INFO:tensorflow:global_step/sec: 203.593\n",
      "INFO:tensorflow:loss = 176.38368, step = 7501 (0.491 sec)\n",
      "INFO:tensorflow:global_step/sec: 168.884\n",
      "INFO:tensorflow:loss = 219.2797, step = 7601 (0.592 sec)\n",
      "INFO:tensorflow:global_step/sec: 120.32\n",
      "INFO:tensorflow:loss = 241.49461, step = 7701 (0.831 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.16\n",
      "INFO:tensorflow:loss = 201.4624, step = 7801 (0.552 sec)\n",
      "INFO:tensorflow:global_step/sec: 203.344\n",
      "INFO:tensorflow:loss = 201.25247, step = 7901 (0.494 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.125\n",
      "INFO:tensorflow:loss = 206.66748, step = 8001 (0.493 sec)\n",
      "INFO:tensorflow:global_step/sec: 197.666\n",
      "INFO:tensorflow:loss = 225.92487, step = 8101 (0.507 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.141\n",
      "INFO:tensorflow:loss = 229.66158, step = 8201 (0.496 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 201.746\n",
      "INFO:tensorflow:loss = 206.86874, step = 8301 (0.494 sec)\n",
      "INFO:tensorflow:global_step/sec: 206.264\n",
      "INFO:tensorflow:loss = 183.67978, step = 8401 (0.486 sec)\n",
      "INFO:tensorflow:global_step/sec: 198.629\n",
      "INFO:tensorflow:loss = 231.63138, step = 8501 (0.503 sec)\n",
      "INFO:tensorflow:global_step/sec: 204.207\n",
      "INFO:tensorflow:loss = 198.64767, step = 8601 (0.488 sec)\n",
      "INFO:tensorflow:global_step/sec: 195.908\n",
      "INFO:tensorflow:loss = 200.86711, step = 8701 (0.511 sec)\n",
      "INFO:tensorflow:global_step/sec: 183.698\n",
      "INFO:tensorflow:loss = 222.76509, step = 8801 (0.544 sec)\n",
      "INFO:tensorflow:global_step/sec: 118.037\n",
      "INFO:tensorflow:loss = 185.83377, step = 8901 (0.847 sec)\n",
      "INFO:tensorflow:global_step/sec: 151.528\n",
      "INFO:tensorflow:loss = 207.75725, step = 9001 (0.660 sec)\n",
      "INFO:tensorflow:global_step/sec: 201.986\n",
      "INFO:tensorflow:loss = 211.99585, step = 9101 (0.496 sec)\n",
      "INFO:tensorflow:global_step/sec: 206.051\n",
      "INFO:tensorflow:loss = 183.69858, step = 9201 (0.484 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.295\n",
      "INFO:tensorflow:loss = 198.77295, step = 9301 (0.520 sec)\n",
      "INFO:tensorflow:global_step/sec: 200.356\n",
      "INFO:tensorflow:loss = 212.72748, step = 9401 (0.499 sec)\n",
      "INFO:tensorflow:global_step/sec: 198.684\n",
      "INFO:tensorflow:loss = 221.5948, step = 9501 (0.503 sec)\n",
      "INFO:tensorflow:global_step/sec: 200.064\n",
      "INFO:tensorflow:loss = 204.28477, step = 9601 (0.502 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.759\n",
      "INFO:tensorflow:loss = 193.52937, step = 9701 (0.537 sec)\n",
      "INFO:tensorflow:global_step/sec: 199.797\n",
      "INFO:tensorflow:loss = 218.52084, step = 9801 (0.500 sec)\n",
      "INFO:tensorflow:global_step/sec: 200.945\n",
      "INFO:tensorflow:loss = 265.5845, step = 9901 (0.498 sec)\n",
      "INFO:tensorflow:global_step/sec: 163.386\n",
      "INFO:tensorflow:loss = 228.23903, step = 10001 (0.612 sec)\n",
      "INFO:tensorflow:global_step/sec: 122.681\n",
      "INFO:tensorflow:loss = 219.54472, step = 10101 (0.815 sec)\n",
      "INFO:tensorflow:global_step/sec: 170.815\n",
      "INFO:tensorflow:loss = 178.87169, step = 10201 (0.585 sec)\n",
      "INFO:tensorflow:global_step/sec: 206.092\n",
      "INFO:tensorflow:loss = 197.00024, step = 10301 (0.486 sec)\n",
      "INFO:tensorflow:global_step/sec: 204.06\n",
      "INFO:tensorflow:loss = 225.37267, step = 10401 (0.489 sec)\n",
      "INFO:tensorflow:global_step/sec: 197.15\n",
      "INFO:tensorflow:loss = 262.07803, step = 10501 (0.507 sec)\n",
      "INFO:tensorflow:global_step/sec: 204.529\n",
      "INFO:tensorflow:loss = 207.30473, step = 10601 (0.489 sec)\n",
      "INFO:tensorflow:global_step/sec: 205.263\n",
      "INFO:tensorflow:loss = 207.03763, step = 10701 (0.489 sec)\n",
      "INFO:tensorflow:global_step/sec: 199.65\n",
      "INFO:tensorflow:loss = 239.34418, step = 10801 (0.500 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.615\n",
      "INFO:tensorflow:loss = 200.27887, step = 10901 (0.519 sec)\n",
      "INFO:tensorflow:global_step/sec: 200\n",
      "INFO:tensorflow:loss = 193.25859, step = 11001 (0.500 sec)\n",
      "INFO:tensorflow:global_step/sec: 200.265\n",
      "INFO:tensorflow:loss = 214.55206, step = 11101 (0.499 sec)\n",
      "INFO:tensorflow:global_step/sec: 173.264\n",
      "INFO:tensorflow:loss = 197.7474, step = 11201 (0.577 sec)\n",
      "INFO:tensorflow:global_step/sec: 125.505\n",
      "INFO:tensorflow:loss = 220.5426, step = 11301 (0.798 sec)\n",
      "INFO:tensorflow:global_step/sec: 175.188\n",
      "INFO:tensorflow:loss = 224.32768, step = 11401 (0.569 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.865\n",
      "INFO:tensorflow:loss = 217.3107, step = 11501 (0.493 sec)\n",
      "INFO:tensorflow:global_step/sec: 203.179\n",
      "INFO:tensorflow:loss = 202.47366, step = 11601 (0.494 sec)\n",
      "INFO:tensorflow:global_step/sec: 190.814\n",
      "INFO:tensorflow:loss = 224.5001, step = 11701 (0.523 sec)\n",
      "INFO:tensorflow:global_step/sec: 203.621\n",
      "INFO:tensorflow:loss = 197.78268, step = 11801 (0.491 sec)\n",
      "INFO:tensorflow:global_step/sec: 201.232\n",
      "INFO:tensorflow:loss = 225.31775, step = 11901 (0.497 sec)\n",
      "INFO:tensorflow:global_step/sec: 205.395\n",
      "INFO:tensorflow:loss = 191.94055, step = 12001 (0.487 sec)\n",
      "INFO:tensorflow:global_step/sec: 189.199\n",
      "INFO:tensorflow:loss = 222.83707, step = 12101 (0.529 sec)\n",
      "INFO:tensorflow:global_step/sec: 203.331\n",
      "INFO:tensorflow:loss = 181.51752, step = 12201 (0.492 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.94\n",
      "INFO:tensorflow:loss = 193.08313, step = 12301 (0.493 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.103\n",
      "INFO:tensorflow:loss = 201.91148, step = 12401 (0.552 sec)\n",
      "INFO:tensorflow:global_step/sec: 122.67\n",
      "INFO:tensorflow:loss = 186.40596, step = 12501 (0.815 sec)\n",
      "INFO:tensorflow:global_step/sec: 164.342\n",
      "INFO:tensorflow:loss = 211.58586, step = 12601 (0.609 sec)\n",
      "INFO:tensorflow:global_step/sec: 203.794\n",
      "INFO:tensorflow:loss = 207.23853, step = 12701 (0.491 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.622\n",
      "INFO:tensorflow:loss = 204.86496, step = 12801 (0.517 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.581\n",
      "INFO:tensorflow:loss = 197.8623, step = 12901 (0.495 sec)\n",
      "INFO:tensorflow:global_step/sec: 204.383\n",
      "INFO:tensorflow:loss = 209.52933, step = 13001 (0.488 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.945\n",
      "INFO:tensorflow:loss = 201.93608, step = 13101 (0.493 sec)\n",
      "INFO:tensorflow:global_step/sec: 198.715\n",
      "INFO:tensorflow:loss = 197.65245, step = 13201 (0.503 sec)\n",
      "INFO:tensorflow:global_step/sec: 196.91\n",
      "INFO:tensorflow:loss = 227.02902, step = 13301 (0.508 sec)\n",
      "INFO:tensorflow:global_step/sec: 201.042\n",
      "INFO:tensorflow:loss = 221.62366, step = 13401 (0.498 sec)\n",
      "INFO:tensorflow:global_step/sec: 203.523\n",
      "INFO:tensorflow:loss = 179.75383, step = 13501 (0.490 sec)\n",
      "INFO:tensorflow:global_step/sec: 172.25\n",
      "INFO:tensorflow:loss = 209.33614, step = 13601 (0.581 sec)\n",
      "INFO:tensorflow:global_step/sec: 122.284\n",
      "INFO:tensorflow:loss = 227.47992, step = 13701 (0.820 sec)\n",
      "INFO:tensorflow:global_step/sec: 165.758\n",
      "INFO:tensorflow:loss = 233.76286, step = 13801 (0.601 sec)\n",
      "INFO:tensorflow:global_step/sec: 205.976\n",
      "INFO:tensorflow:loss = 196.70288, step = 13901 (0.485 sec)\n",
      "INFO:tensorflow:global_step/sec: 195.873\n",
      "INFO:tensorflow:loss = 229.85722, step = 14001 (0.512 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.199\n",
      "INFO:tensorflow:loss = 226.72879, step = 14101 (0.494 sec)\n",
      "INFO:tensorflow:global_step/sec: 198.267\n",
      "INFO:tensorflow:loss = 216.06699, step = 14201 (0.503 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.263\n",
      "INFO:tensorflow:loss = 218.19481, step = 14301 (0.494 sec)\n",
      "INFO:tensorflow:global_step/sec: 203.71\n",
      "INFO:tensorflow:loss = 203.33423, step = 14401 (0.491 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.503\n",
      "INFO:tensorflow:loss = 236.72353, step = 14501 (0.520 sec)\n",
      "INFO:tensorflow:global_step/sec: 200.288\n",
      "INFO:tensorflow:loss = 223.81659, step = 14601 (0.499 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 14649 into ./models/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 86.26987.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.linear.LinearRegressor at 0x7fc9dddf7e48>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Maybe should set tf.estimator.RunConfig to run on GPU?\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "train_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
    "    x=train_x,\n",
    "    y=train_y,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    shuffle=True,\n",
    "    num_threads=1\n",
    ")\n",
    "\n",
    "model.train(input_fn=train_input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "To evaluate our model, we first evaluate the baseline performance by predicting a user's rating as a simple average of his/her previous ratings. Then, we predict the ratings by using the wide-deep model we trained. Finally, we also generate top-k movie recommentation for each user and test the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserId</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3.655172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3.711111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2.756757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2.939850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserId    Rating\n",
       "0       1  3.655172\n",
       "1       2  3.711111\n",
       "2       3  2.756757\n",
       "3       4  4.111111\n",
       "4       5  2.939850"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline = train.groupby(['UserId'])['Rating'].mean()\n",
    "baseline = baseline.to_frame().reset_index()\n",
    "baseline.head()\n",
    "# baseline = pd.merge(test, baseline.to_frame(['UserId', 'prediction']), on=['UserId'], how='inner')\n",
    "# baseline.head()\n",
    "# baseline.sort_values(by=['UserId']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'rating'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/reco_gpu/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3077\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3078\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3079\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'rating'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-4bc992431b03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m }\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0meval_rmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0meval_mae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0meval_rsquared\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrsquared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/git/Recommenders/reco_utils/evaluation/python_evaluation.py\u001b[0m in \u001b[0;36mrmse\u001b[0;34m(rating_true, rating_pred, col_user, col_item, col_rating, col_prediction)\u001b[0m\n\u001b[1;32m    104\u001b[0m     return np.sqrt(\n\u001b[1;32m    105\u001b[0m         mean_squared_error(\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mrating_true_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDEFAULT_RATING_COL\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrating_true_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mPREDICTION_COL\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         )\n\u001b[1;32m    108\u001b[0m     )\n",
      "\u001b[0;32m~/miniconda3/envs/reco_gpu/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2686\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2687\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2688\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/reco_gpu/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2693\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2695\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2697\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/reco_gpu/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   2487\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2489\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2490\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2491\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/reco_gpu/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   4113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4115\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4116\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4117\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/reco_gpu/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3078\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3082\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'rating'"
     ]
    }
   ],
   "source": [
    "cols = {\n",
    "    'col_user': 'UserId',\n",
    "    'col_item': 'MovieId',\n",
    "    'col_rating': 'Rating',\n",
    "    'col_prediction': 'prediction',\n",
    "}\n",
    "\n",
    "eval_rmse = rmse(test, baseline, **cols)\n",
    "eval_mae = mae(test, baseline, **cols)\n",
    "eval_rsquared = rsquared(test, baseline, **cols)\n",
    "eval_exp_var = exp_var(test, baseline, **cols)\n",
    "\n",
    "print(\"RMSE:\\t\\t%f\" % eval_rmse,\n",
    "      \"MAE:\\t\\t%f\" % eval_mae,\n",
    "      \"rsquared:\\t%f\" % eval_rsquared,\n",
    "      \"exp var:\\t%f\" % eval_exp_var, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NN model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-12-15-23:44:15\n",
      "INFO:tensorflow:Restoring parameters from ./models/model.ckpt-14649\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-15-23:44:16\n",
      "INFO:tensorflow:Saving dict for global step 14649: average_loss = 0.91905546, global_step = 14649, loss = 117.22646\n",
      "{'average_loss': 0.91905546, 'loss': 117.22646, 'global_step': 14649}\n"
     ]
    }
   ],
   "source": [
    "test_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
    "    x=test_x,\n",
    "    y=test_y,\n",
    "    num_epochs=1,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "result = model.evaluate(input_fn=test_input_fn, steps=None)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Item rating prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/model.ckpt-14649\n",
      "       UserId  MovieId  Rating  Timestamp  prediction\n",
      "42083     600      651     4.0  888451492    4.182480\n",
      "71825     607      494     5.0  883879556    3.595096\n",
      "99535     875     1103     5.0  876465144    4.357851\n",
      "47879     648      238     3.0  882213535    4.077431\n",
      "36734     113      273     4.0  875935609    3.902378\n"
     ]
    }
   ],
   "source": [
    "pred_list = [p['predictions'][0] for p in list(model.predict(input_fn=test_input_fn))]\n",
    "predictions = test.copy()\n",
    "predictions['prediction']  = pd.Series(pred_list).values\n",
    "print(predictions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:\t\t0.958674\n",
      "MAE:\t\t0.755316\n",
      "rsquared:\t0.285917\n",
      "exp var:\t0.285921\n"
     ]
    }
   ],
   "source": [
    "eval_rmse = rmse(test, predictions, **cols)\n",
    "eval_mae = mae(test, predictions, **cols)\n",
    "eval_rsquared = rsquared(test, predictions, **cols)\n",
    "eval_exp_var = exp_var(test, predictions, **cols)\n",
    "\n",
    "print(\"RMSE:\\t\\t%f\" % eval_rmse,\n",
    "      \"MAE:\\t\\t%f\" % eval_mae,\n",
    "      \"rsquared:\\t%f\" % eval_rsquared,\n",
    "      \"exp var:\\t%f\" % eval_exp_var, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Recommend k items\n",
    "\n",
    "1) Remove seen items and 2) add timestamp info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before excude seen items: 1586126\n",
      "After excude seen items: 1511126\n",
      "   UserId  MovieId    Timestamp\n",
      "0     600      651  888451492.0\n",
      "1     607      494  883879556.0\n",
      "2     875     1103  876465144.0\n",
      "3     648      238  882213535.0\n",
      "4     113      273  875935609.0\n"
     ]
    }
   ],
   "source": [
    "# Get the cross join of all user-item pairs and score them.\n",
    "user_item_col = ['UserId', 'MovieId']\n",
    "user_item_list = list(itertools.product(user_list, item_list))\n",
    "users_items = pd.DataFrame(user_item_list, columns=user_item_col)\n",
    "print(\"Before excude seen items:\", len(users_items))\n",
    "\n",
    "# Remove seen items (items in the train set)\n",
    "users_items_exclude_train = users_items.loc[\n",
    "    ~users_items.set_index(user_item_col).index.isin(train.set_index(user_item_col).index)\n",
    "]\n",
    "print(\"After excude seen items:\", len(users_items_exclude_train))\n",
    "\n",
    "# Add timestamp info\n",
    "users_items_exclude_train = pd.merge(test, users_items_exclude_train,\n",
    "                                     on=user_item_col, how='outer')\n",
    "users_items_exclude_train.drop('Rating', axis=1, inplace=True)\n",
    "users_items_exclude_train.fillna(test['Timestamp'].max(), inplace=True) \n",
    "print(users_items_exclude_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/model.ckpt-14649\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserId</th>\n",
       "      <th>MovieId</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>600</td>\n",
       "      <td>651</td>\n",
       "      <td>888451492.0</td>\n",
       "      <td>4.182480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>607</td>\n",
       "      <td>494</td>\n",
       "      <td>883879556.0</td>\n",
       "      <td>3.595096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>875</td>\n",
       "      <td>1103</td>\n",
       "      <td>876465144.0</td>\n",
       "      <td>4.357851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>648</td>\n",
       "      <td>238</td>\n",
       "      <td>882213535.0</td>\n",
       "      <td>4.077431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>113</td>\n",
       "      <td>273</td>\n",
       "      <td>875935609.0</td>\n",
       "      <td>3.902378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserId  MovieId    Timestamp  prediction\n",
       "0     600      651  888451492.0    4.182480\n",
       "1     607      494  883879556.0    3.595096\n",
       "2     875     1103  876465144.0    4.357851\n",
       "3     648      238  882213535.0    4.077431\n",
       "4     113      273  875935609.0    3.902378"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reco_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
    "    x=users_items_exclude_train,\n",
    "    num_epochs=1,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "reco = list(model.predict(input_fn=reco_input_fn))\n",
    "reco_list = [p['predictions'][0] for p in reco]\n",
    "users_items_exclude_train['prediction']  = pd.Series(reco_list).values\n",
    "users_items_exclude_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP:\t0.000414\n",
      "NDCG:\t0.005410\n",
      "Precision@K:\t0.006787\n",
      "Recall@K:\t0.002281\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "eval_map = map_at_k(test, users_items_exclude_train, k=k, **cols)\n",
    "eval_ndcg = ndcg_at_k(test, users_items_exclude_train, k=k, **cols)\n",
    "eval_precision = precision_at_k(test, users_items_exclude_train, k=k, **cols)\n",
    "eval_recall = recall_at_k(test, users_items_exclude_train, k=k, **cols)\n",
    "\n",
    "print(\"MAP:\\t%f\" % eval_map,\n",
    "      \"NDCG:\\t%f\" % eval_ndcg,\n",
    "      \"Precision@K:\\t%f\" % eval_precision,\n",
    "      \"Recall@K:\\t%f\" % eval_recall, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reco_gpu",
   "language": "python",
   "name": "reco_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
