{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "by using **Azure Machine Learning service** ([AML or AzureML](https://azure.microsoft.com/en-us/services/machine-learning-service/)).  \n",
    "\n",
    "Specifically, we utilize TensorFlow's higher level Estimator API to build [wide-and-deep model](https://ai.googleblog.com/2016/06/wide-deep-learning-better-together-with.html) for a movie recommendation scenario. While doing that, we try to search optimal hyperparameters via [AML hyperdrive](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisite\n",
    "\n",
    "* azureml -- You can skip this if you already know what values of hyperparameters you want to use\n",
    "\n",
    "\n",
    "For details about how to install and setup AML, see following materials:\n",
    "- [AML quickstart](https://docs.microsoft.com/en-us/azure/machine-learning/service/quickstart-create-workspace-with-python)\n",
    "- [Train a TensorFlow model](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-train-tensorflow)\n",
    "- [Hyperparameter tuning](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.train.dnn import TensorFlow\n",
    "from azureml.train.hyperdrive import *\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "from reco_utils.dataset import movielens\n",
    "from reco_utils.dataset.python_splitters import python_random_split\n",
    "\n",
    "print(\"Azure ML SDK Version:\", azureml.core.VERSION)\n",
    "print(\"Tensorflow Version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top k items to recommend\n",
    "TOP_K = 10\n",
    "\n",
    "# Select Movielens data size: 100k, 1m, 10m, or 20m\n",
    "MOVIELENS_DATA_SIZE = '100k'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Hyperparameter Tuning via AML\n",
    "\n",
    "This section assumes you already created a **Azure ML workspace** and have a `./aml_config/config.json` file to load the workspace from this notebook. If not, please follow instructions in the [tutorial](https://docs.microsoft.com/en-us/azure/machine-learning/service/quickstart-get-started) to create a workspace and make a `./aml_config/config.json` file containing:\n",
    "```\n",
    "{\n",
    "    \"subscription_id\": \"your-subscription-id\",\n",
    "    \"resource_group\": \"your-resource-group\",\n",
    "    \"workspace_name\": \"your-workspace-name\"\n",
    "}\n",
    "```\n",
    "  \n",
    "From the following cells, we will\n",
    "1. Create a remote compute target (gpu-cluster) if it does not exist already,\n",
    "2. Mount data store and upload the training set, and\n",
    "3. Run a hyperparameter tuning experiment.\n",
    "\n",
    "First, let's connect to the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to a workspace\n",
    "ws = Workspace.from_config()\n",
    "print(\"Workspace name: \", ws.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a remote compute target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTER_NAME = 'gpu-cluster'\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=CLUSTER_NAME)\n",
    "    print(\"Found existing compute target\")\n",
    "except ComputeTargetException:\n",
    "    print(\"Creating a new compute target...\")\n",
    "    compute_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size='STANDARD_NC6',\n",
    "        vm_priority='lowpriority',\n",
    "        min_nodes=1,\n",
    "        max_nodes=4\n",
    "    )\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, CLUSTER_NAME, compute_config)\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "# Use the 'status' property to get a detailed status for the current cluster. \n",
    "print(compute_target.status.serialize())\n",
    "\n",
    "# Check list of aml-computes\n",
    "compute_targets = ws.compute_targets\n",
    "for name, ct in compute_targets.items():\n",
    "    print(name, ct.type, ct.provisioning_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare dataset\n",
    "1. Download data and train/test split\n",
    "2. Upload to storage\n",
    "\n",
    "Next, upload the training set to the data store. This example uses the workspace's default **blob storage**.\n",
    "  \n",
    "We also prepare a training script [wide_deep_training.py](../../reco_utils/aml/wide_deep_training.py) for the hyperparameter tuning, which will log our target metrics (e.g. [RMSE](https://en.wikipedia.org/wiki/Root-mean-square_deviation)) to AML experiment so that we can track the metrics and optimize it via **hyperdrive**.\n",
    "\n",
    "```\n",
    "TODO - maybe attach a code snippet here for description\n",
    "1. logging part\n",
    "\n",
    "2. wide and deep model\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = movielens.load_pandas_df(\n",
    "    size=MOVIELENS_DATA_SIZE,\n",
    "    header=['UserId','MovieId','Rating','Timestamp'],\n",
    "    title_col=None,\n",
    "    genres_col=None,  # TODO to use genres, should encode\n",
    ")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, eval_df = python_random_split(data, ratio=0.75, seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "TRAIN_FILE_NAME = \"movielens_\" + MOVIELENS_DATA_SIZE + \"_train.pkl\"\n",
    "EVAL_FILE_NAME = \"movielens_\" + MOVIELENS_DATA_SIZE + \"_eval.pkl\"\n",
    "train_df.to_pickle(os.path.join(DATA_DIR, TRAIN_FILE_NAME))\n",
    "eval_df.to_pickle(os.path.join(DATA_DIR, EVAL_FILE_NAME))\n",
    "\n",
    "# Note, all the files under DATA_DIR will be uploaded\n",
    "ds = ws.get_default_datastore()\n",
    "ds.upload(\n",
    "    src_dir=DATA_DIR,\n",
    "    target_path='data',\n",
    "    overwrite=True,\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare training script. All the script in the folder will be uploaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPT_DIR = \"../../reco_utils\"\n",
    "ENTRY_SCRIPT_NAME = \"aml/wide_deep_training.py\"\n",
    "# SCRIPT_DIR = './aml_scripts'\n",
    "# ENTRY_SCRIPT_NAME = 'wide_deep_training.py'\n",
    "\n",
    "# os.makedirs(SCRIPT_DIR, exist_ok=True)\n",
    "# TODO maybe upload the entire reco_utils folder? will that work? -- feedback\n",
    "# shutil.copy('../../reco_utils/aml/wide_deep_training.py', SCRIPT_DIR)\n",
    "# shutil.copy('../../reco_utils/aml/tf_log_hook.py', SCRIPT_DIR)\n",
    "# shutil.copy('../../reco_utils/common/tf_utils.py', SCRIPT_DIR)\n",
    "# shutil.copy('../../reco_utils/evaluation/python_evaluation.py', SCRIPT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a search space for the hyperparameters. All the parameter values will be passed to the training script where they are parsed by `argparse`, e.g.:\n",
    "```\n",
    "TODO code snippet for argparse\n",
    "```\n",
    "    \n",
    "AML hyperdrive provides some very useful searching strategies including `RandomParameterSampling`, `GridParameterSampling`, and `BayesianParameterSampling`. Details about each approach are beyond the scope of this notebook and you can find them from [Azure doc](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters). Here, we use the random sampling for simplicity. \n",
    "\n",
    "> Note: Currently, this repo accepts either 'rmse' or 'mae' for `METRICS` as implemented in [tf_utils.py](../../reco_utils/common/tf_utils.py), but you can define any custom metrics and utilize it along with AML hyperdrive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = \"movielens_\" + MOVIELENS_DATA_SIZE + \"_wide_deep_params\"\n",
    "METRICS_LIST = ['RMSE', 'NDCG']\n",
    "\n",
    "script_params = {\n",
    "    '--datastore': ds.as_mount(),\n",
    "    '--train-datapath': \"data/\" + TRAIN_FILE_NAME,\n",
    "    '--eval-datapath': \"data/\" + EVAL_FILE_NAME,\n",
    "    '--user-col': 'UserId',\n",
    "    '--item-col': 'MovieId',\n",
    "    '--rating-col': 'Rating',\n",
    "    '--timestamp-col': 'Timestamp',\n",
    "#     '--item-feat-col': 'Genres',\n",
    "    '--batch-size': 64,\n",
    "    '--epochs': 50,\n",
    "    '--metrics-list': METRICS_LIST,\n",
    "}\n",
    "\n",
    "# hyperparameters search space\n",
    "hyper_params = {\n",
    "    '--model-type': choice('wide', 'deep', 'wide-deep'),\n",
    "    # Wide model hyperparameters\n",
    "    '--linear-optimizer': choice('Ftrl', 'SGD'),\n",
    "    '--linear-optimizer-lr': loguniform(-6, -2),\n",
    "    # Deep model hyperparameters\n",
    "    '--dnn-optimizer': choice('Adagrad', 'Adam'),\n",
    "    '--dnn-optimizer-lr': loguniform(-6, -2),\n",
    "    '--dnn-user-embedding-dim': choice(4, 16),\n",
    "    '--dnn-item-embedding-dim': choice(8, 64),\n",
    "    '--dnn-hidden-units': choice(\n",
    "        \"256,256,256,128\",\n",
    "        \"256,128\",\n",
    "        \"256,64,256\",\n",
    "        \"512,128,32\"\n",
    "    ),\n",
    "    '--dnn-batch-norm': choice(True, False),\n",
    "}\n",
    "\n",
    "ps = RandomParameterSampling(hyper_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `azureml.train.dnn.TensorFlow`, a custom AML `Estimator` class which utilizes a preset docker image in the cluster (see more information from [here](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-train-tensorflow)).\n",
    "\n",
    "Once you submit the experiment, you can see the progress from the notebook by using `azureml.widgets.RunDetails`. You can directly check the details from the Azure portal as well. To get the link, run `run.get_portal_url()`.\n",
    "\n",
    "> Since we will do hyperparameter tuning, we create a `HyperDriveRunConfig` and pass it to the experiment object. If you already know what hyperparameters to use and still want to utilize AML for other purposes (e.g. model management), you can set the hyperparameter values directly to `script_params` and run the experiment, `run = exp.submit(est)`, instead.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = TensorFlow(\n",
    "    source_directory=SCRIPT_DIR,\n",
    "    entry_script=ENTRY_SCRIPT_NAME,\n",
    "    script_params=script_params,\n",
    "    compute_target=compute_target,\n",
    "    use_gpu=True,\n",
    "    conda_packages=['pandas', 'scikit-learn'],\n",
    ")\n",
    "\n",
    "# early termnination policy\n",
    "policy = MedianStoppingPolicy(delay_evaluation=5)\n",
    "# BanditPolicy(evaluation_interval=2, slack_factor=0.1)\n",
    "\n",
    "hd_config = HyperDriveRunConfig(\n",
    "    estimator=est, \n",
    "    hyperparameter_sampling=ps,\n",
    "    policy=policy,  \n",
    "    primary_metric_name='rmse',\n",
    "    primary_metric_goal=PrimaryMetricGoal.MINIMIZE, \n",
    "    max_total_runs=4,  #20,\n",
    "    max_concurrent_runs=4\n",
    ")\n",
    "\n",
    "# Create an experiment to track the runs in the workspace\n",
    "exp = Experiment(workspace=ws, name=EXP_NAME)\n",
    "\n",
    "# run = exp.submit(config=hd_config)\n",
    "# TODO is this possible? to:\n",
    "with exp.submit(config=hd_config) as run:\n",
    "#     print(run.get_portal_url())\n",
    "    RunDetails(run).show()\n",
    "    run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO get bestrun and printout metrics!!!\n",
    "best_run = run.get_best_run_by_primary_metric()\n",
    "\n",
    "\n",
    "### Maybe show the worst model (or avg model) to demonstrate the importance of hyperparam tuning...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO prediction..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.cancel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test\n",
    "\n",
    "To load a registered model in the future,\n",
    "```\n",
    "from azureml.core.model import Model\n",
    "\n",
    "model = Model(ws, 'model_name')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = './model'\n",
    "\n",
    "best_run = run.get_best_run_by_primary_metric()\n",
    "# Check model files uploaded during the run\n",
    "print(best_run.get_file_names())\n",
    "\n",
    "# Register the model in the workspace so that can later query, examine, and deploy this model.\n",
    "# TODO check model path...\n",
    "model = best_run.register_model(model_name=MODEL_NAME, model_path='./outputs/model')\n",
    "print(model.name, model.id, model.version)\n",
    "\n",
    "# Download the model to local. (alternatively, run.download_file(name=f, output_file_path=output_file_path))\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "model.download(target_dir=MODEL_DIR)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"       \n",
    "        \n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "saver = tf.train.import_meta_graph(\"./model/mnist-tf.model.meta\")\n",
    "graph = tf.get_default_graph()\n",
    "\n",
    "for op in graph.get_operations():\n",
    "    if op.name.startswith('network'):\n",
    "        print(op.name)\n",
    "\n",
    "# input tensor. this is an array of 784 elements, each representing the intensity of a pixel in the digit image.\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"network/X:0\")\n",
    "# output tensor. this is an array of 10 elements, each representing the probability of predicted value of the digit.\n",
    "output = tf.get_default_graph().get_tensor_by_name(\"network/output/MatMul:0\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, './model/mnist-tf.model')\n",
    "    k = output.eval(feed_dict={X : X_test})\n",
    "# get the prediction, which is the index of the element that has the largest probability value.\n",
    "y_hat = np.argmax(k, axis=1)\n",
    "\n",
    "# print the first 30 labels and predictions\n",
    "print('labels:  \\t', y_test[:30])\n",
    "print('predictions:\\t', y_hat[:30])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# TODO...\n",
    "model_root = Model.get_model_path('tf-dnn-mnist')\n",
    "    saver = tf.train.import_meta_graph(os.path.join(model_root, 'mnist-tf.model.meta'))\n",
    "    X = tf.get_default_graph().get_tensor_by_name(\"network/X:0\")\n",
    "    output = tf.get_default_graph().get_tensor_by_name(\"network/output/MatMul:0\")\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    saver.restore(sess, os.path.join(model_root, 'mnist-tf.model'))\n",
    "\n",
    "def run(raw_data):\n",
    "    data = np.array(json.loads(raw_data)['data'])\n",
    "    # make prediction\n",
    "    out = output.eval(session=sess, feed_dict={X: data})\n",
    "    y_hat = np.argmax(out, axis=1)\n",
    "    return y_hat.tolist()\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reco_utils.evaluation.python_evaluation import (\n",
    "    rmse, mae, rsquared, exp_var,\n",
    "    map_at_k, ndcg_at_k, precision_at_k, recall_at_k\n",
    ")\n",
    "\n",
    "# Prepare test data\n",
    "X_test = test.copy()\n",
    "y_test = X_test.pop('Rating')\n",
    "\n",
    "# test_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
    "#     x=X_test,\n",
    "#     num_epochs=1,\n",
    "#     shuffle=False\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.contrib.predictor.from_saved_model(MODEL_DIR+\"/model/1546314741\")\n",
    "# model = tf.contrib.estimator.SavedModelEstimator(MODEL_DIR+\"/model/1546314741\")\n",
    "\n",
    "# Convert input data into serialized Example strings.\n",
    "examples = []\n",
    "for index, row in X_test.iterrows():\n",
    "    feature = {}\n",
    "    for col, value in row.iteritems():\n",
    "        feature[col] = tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "    example = tf.train.Example(\n",
    "        features=tf.train.Features(\n",
    "            feature=feature\n",
    "        )\n",
    "    )\n",
    "    examples.append(example.SerializeToString())\n",
    "\n",
    "predictions = model({'inputs': examples})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# def predict_input_fn():\n",
    "#     example = tf.train.Example()\n",
    "#     example.features.feature['feature1'].bytes_list.value.extend(['yellow'])\n",
    "#     example.features.feature['feature2'].float_list.value.extend([1.])\n",
    "#     return {'inputs':tf.constant([example.SerializeToString()])}\n",
    "\n",
    "# If all modes were exported, you can immediately evaluate and predict, or\n",
    "# continue training. Otherwise only predict is available.\n",
    "# See https://www.tensorflow.org/api_docs/python/tf/contrib/estimator/export_all_saved_models\n",
    "\n",
    "# eval_results = model.evaluate(input_fn=input_fn, steps=1)\n",
    "# print(eval_results)\n",
    "# model.train(input_fn=input_fn, steps=20)\n",
    "\n",
    "\n",
    "\n",
    "predictions = predict_fn(\n",
    "    {\"x\": [[6.4, 3.2, 4.5, 1.5],\n",
    "           [5.8, 3.1, 5.0, 1.7]]})\n",
    "print(predictions['scores'])\n",
    "\n",
    "\n",
    "\n",
    "pred_list = [p['predictions'][0] for p in list(model.predict(predict_input_fn))]\n",
    "predictions = test.copy()\n",
    "predictions['prediction']  = pd.Series(pred_list).values\n",
    "print(predictions.head())\n",
    "\n",
    "cols = {\n",
    "    'col_user': 'UserId',\n",
    "    'col_item': 'MovieId',\n",
    "    'col_rating': 'Rating',\n",
    "    'col_prediction': 'prediction'\n",
    "}\n",
    "\n",
    "predictions.drop('Rating', axis=1, inplace=True)\n",
    "\n",
    "eval_rmse = rmse(test, predictions, **cols)\n",
    "eval_mae = mae(test, predictions, **cols)\n",
    "eval_rsquared = rsquared(test, predictions, **cols)\n",
    "eval_exp_var = exp_var(test, predictions, **cols)\n",
    "\n",
    "print(\"RMSE:\\t\\t%f\" % eval_rmse,\n",
    "      \"MAE:\\t\\t%f\" % eval_mae,\n",
    "      \"rsquared:\\t%f\" % eval_rsquared,\n",
    "      \"exp var:\\t%f\" % eval_exp_var, sep='\\n')\n",
    "\n",
    "# Load the downloaded model and test\n",
    "# with tf.Session() as sess:\n",
    "#     tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING], MODEL_DIR)\n",
    "\n",
    "# #     test_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
    "# #         x=X_test,\n",
    "# #         y=y_test,\n",
    "# #         batch_size=BATCH_SIZE,\n",
    "# #         num_epochs=1,\n",
    "# #         shuffle=False\n",
    "# #     )\n",
    "    \n",
    "#     input_x_holder =sess.graph.get_operation_by_name(\"input_example_tensor\").outputs[0]\n",
    "# #check your dnn classifier txt pb to know which operation you should use.\n",
    "# predictions_holder = sess.graph.get_operation_by_name(\"dnn/binary_logistic_head/predictions/probabilities\").outputs[0]\n",
    "    \n",
    "#     predictor = tf.contrib.predictor.from_saved_model(MODEL_DIR)\n",
    "#         model_input = tf.train.Example(features=tf.train.Features( feature={\"words\": tf.train.Feature(int64_list=tf.train.Int64List(value=features_test_set)) })) \n",
    "#         model_input = model_input.SerializeToString()\n",
    "#         output_dict = predictor({\"predictor_inputs\":[model_input]})\n",
    "#         y_predicted = output_dict[\"pred_output_classes\"][0]\n",
    "#         output_dict['scores']\n",
    "\n",
    "#         input_tensor=tf.get_default_graph().get_tensor_by_name(\"input_tensors:0\")\n",
    "#         model_input=input_tensor.SerializeToString()        \n",
    "#         output_dict= predictor({\"inputs\":[model_input]})\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean-up resources\n",
    "ws.delete(delete_dependent_resources=True)\n",
    "\n",
    "# optionally, delete the Azure Managed Compute cluster\n",
    "compute_target.delete()\n",
    "\n",
    "# Clean-up temporal local-copy of script, model and data files\n",
    "shutil.rmtree(SCRIPT_DIR)\n",
    "shutil.rmtree(DATA_DIR)\n",
    "shutil.rmtree(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "* [Fine-tune natural language processing models using Azure Machine Learning service](https://azure.microsoft.com/en-us/blog/fine-tune-natural-language-processing-models-using-azure-machine-learning-service/)\n",
    "* [Training, hyperparameter tune, and deploy with TensorFlow](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/training-with-deep-learning/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml",
   "language": "python",
   "name": "aml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
