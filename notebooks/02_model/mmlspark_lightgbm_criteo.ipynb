{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM: A Highly Efficient Gradient Boosting Decision Tree\n",
    "This notebook will give you a quick example of how to train LightGBM model on Spark and deploy it using MML Spark for a content personalization scenario.<br> \n",
    "LightGBM \\[1\\] is a gradient boosting framework that uses tree-based learning algorithms.<br>\n",
    "MML Spark \\[2\\] allows LightGBM to be called in a Spark environment which provides several advantages:\n",
    "- Distributed computation for model development\n",
    "- Easy integration into existing Spark workflows\n",
    "- Model serving through Spark Serving \\[3\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Settings and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34) \n",
      "[GCC 7.3.0]\n",
      "PySpark version: 2.3.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from tempfile import TemporaryDirectory\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "import pyspark\n",
    "from pyspark.ml.feature import FeatureHasher\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import FloatType\n",
    "import requests\n",
    "\n",
    "from reco_utils.common.spark_utils import start_or_get_spark\n",
    "from reco_utils.common.notebook_utils import is_databricks\n",
    "from reco_utils.dataset.criteo_dac import load_spark_df\n",
    "from reco_utils.dataset.spark_splitters import spark_random_split\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"PySpark version: {}\".format(pyspark.version.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup MML Spark\n",
    "if not is_databricks():\n",
    "    spark = start_or_get_spark(packages=['Azure:mmlspark:0.16'])\n",
    "\n",
    "from mmlspark import ComputeModelStatistics\n",
    "from mmlspark import DiscreteHyperParam\n",
    "from mmlspark import HyperparamBuilder\n",
    "from mmlspark import LightGBMClassifier\n",
    "from mmlspark import RandomSpace\n",
    "from mmlspark import RangeHyperParam\n",
    "from mmlspark import TuneHyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "The Criteo Display Advertising Challenge (DAC) dataset [3] is a well-known industry benchmarking dataset for developing CTR prediction models, and is used frequently by research papers. The original dataset is too large for a lightweight demo, so we use a smaller sample for a demo dataset. <br><br>\n",
    "The sample data consist of 100,000 rows with 1 label column and 39 feature columns, where 13 columns are integer values (int00-int12) and 26 columns are categorical features (cat00-cat25).<br><br>\n",
    "What the columns represent is not provided, but for this case we can consider the integer and categorical values as features representing the user and / or item content. The label is binary and indicates a user interaction with an item, so this is a useful dataset to demonstrate how to build a model that will predict likelihood of a user interacting with an item based on the user and item content features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = load_spark_df(size='sample', spark=spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Processing\n",
    "The feature data provided has many missing values across both integer and categorical feature fields. In addition the categorical features have many distinct values, so effectively cleaning and representing the feature data is an important step prior to training a model.<br>\n",
    "One of the simplest ways of managing both features that have missing values as well as high cardinality is to use the hashing trick. The FeatureHasher transformer will pass integer values through and will hash categorical features into a sparse vector of lower dimensionality which can be used effectively by LightGBM.<br>\n",
    "Lastly the dataset is split randomly for training and testing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [c for c in raw_data.columns if c != 'label']\n",
    "feature_processor = FeatureHasher(inputCols=columns, outputCol='features')\n",
    "data = feature_processor.transform(raw_data)\n",
    "train, test = spark_random_split(data, ratio=0.75, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "In MML Spark the LightGBM implementation for binary classification is invoked using the LightGBMClassifier class and specifying the objective as 'binary'. In this instance the occurrence of positive labels is quite low, so setting the isUnbalance flag to true helps account for this imbalance.<br>\n",
    "\n",
    "### Hyper-parameters\n",
    "Key hyper-parameters \\[5\\] for LightGBM classifier on Spark are the number of leaves (numLeaves) in each tree, the number of iterations (numIterations) for training, the learning rate (learningRate) and the fraction of features used during training a tree (featureFraction). Lastly, early stopping round (earlyStoppingRound) can be useful to stop learning at the point where overfitting can begin to occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "NUM_LEAVES = 64\n",
    "NUM_ITERATIONS = 100\n",
    "LEARNING_RATE = 0.15\n",
    "FEATURE_FRACTION = 0.8\n",
    "EARLY_STOPPING_ROUND = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = LightGBMClassifier(\n",
    "    labelCol='label',\n",
    "    featuresCol='features',\n",
    "    objective='binary',\n",
    "    isUnbalance=True,\n",
    "    boostingType='gbdt',\n",
    "    boostFromAverage=True,\n",
    "    numLeaves=NUM_LEAVES,\n",
    "    numIterations=NUM_ITERATIONS,\n",
    "    learningRate=LEARNING_RATE,\n",
    "    featureFraction=FEATURE_FRACTION,\n",
    "    earlyStoppingRound=EARLY_STOPPING_ROUND,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+\n",
      "|evaluation_type|               AUC|\n",
      "+---------------+------------------+\n",
      "| Classification|0.6716842093722328|\n",
      "+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = lgbm.fit(train)\n",
    "\n",
    "evaluator = (\n",
    "    ComputeModelStatistics()\n",
    "    .setScoredLabelsCol(\"prediction\")\n",
    "    .setLabelCol(\"label\")\n",
    "    .setEvaluationMetric(\"AUC\")\n",
    ")\n",
    "\n",
    "predictions = model.transform(test)\n",
    "evaluator.transform(predictions).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tuning\n",
    "\n",
    "MML Spark supports hyper-parameter tuning from a specified space of parameters which can be randomly sampled (or sampled from a grid of options) from continuous or discrete ranges of values. TuneHyperparameters can apply n-fold cross-validation with the given evaluation metric to more robustly identify the best set of parameters to use for the given model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = (\n",
    "    HyperparamBuilder()\n",
    "    .addHyperparam(lgbm, lgbm.learningRate, RangeHyperParam(0.001, 1.0))\n",
    "    .addHyperparam(lgbm, lgbm.numIterations, RangeHyperParam(10, 100))\n",
    "    .addHyperparam(lgbm, lgbm.numLeaves, DiscreteHyperParam([32, 64, 128]))\n",
    ").build()\n",
    "paramSpace = RandomSpace(params).space()\n",
    "\n",
    "tuner = TuneHyperparameters(\n",
    "    evaluationMetric=\"AUC\", \n",
    "    models=[lgbm], \n",
    "    numFolds=5,\n",
    "    numRuns=10, \n",
    "    parallelism=1,\n",
    "    paramSpace=paramSpace, \n",
    "    seed=42\n",
    ")\n",
    "\n",
    "bestModel = tuner.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bestModel.getBestModelInfo())\n",
    "print(bestModel.getBestModel())\n",
    "\n",
    "predictions = bestModel.transform(test)\n",
    "evaluator.transform(predictions).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Saving and Loading\n",
    "The model can be saved and reloaded for use in another workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with TemporaryDirectory() as tmp:\n",
    "    save_file = os.path.join(tmp, r'finished.model')\n",
    "    model.save(save_file)\n",
    "    loaded_model = model.load(save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+\n",
      "|evaluation_type|               AUC|\n",
      "+---------------+------------------+\n",
      "| Classification|0.6716842093722328|\n",
      "+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Re-evaluate the performance again\n",
    "predictions = loaded_model.transform(test)\n",
    "evaluator.transform(predictions).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment\n",
    "MML Spark provides an easy way to quickly spin up a server to deploy trained models built on top of Spark Streaming DataFrames. In this example the server reads a request, parses it to the same input as the original raw data and applies feature processing then computes the probability of engagement given the user and item features provided. This probability is written back as a response to the original request.<br><br>\n",
    "Content-based personalization can be accomplished by leveraging this engagement prediction service as the key machine-learning component inside a larger system. To personalize content for a user, a set of items is selected for evaluation and item-content features are extracted for each. These item features can be combined with the user features for each user-item combination and sent to the engagement prediction service which evaluates the probability that a user will engage with each item. The probability can be used to rank the items and select the top-k desired results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define spark serving input\n",
    "input_df = (\n",
    "    spark.readStream.server()\n",
    "    .address(\"localhost\", 8089, \"predict\")\n",
    "    .load()\n",
    "    .parseRequest(raw_data.schema)\n",
    ")\n",
    "\n",
    "# Process features and make predictions\n",
    "get_pos_prob = udf(lambda x: float(x[1]))\n",
    "\n",
    "processed_df = feature_processor.transform(input_df)\n",
    "output_df = (\n",
    "    loaded_model.transform(processed_df)\n",
    "    .withColumn('p_eng', get_pos_prob(col('probability')).cast(FloatType()))\n",
    "    .makeReply(\"p_eng\")\n",
    ")\n",
    "\n",
    "# Define spark serving output and start server\n",
    "checkpoint = TemporaryDirectory()\n",
    "server = (\n",
    "    output_df.writeStream.server()\n",
    "    .replyTo(\"predict\")\n",
    "    .queryName(\"prediction\")\n",
    "    .option(\"checkpointLocation\", \"file://{}\".format(checkpoint.name))\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Waiting for data to arrive',\n",
       " 'isDataAvailable': False,\n",
       " 'isTriggerActive': False}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "server.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response {\"p_eng\":0.16379395}\n"
     ]
    }
   ],
   "source": [
    "query = raw_data.limit(1).collect()[0].asDict()\n",
    "r = requests.post(data=query, url=\"http://localhost:8089/predict\")\n",
    "print(\"Response {}\".format(r.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Stopped', 'isDataAvailable': False, 'isTriggerActive': False}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleanup\n",
    "server.stop()\n",
    "checkpoint.cleanup()\n",
    "server.status"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (reco_pyspark)",
   "language": "python",
   "name": "reco_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
