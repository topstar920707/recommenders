{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content Based Personalization\n",
    "## LightGBM on Azure Databricks<br>\n",
    "This notebook provides a quick example of how to train LightGBM model on Azure Databricks and deploy it using MML Spark for a content personalization scenario.<br><br>\n",
    "LightGBM \\[1\\] is a gradient boosting framework that uses tree-based learning algorithms.<br>\n",
    "MML Spark \\[2\\] allows LightGBM to be called in a Spark environment which provides several advantages:\n",
    "- Distributed computation for model development\n",
    "- Easy integration into existing Spark workflows\n",
    "- Model serving through Spark Serving \\[3\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Settings and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34) \n",
      "[GCC 7.3.0]\n",
      "PySpark version: 2.3.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from tempfile import TemporaryDirectory\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "import pyspark\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.ml.feature import FeatureHasher\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import FloatType\n",
    "import requests\n",
    "\n",
    "from reco_utils.common.spark_utils import start_or_get_spark\n",
    "from reco_utils.common.notebook_utils import is_databricks\n",
    "from reco_utils.dataset.criteo import load_spark_df\n",
    "from reco_utils.dataset.spark_splitters import spark_random_split\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"PySpark version: {}\".format(pyspark.version.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup MML Spark\n",
    "if not is_databricks():\n",
    "    spark = start_or_get_spark(packages=['Azure:mmlspark:0.16'])\n",
    "\n",
    "from mmlspark import ComputeModelStatistics\n",
    "from mmlspark import DiscreteHyperParam\n",
    "from mmlspark import HyperparamBuilder\n",
    "from mmlspark import LightGBMClassifier\n",
    "from mmlspark import RandomSpace\n",
    "from mmlspark import RangeHyperParam\n",
    "from mmlspark import TuneHyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "The Criteo Display Advertising Challenge (DAC) dataset [3] is a well-known industry benchmarking dataset for developing CTR prediction models, and is used frequently by research papers. The original dataset contains over 45M rows and provides a good challenge for scalable machine learning. The dataset has a binary label column and 38 feature columns, where 13 columns are integer values (int00-int12) and 25 columns are categorical features (cat00-cat24). There is also a sample data consist of 100,000 rows which can be used by setting DATA_SIZE = 'sample'.<br><br>\n",
    "What the columns represent is not provided, but for this case we can consider the integer and categorical values as features representing the user and / or item content. The label is binary and indicates a user interaction with an item, so this is a useful dataset to demonstrate how to build a model that will predict likelihood of a user interacting with an item based on the user context and item content features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SIZE = 'sample'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8.79MB [00:08, 1.04MB/s]                            \n"
     ]
    }
   ],
   "source": [
    "raw_data = load_spark_df(size=DATA_SIZE, spark=spark, local_cache_path='../..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Processing\n",
    "The feature data provided has many missing values across both integer and categorical feature fields. In addition the categorical features have many distinct values, so effectively cleaning and representing the feature data is an important step prior to training a model.<br><br>\n",
    "One of the simplest ways of managing both features that have missing values as well as high cardinality is to use the hashing trick. The FeatureHasher transformer will pass integer values through and will hash categorical features into a sparse vector of lower dimensionality which can be used effectively by LightGBM.<br><br>\n",
    "Lastly the dataset is split randomly for training and testing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [c for c in raw_data.columns if c != 'label']\n",
    "feature_processor = FeatureHasher(inputCols=columns, outputCol='features')\n",
    "data = feature_processor.transform(raw_data)\n",
    "train, test = spark_random_split(data, ratio=0.8, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data to tables to finalize feature transformation\n",
    "train.write.mode('overwrite').saveAsTable('train')\n",
    "test.write.mode('overwrite').saveAsTable('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = spark.table('train')\n",
    "test = spark.table('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "In MML Spark the LightGBM implementation for binary classification is invoked using the LightGBMClassifier class and specifying the objective as 'binary'. In this instance the occurrence of positive labels is quite low, so setting the isUnbalance flag to true helps account for this imbalance.<br><br>\n",
    "\n",
    "### Hyper-parameters\n",
    "Below are some of the key hyper-parameters \\[5\\] for training a LightGBM classifier on Spark\n",
    "- numLeaves: the number of leaves in each tree\n",
    "- numIterations: the number of iterations to apply boosting\n",
    "- learningRate: the learning rate for training across trees\n",
    "- featureFraction: the fraction of features used for training a tree\n",
    "- earlyStoppingRound: round at which early stopping can be applied to avoid overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "NUM_LEAVES = 32\n",
    "NUM_ITERATIONS = 10\n",
    "LEARNING_RATE = 0.15\n",
    "FEATURE_FRACTION = 0.8\n",
    "EARLY_STOPPING_ROUND = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = LightGBMClassifier(\n",
    "    labelCol='label',\n",
    "    featuresCol='features',\n",
    "    objective='binary',\n",
    "    isUnbalance=True,\n",
    "    boostingType='gbdt',\n",
    "    boostFromAverage=True,\n",
    "    numLeaves=NUM_LEAVES,\n",
    "    numIterations=NUM_ITERATIONS,\n",
    "    learningRate=LEARNING_RATE,\n",
    "    featureFraction=FEATURE_FRACTION,\n",
    "    earlyStoppingRound=EARLY_STOPPING_ROUND,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = lgbm.fit(train)\n",
    "predictions = model.transform(test)\n",
    "predictions.cache.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+\n",
      "|evaluation_type|          accuracy|\n",
      "+---------------+------------------+\n",
      "| Classification|0.7075339448071455|\n",
      "+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluator = (\n",
    "    ComputeModelStatistics()\n",
    "    .setScoredLabelsCol(\"prediction\")\n",
    "    .setLabelCol(\"label\")\n",
    "    .setEvaluationMetric(\"accuracy\")\n",
    ")\n",
    "\n",
    "evaluator.transform(predictions).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tuning\n",
    "\n",
    "MML Spark supports hyper-parameter tuning from a specified space of parameters which can be randomly sampled (or sampled from a grid of options) from continuous or discrete ranges of values. TuneHyperparameters can apply n-fold cross-validation with the given evaluation metric to more robustly identify the best set of parameters to use for the given model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FOLDS = 3\n",
    "NUM_RUNS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = (\n",
    "    HyperparamBuilder()\n",
    "    .addHyperparam(lgbm, lgbm.learningRate, RangeHyperParam(0.001, 1.0))\n",
    "    .addHyperparam(lgbm, lgbm.numIterations, RangeHyperParam(10, 100))\n",
    "    .addHyperparam(lgbm, lgbm.numLeaves, DiscreteHyperParam([32, 64, 128]))\n",
    ").build()\n",
    "paramSpace = RandomSpace(params).space()\n",
    "\n",
    "tuner = TuneHyperparameters(\n",
    "    evaluationMetric=\"AUC\", \n",
    "    models=[lgbm], \n",
    "    numFolds=NUM_FOLDS,\n",
    "    numRuns=NUM_ROUNDS, \n",
    "    parallelism=1,\n",
    "    paramSpace=paramSpace, \n",
    "    seed=42\n",
    ")\n",
    "\n",
    "best_model = tuner.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = best_model.transform(test)\n",
    "evaluator.transform(predictions).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Saving and Loading\n",
    "The full pipeline for operating on raw data including feature processing and model prediction can be saved and reloaded for use in another workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with TemporaryDirectory() as tmp:\n",
    "    save_file = os.path.join(tmp, r'finished.model')\n",
    "    pipeline = PipelineModel(stages=[feature_processor, model])\n",
    "    pipeline.save(save_file)\n",
    "    loaded_pipeline = PipelineModel.load(save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+\n",
      "|evaluation_type|accuracy|\n",
      "+---------------+--------+\n",
      "| Classification|   0.739|\n",
      "+---------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the loaded model on raw data\n",
    "predictions = loaded_pipeline.transform(raw_data.limit(1000))\n",
    "evaluator.transform(predictions).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment\n",
    "MML Spark provides an easy way to quickly spin up a server to deploy trained models built on top of Spark Streaming DataFrames. In this example the server reads a request, parses it to the same input as the original raw data and applies feature processing then computes the probability of engagement given the user and item features provided. This probability is written back as a response to the original request.<br><br>\n",
    "Content-based personalization can be accomplished by leveraging this engagement prediction service as the key machine-learning component inside a larger system. To personalize content for a user, a set of items is selected for evaluation and item-content features are extracted for each. These item features can be combined with the user features for each user-item combination and sent to the engagement prediction service which evaluates the probability that a user will engage with each item. The probability can be used to rank the items and select the top-k desired results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define spark serving input\n",
    "input_df = (\n",
    "    spark.readStream.server()\n",
    "    .address(\"localhost\", 8089, \"predict\")\n",
    "    .load()\n",
    "    .parseRequest(raw_data.schema)\n",
    ")\n",
    "\n",
    "# Process features and reply with the probability of engagement\n",
    "get_p_eng = udf(lambda p: float(p.toArray()[1]))\n",
    "output_df = (\n",
    "    loaded_pipeline.transform(input_df)\n",
    "    .withColumn(\"p_eng\", get_p_eng(col(\"probability\")))\n",
    "    .makeReply(\"p_eng\")\n",
    ")\n",
    "\n",
    "# Define spark serving output and start server\n",
    "checkpoint = TemporaryDirectory()\n",
    "server = (\n",
    "    output_df.writeStream.server()\n",
    "    .replyTo(\"predict\")\n",
    "    .queryName(\"prediction\")\n",
    "    .option(\"checkpointLocation\", \"file://{}\".format(checkpoint.name))\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Waiting for data to arrive',\n",
       " 'isDataAvailable': False,\n",
       " 'isTriggerActive': False}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "server.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 0.7075531769538851\n"
     ]
    }
   ],
   "source": [
    "query = raw_data.limit(1).collect()[0].asDict()\n",
    "r = requests.post(json=query, url=\"http://localhost:8089/predict\")\n",
    "print(\"Response {}\".format(r.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Stopped', 'isDataAvailable': False, 'isTriggerActive': False}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleanup\n",
    "server.stop()\n",
    "checkpoint.cleanup()\n",
    "server.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
