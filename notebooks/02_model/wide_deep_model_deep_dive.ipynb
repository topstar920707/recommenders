{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wide and Deep Model for Movie Recommendation\n",
    "```\n",
    "TODO details about wide-deep\n",
    "```\n",
    "\n",
    "[Wide-deep model](https://arxiv.org/abs/1606.07792) for a recommender system, and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisite\n",
    "* tensorflow (version 1.10 or higher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "from reco_utils.common import tf_utils\n",
    "from reco_utils.dataset import movielens\n",
    "from reco_utils.dataset.python_splitters import python_random_split\n",
    "from reco_utils.evaluation.python_evaluation import (\n",
    "    rmse, mae, rsquared, exp_var,\n",
    "    map_at_k, ndcg_at_k, precision_at_k, recall_at_k\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(\"Tensorflow Version:\", tf.__version__)\n",
    "\n",
    "devices = device_lib.list_local_devices()\n",
    "[x.name for x in devices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# top k items to recommend\n",
    "TOP_K = 10\n",
    "\n",
    "# Select Movielens data size: 100k, 1m, 10m, or 20m\n",
    "MOVIELENS_DATA_SIZE = '100k'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading\n",
    "\n",
    "Download [MovieLens](https://grouplens.org/datasets/movielens/) data and split train / test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = movielens.load_pandas_df(\n",
    "    size=MOVIELENS_DATA_SIZE,\n",
    "    header=['UserId','MovieId','Rating','Timestamp'],\n",
    ")\n",
    "print(data.head())\n",
    "\n",
    "train, test = python_random_split(data, ratio=0.75, seed=123)\n",
    "\n",
    "X_train = train.copy()\n",
    "y_train = X_train.pop('Rating')\n",
    "X_test = test.copy()\n",
    "y_test = X_test.pop('Rating')\n",
    "\n",
    "# Distinct users and items (use for feature embedding)\n",
    "user_list = data['UserId'].unique()\n",
    "item_list = data['MovieId'].unique()\n",
    "print(\"#Users =\", len(user_list))\n",
    "print(\"#Items =\", len(item_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `wide` - Linear model\n",
    "* `deep` - DNN model\n",
    "* `wide_deep` - Linear combination of the linear and DNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Hyper parameters\n",
    "\"\"\"\n",
    "# Model checkpoints folder\n",
    "MODEL_DIR = './models'\n",
    "\n",
    "MODEL_TYPE = 'wide'\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "LINEAR_OPTIMIZER = 'SGD'\n",
    "LINEAR_OPTIMIZER_LR = 0.001\n",
    "DNN_OPTIMIZER = 'Adagrad'\n",
    "DNN_OPTIMIZER_LR = 0.001\n",
    "DNN_HIDDEN_UNITS = [256, 256, 128]\n",
    "DNN_DROPOUT = None\n",
    "DNN_BATCH_NORM = False\n",
    "\n",
    "# Rule of thumb for embedding_dimensions =  number_of_categories ** 0.25\n",
    "DNN_USER_DIM = int(len(user_list) ** 0.25)\n",
    "DNN_ITEM_DIM = int(len(item_list) ** 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature embedding\n",
    "\n",
    "Wide and deep model utilizes two different types of feature set: 1) a wide set of cross-producted features to capture how the co-occurrence of a query-item feature pair correlates with the target label or rating, and 2) a deep, lower-dimensional embedding vectors for every query and item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_columns = []\n",
    "deep_columns = []\n",
    "\n",
    "if MODEL_TYPE == 'wide' or MODEL_TYPE == 'wide_deep':\n",
    "    wide_columns = tf_utils.build_feature_columns(\n",
    "        'wide', user_list, item_list, 'UserId', 'MovieId'\n",
    "    )\n",
    "if MODEL_TYPE == 'deep' or MODEL_TYPE == 'wide_deep':\n",
    "    deep_columns = tf_utils.build_feature_columns(\n",
    "        'deep', user_list, item_list, 'UserId', 'MovieId', None, 'Timestamp',\n",
    "        DNN_USER_DIM, DNN_ITEM_DIM, 0\n",
    "    )\n",
    "    print(\"Embedding {} users to {}-dim vector\".format(len(user_list), DNN_USER_DIM))\n",
    "    print(\"Embedding {} items to {}-dim vector\".format(len(item_list), DNN_ITEM_DIM))\n",
    "\n",
    "# To check features\n",
    "# TODO\n",
    "# features = tf.parse_example(..., features=make_parse_example_spec(columns))\n",
    "# dense_tensor = input_layer(features, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO set run tf config if needed\n",
    "\n",
    "if MODEL_TYPE == 'wide':\n",
    "    model = tf.estimator.LinearRegressor(  # LinearClassifier(\n",
    "        model_dir=MODEL_DIR,\n",
    "        feature_columns=wide_columns,\n",
    "        optimizer=tf_utils.build_optimizer(LINEAR_OPTIMIZER, LINEAR_OPTIMIZER_LR)\n",
    "    )\n",
    "elif MODEL_TYPE == 'deep':\n",
    "    model = tf.estimator.DNNRegressor(  # DNNClassifier(\n",
    "        model_dir=MODEL_DIR,\n",
    "        feature_columns=deep_columns,\n",
    "        hidden_units=DNN_HIDDEN_UNITS,\n",
    "        optimizer=tf_utils.build_optimizer(DNN_OPTIMIZER, DNN_OPTIMIZER_LR),\n",
    "        dropout=DNN_DROPOUT,\n",
    "        batch_norm=DNN_BATCH_NORM\n",
    "    )\n",
    "elif MODEL_TYPE == 'wide_deep':\n",
    "    model = tf.estimator.DNNLinearCombinedRegressor(  # DNNLinearCombinedClassifier(\n",
    "        model_dir=MODEL_DIR,\n",
    "        # wide settings\n",
    "        linear_feature_columns=wide_columns,\n",
    "        linear_optimizer=tf_utils.build_optimizer(LINEAR_OPTIMIZER, LINEAR_OPTIMIZER_LR),\n",
    "        # deep settings\n",
    "        dnn_feature_columns=deep_columns,\n",
    "        dnn_hidden_units=DNN_HIDDEN_UNITS,\n",
    "        dnn_optimizer=tf_utils.build_optimizer(DNN_OPTIMIZER, DNN_OPTIMIZER_LR),\n",
    "        dnn_dropout=DNN_DROPOUT,\n",
    "        batch_norm=DNN_BATCH_NORM\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\"Model type should be either 'wide', 'deep', or 'wide_deep'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
    "    x=X_train,\n",
    "    y=y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    shuffle=True,\n",
    "    num_threads=1\n",
    ")\n",
    "\n",
    "# Add additional evaluation metrics\n",
    "metrics_fn = tf_utils.eval_metrics('rmse')\n",
    "model = tf.contrib.estimator.add_metrics(model, metrics_fn)\n",
    "\n",
    "monitors = [tf.contrib.learn.monitors.ValidationMonitor(\n",
    "    input_fn=train_input_fn,\n",
    "    every_n_steps=100\n",
    ")]\n",
    "hooks = tf.contrib.learn.monitors.replace_monitors_with_hooks(monitors, model)\n",
    "\n",
    "# validation_metrics = {\n",
    "#     'rmse': tf.contrib.learn.MetricSpec(\n",
    "#         metric_fn=metrics_fn,\n",
    "#         prediction_key='predictions'\n",
    "#     )\n",
    "# }\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "# logging_hook = tf.train.LoggingTensorHook(validation_metrics, every_n_iter=10, at_end=True)\n",
    "\n",
    "\n",
    "#   tf.add_to_collection('losses', cross_entropy_mean)\n",
    "\n",
    "#   # The total loss is defined as the cross entropy loss plus all of the weight\n",
    "#   # decay terms (L2 loss).\n",
    "#   return tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
    "\n",
    "#     tf.train.LoggingTensorHook(\n",
    "#         tensors=tf.get_default_graph().get_tensor_by_name('rmse'),\n",
    "#         every_n_iter=100,\n",
    "#         at_end=True\n",
    "#     )\n",
    "\n",
    "model.train(input_fn=train_input_fn, hooks=hooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "We predict the ratings by using the wide-deep model we trained. Finally, we also generate top-k movie recommentation for each user and test the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO maybe drop the cold users and items\n",
    "test_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
    "    x=X_test,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_epochs=1,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "pred_list = [p['predictions'][0] for p in list(model.predict(input_fn=test_input_fn))]\n",
    "predictions = test.copy()\n",
    "predictions['prediction']  = pd.Series(pred_list).values\n",
    "print(predictions.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Item rating prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = {\n",
    "    'col_user': 'UserId',\n",
    "    'col_item': 'MovieId',\n",
    "    'col_rating': 'Rating',\n",
    "    'col_prediction': 'prediction'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare w/ TF Evaluation\n",
    "result = model.evaluate(input_fn=test_input_fn, steps=None)\n",
    "print(\"\\nEvaluation:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Recommend k items\n",
    "\n",
    "1) Remove seen items and 2) add timestamp info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the cross join of all user-item pairs and score them.\n",
    "user_item_col = ['UserId', 'MovieId']\n",
    "user_item_list = list(itertools.product(user_list, item_list))\n",
    "users_items = pd.DataFrame(user_item_list, columns=user_item_col)\n",
    "print(\"Before excude seen items:\", len(users_items))\n",
    "\n",
    "# Remove seen items (items in the train set)\n",
    "users_items_exclude_train = users_items.loc[\n",
    "    ~users_items.set_index(user_item_col).index.isin(X_train.set_index(user_item_col).index)\n",
    "]\n",
    "print(\"After excude seen items:\", len(users_items_exclude_train))\n",
    "\n",
    "# Add timestamp info\n",
    "X_rank_eval = pd.merge(X_test, users_items_exclude_train, on=user_item_col, how='outer')\n",
    "X_rank_eval.fillna(X_test['Timestamp'].max(), inplace=True) \n",
    "print(X_rank_eval.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = list(model.predict(\n",
    "    input_fn=tf.estimator.inputs.pandas_input_fn(\n",
    "        x=X_rank_eval,\n",
    "        batch_size=100,\n",
    "        num_epochs=1,\n",
    "        shuffle=False\n",
    "    )\n",
    "))\n",
    "reco = X_rank_eval.copy()\n",
    "reco['prediction'] = pd.Series([p['predictions'][0] for p in predictions]).values\n",
    "\n",
    "# TODO for now, fix TOP_K\n",
    "TOP_K = 10\n",
    "eval_ndcg = ndcg_at_k(test, reco, k=TOP_K, **cols)\n",
    "\n",
    "print(\"ndcg:\", eval_ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "eval_map = map_at_k(test, users_items_exclude_train, k=k, **cols)\n",
    "eval_ndcg = ndcg_at_k(test, users_items_exclude_train, k=k, **cols)\n",
    "eval_precision = precision_at_k(test, users_items_exclude_train, k=k, **cols)\n",
    "eval_recall = recall_at_k(test, users_items_exclude_train, k=k, **cols)\n",
    "\n",
    "print(\"MAP:\\t%f\" % eval_map,\n",
    "      \"NDCG:\\t%f\" % eval_ndcg,\n",
    "      \"Precision@K:\\t%f\" % eval_precision,\n",
    "      \"Recall@K:\\t%f\" % eval_recall, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export and reload model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORT_DIR = './saved_model'\n",
    "os.makedirs(EXPORT_DIR, exist_ok=True)\n",
    "\n",
    "# maybe use build_supervised_input_receiver_fn_from_input_fn\n",
    "train_rcvr_fn = tf.contrib.estimator.build_supervised_input_receiver_fn_from_input_fn(\n",
    "    train_input_fn\n",
    ")\n",
    "\n",
    "serve_rcvr_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(\n",
    "    tf.feature_column.make_parse_example_spec(feat_columns)\n",
    ")\n",
    "\n",
    "rcvr_fn_map = {\n",
    "    tf.estimator.ModeKeys.TRAIN: train_rcvr_fn,\n",
    "    tf.estimator.ModeKeys.EVAL: train_rcvr_fn,\n",
    "    tf.estimator.ModeKeys.PREDICT: serve_rcvr_fn\n",
    "}\n",
    "\n",
    "export_dir = tf.contrib.estimator.export_all_saved_models(\n",
    "    model,\n",
    "    export_dir_base=EXPORT_DIR,\n",
    "    input_receiver_fn_map=rcvr_fn_map\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = tf.contrib.estimator.SavedModelEstimator(export_dir)\n",
    "\n",
    "result = saved_model.evaluate(input_fn=test_input_fn, steps=None)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_input_fn():\n",
    "    examples = []\n",
    "    for index, row in test_x.iterrows():\n",
    "        feature = {}\n",
    "        for col, value in row.iteritems():\n",
    "            feature[col] =  tf.train.Feature(bytes_list=tf.train.BytesList(value=[bytes(str(value), encoding='ascii')]))\n",
    "        example = tf.train.Example(\n",
    "            features=tf.train.Features(\n",
    "                feature=feature\n",
    "            )\n",
    "        )\n",
    "        examples.append(example.SerializeToString())\n",
    "    return {'inputs': tf.constant(examples)}\n",
    "       \n",
    "# def predict_input_fn():\n",
    "#     example = tf.train.Example()\n",
    "#     example.features.feature['UserId'].bytes_list.value.extend(['496'])\n",
    "#     example.features.feature['MovieId'].bytes_list.value.extend(['136'])\n",
    "#     return {'inputs': tf.constant([example.SerializeToString()])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Convert input data into serialized Example strings.\n",
    "\n",
    "\n",
    "# features = tf.parse_example(\n",
    "#     serialized=serialized_examples,\n",
    "#     features=make_parse_example_spec(feature_columns))\n",
    "# predictions_dict = next(prediction)\n",
    "# predictions_dict\n",
    "# pred_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
    "#     x=test_x,\n",
    "#     num_epochs=1,\n",
    "#     shuffle=False\n",
    "# )\n",
    "\n",
    "predictions = saved_model.predict(predict_input_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "shutil.rmtree(EXPORT_DIR)\n",
    "shutil.rmtree(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/MtDersvan/tf_playground/blob/master/wide_and_deep_tutorial/wide_and_deep_export_r1.3.ipynb\n",
    "https://github.com/Azure/MachineLearningNotebooks/tree/master/how-to-use-azureml/training-with-deep-learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml",
   "language": "python",
   "name": "aml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
