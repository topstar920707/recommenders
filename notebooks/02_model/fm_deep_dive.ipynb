{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factorization Machine Deep Dive\n",
    "\n",
    "Factorization machine (FM) is one of the representative algorithms that are used for building content-based recommender model. The algorithm is powerful in terms of capturing the effects of not just the input features but also their interactions. The algorithm provides better generalization capability and expressiveness compared to the other classic algorithms such as SVM. The most recent research extends the basic FM algorithms by using deep learning techniques, which achieve remarkable improvement in a few practical use cases.\n",
    "\n",
    "This notebook presents a deep dive into the Factorization Machine algorithm, and demonstrates some best practices of using the contemporary FM implementations like [`xlearn`](https://github.com/aksnzhy/xlearn) for dealing with tasks like click-through rate prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Factorization Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Factorization Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FM is an algorithm that uses factorization in prediction tasks with data set of high sparsity. The algorithm was original proposed in [\\[1\\]](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf). Traditionally, the algorithms such as SVM failed in dealing with highly sparse data that is usually seen in many contemporary problems, e.g., click-through rate prediction, recommendation, etc. FM handles the problem by modeling not just first-order linear components for predicting the label, but also the cross-product of the feature variables in order to capture more generalized correlation between variables and label. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, usually, the data that appears in recommendation problems can be encoded in the following way - the user, item, and feature vectors are transformed into one-hot representation for building model. For this arrangement, using the classic algorithms like linear regression, SVM, for predicting the target, are that\n",
    "1. The feature vectors are highly sparse, and thus it makes it hard to converge the parameters to fit the model efficienly\n",
    "2. Cross-product of features will be sparse as well, and this in turn, reduces the expressiveness of a model if it is designed to capture the high-order interactions between features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://recodatasets.blob.core.windows.net/images/fm_data.png?sanitize=true\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FM algorithm is designed to tackle the above two problems by factorizing latent vectors that model the low- and high-order components. The general idea of a FM model is expressed in the following equation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{y}(\\textbf{x})=w_{0}+\\sum^{n}_{i=1}w_{i}x_{i}+\\sum^{n}_{i=1}\\sum^{n}_{j=i+1}<\\textbf{v}_{i}, \\textbf{v}_{j}>x_{i}x_{j}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\hat{y}$ and $\\textbf{x}$ are the target to predict and input feature vectors, respectively. $w_{i}$ is the model parameters for the first-order component. $<\\textbf{v}_{i}, \\textbf{v}_{j}>$ is the dot product of two latent factors for the second-order interaction of feature variables, and it is defined as  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$<\\textbf{v}_{i}, \\textbf{v}_{j}>=\\sum^{k}_{f=1}v_{i,f}\\cdot v_{j,f}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to using fixed parameter for the high-order interaction components, using the factorized vectors increase generalization as well as expressiveness of the model. In addition to this, the computation complexity of the model is $O(kn)$ where $k$ and $n$ are the dimensionalities of the factorization vector and input feature vector, respectively. In practice, usually a two-way FM model is used, i.e., only the second-order feature interactions are considered, to favor computational efficiency without loss of model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Field-Aware Factorization Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Field-aware factorization machine (FFM) is an extension to FM. It was originally introduced in [\\[2\\]](https://www.csie.ntu.edu.tw/~cjlin/papers/ffm.pdf). The advantage of FFM over FM is that, it uses different factorized latent factors for different groups of features. The \"group\" is called \"field\" in the context of FFM. Putting features into fields resolves the issue that the latent factors shared by features that intuitively represent different categories of information may not well generalize the correlation. \n",
    "\n",
    "Different from the formula for the 2-order cross product as can be seen above in the FM equation, in the FFM settings, the equation changes to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\theta_{\\text{FFM}}(\\textbf{w}\\textbf{x})=\\sum^{n}_{j1=1}\\sum^{n}_{j2=j1+1}<\\textbf{v}_{j1,f2}, \\textbf{v}_{j2,f1}>x_{j1}x_{j2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $f_1$ and $f_2$ are the fields of $j_1$ and $j_2$, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to FM, the computational complexity increases to $O(n^2k)$. However, since the latent factors in FFM only need to learn the effect within the field, so the $k$ values in FFM is usually much smaller than that in FM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 FM/FFM extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the recent years, FM/FFM extensions were proposed to enhance the model performance further. The new algorithms leverage the powerful deep learning neural network to improve the generalization capability of the original FM/FFM algorithms. Representatives of the such algorithms are summarized as below. Some of them are implemented and demonstrated in the microsoft/recommenders repository. \n",
    "\n",
    "|Algorithm|Notes|References|Example in Microsoft/Recommenders|\n",
    "|--------------------|---------------------|------------------------|\n",
    "|DeepFM|Combination of FM and DNN where DNN handles high-order interactions|[\\[3\\]](https://arxiv.org/abs/1703.04247)|-|\n",
    "|xDeepFM|Combination of FM, DNN, and Compressed Interaction Network, for vectorized feature interactions|[\\[4\\]](https://dl.acm.org/citation.cfm?id=3220023)|[notebook](https://github.com/microsoft/recommenders/blob/master/notebooks/00_quick_start/xdeepfm_criteo.ipynb) / [utilities](https://github.com/microsoft/recommenders/blob/master/reco_utils/recommender/deeprec/models/xDeepFM.py)|\n",
    "|Factorization Machine Supported Neural Network|Use FM user/item weight vectors as input layers for DNN model|[\\[5\\]](https://link.springer.com/chapter/10.1007/978-3-319-30671-1_4)|-|\n",
    "|Product-based Neural Network|An additional product-wise layer between embedding layer and fully connected layer to improve expressiveness of interactions of features across fields|[\\[6\\]](https://ieeexplore.ieee.org/abstract/document/7837964)|-|\n",
    "|Neural Factorization Machines|Improve the factorization part of FM by using stacks of NN layers to improve non-linear expressiveness|[\\[7\\]](https://dl.acm.org/citation.cfm?id=3080777)|-|\n",
    "|Wide and deep|Combination of linear model (wide part) and deep neural network model (deep part) for memorisation and generalization|[\\[8\\]](https://dl.acm.org/citation.cfm?id=2988454)|[notebook](https://github.com/microsoft/recommenders/blob/master/notebooks/00_quick_start/wide_deep_movielens.ipynb) / [utilities](https://github.com/microsoft/recommenders/tree/master/reco_utils/recommender/wide_deep)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Factorization Machine Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table summarizes the implementations of FM/FFM. Some of them (e.g., xDeepFM and VW) are implemented and/or demonstrated in the microsoft/recommenders repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Implementation|Language|Notes|Examples in Microsoft/Recommenders|\n",
    "|-----------------|------------------|------------------|---------------------|\n",
    "|[libfm](https://github.com/srendle/libfm)|C++|Implementation of FM algorithm|-|\n",
    "|[libffm](https://github.com/ycjuan/libffm)|C++|Original implemenation of FFM algorithm. It is handy in model building, but does not support Python interface|-|\n",
    "|[xlearn](https://github.com/aksnzhy/xlearn)|C++ with Python interface|More computationally efficient compared to libffm without loss of modeling effectiveness|[notebook](https://github.com/microsoft/recommenders/blob/master/notebooks/02_model/fm_deep_dive.ipynb)|\n",
    "|[Vowpal Wabbit FM](https://github.com/VowpalWabbit/vowpal_wabbit/wiki/Matrix-factorization-example)|Online library with estimator API|Easy to use by calling API, but flexibility and configurability are limited|[notebook](https://github.com/microsoft/recommenders/blob/master/notebooks/02_model/vowpal_wabbit_deep_dive.ipynb) / [utilities](https://github.com/microsoft/recommenders/tree/master/reco_utils/recommender/vowpal_wabbit)\n",
    "|[microsoft/recommenders xDeepFM](https://github.com/microsoft/recommenders/blob/master/reco_utils/recommender/deeprec/models/xDeepFM.py)|Python|Support flexible interface with different configurations of FM and FM extensions, i.e., LR, FM, and/or CIN|[notebook](https://github.com/microsoft/recommenders/blob/master/notebooks/00_quick_start/xdeepfm_criteo.ipynb) / [utilities](https://github.com/microsoft/recommenders/blob/master/reco_utils/recommender/deeprec/models/xDeepFM.py)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other than `libfm` and `libffm`, all the other three can be used in a Python environment. \n",
    "\n",
    "* A deep dive of using Vowbal Wabbit for FM model can be found [here](https://github.com/microsoft/recommenders/blob/master/notebooks/02_model/vowpal_wabbit_deep_dive.ipynb)\n",
    "* A quick start of Microsoft xDeepFM algorithm can be found [here](https://github.com/microsoft/recommenders/blob/master/notebooks/00_quick_start/xdeepfm_criteo.ipynb). \n",
    "\n",
    "Therefore, in the example below, only code examples and best practices of using `xlearn` are presented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 xlearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setups for using `xlearn`.\n",
    "\n",
    "1. `xlearn` is implemented in C++ and has Python bindings, so it can be directly installed as a Python package from PyPI. The installation of `xlearn` is enabled in the [Recommenders repo environment setup script](https://github.com/microsoft/recommenders/blob/master/scripts/generate_conda_file.py). One can follow the general setup steps to install the environment as required, in which `xlearn` is installed as well.\n",
    "2. NOTE `xlearn` may require some base libraries installed as prerequisites in the system, e.g., `cmake`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a succesful creation of the environment, one can load the packages to run `xlearn` in a Jupyter notebook or Python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import os\n",
    "import papermill as pm\n",
    "from tempfile import TemporaryDirectory\n",
    "import xlearn as xl\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib notebook\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from reco_utils.common.constants import SEED\n",
    "from reco_utils.common.timer import Timer\n",
    "from reco_utils.recommender.deeprec.deeprec_utils import (\n",
    "    download_deeprec_resources, prepare_hparams\n",
    ")\n",
    "from reco_utils.recommender.deeprec.models.xDeepFM import XDeepFMModel\n",
    "from reco_utils.recommender.deeprec.IO.iterator import FFMTextIterator\n",
    "from reco_utils.tuning.parameter_sweep import generate_param_grid\n",
    "from reco_utils.dataset.pandas_df_utils import LibffmConverter\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Xlearn version: {}\".format(xl.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the FM model building, data is usually represented in the libsvm data format. That is, `label feat1:val1 feat2:val2 ...`, where `label` is the target to predict, and `val` is the value to each feature `feat`.\n",
    "\n",
    "FFM algorithm requires data to be represented in the libffm format, where each vector is split into several fields with categorical/numerical features inside. That is, `label field1:feat1:val1 field2:feat2:val2 ...`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Microsoft/Recommenders utility functions, [a libffm converter](https://github.com/microsoft/recommenders/blob/290dd920d4a6a4d3bff71dd9ee7273be0c02dbbc/reco_utils/dataset/pandas_df_utils.py#L86) is provided to achieve the transformation from a tabular feature vectors to the corresponding libffm representation. For example, the following shows how to transform the format of a synthesized data by using the module of `LibffmConverter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_original = pd.DataFrame({\n",
    "    'rating': [1, 0, 0, 1, 1],\n",
    "    'field1': ['xxx1', 'xxx2', 'xxx4', 'xxx4', 'xxx4'],\n",
    "    'field2': [3, 4, 5, 6, 7],\n",
    "    'field3': [1.0, 2.0, 3.0, 4.0, 5.0],\n",
    "    'field4': ['1', '2', '3', '4', '5']\n",
    "})\n",
    "\n",
    "converter = LibffmConverter().fit(df_feature_original, col_rating='rating')\n",
    "df_out = converter.transform(df_feature_original)\n",
    "df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are in total {0} fields and {1} features.'.format(converter.field_count, converter.feature_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the use of `xlearn`, the following example uses the [Criteo data set](https://labs.criteo.com/category/dataset/), which has already been processed in the libffm format, for building and evaluating a FFM model built by using `xlearn`. Sometimes, it is important to know the total numbers of fields and features. When building a FFM model, `xlearn` can count these numbers automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "YAML_FILE_NAME = \"xDeepFM.yaml\"\n",
    "TRAIN_FILE_NAME = \"cretio_tiny_train\"\n",
    "VALID_FILE_NAME = \"cretio_tiny_valid\"\n",
    "TEST_FILE_NAME = \"cretio_tiny_test\"\n",
    "MODEL_FILE_NAME = \"model.out\"\n",
    "OUTPUT_FILE_NAME = \"output.txt\"\n",
    "\n",
    "LEARNING_RATE = 0.2\n",
    "LAMBDA = 0.002\n",
    "# The metrics for binary classification options are \"acc\", \"prec\", \"f1\" and \"auc\"\n",
    "# for regression, options are \"rmse\", \"mae\", \"mape\"\n",
    "METRIC = \"auc\" \n",
    "EPOCH = 10\n",
    "OPT_METHOD = \"sgd\" # options are \"sgd\", \"adagrad\" and \"ftrl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpdir = TemporaryDirectory()\n",
    "\n",
    "data_path = tmpdir.name\n",
    "yaml_file = os.path.join(data_path, YAML_FILE_NAME)\n",
    "train_file = os.path.join(data_path, TRAIN_FILE_NAME)\n",
    "valid_file = os.path.join(data_path, VALID_FILE_NAME)\n",
    "test_file = os.path.join(data_path, TEST_FILE_NAME)\n",
    "model_file = os.path.join(data_path, MODEL_FILE_NAME)\n",
    "output_file = os.path.join(data_path, OUTPUT_FILE_NAME)\n",
    "\n",
    "if not os.path.exists(yaml_file):\n",
    "    download_deeprec_resources(r'https://recodatasets.blob.core.windows.net/deeprec/', data_path, 'xdeepfmresources.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following steps are from the [official documentation of `xlearn`](https://xlearn-doc.readthedocs.io/en/latest/index.html) for building a model. To begin with, we do not modify any training parameter values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE, if `xlearn` is run through command line, the training process can be displayed in the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training task\n",
    "ffm_model = xl.create_ffm()        # Use field-aware factorization machine (ffm)\n",
    "ffm_model.setTrain(train_file)     # Set the path of training dataset\n",
    "ffm_model.setValidate(valid_file)  # Set the path of validation dataset\n",
    "\n",
    "# Parameters:\n",
    "#  0. task: binary classification\n",
    "#  1. learning rate: 0.2\n",
    "#  2. regular lambda: 0.002\n",
    "#  3. evaluation metric: auc\n",
    "#  4. number of epochs: 10\n",
    "#  5. optimization method: sgd\n",
    "param = {\"task\":\"binary\", \n",
    "         \"lr\": LEARNING_RATE, \n",
    "         \"lambda\": LAMBDA, \n",
    "         \"metric\": METRIC,\n",
    "         \"epoch\": EPOCH,\n",
    "         \"opt\": OPT_METHOD\n",
    "        }\n",
    "\n",
    "# Start to train\n",
    "# The trained model will be stored in model.out\n",
    "with Timer() as time_train:\n",
    "    ffm_model.fit(param, model_file)\n",
    "\n",
    "# Prediction task\n",
    "ffm_model.setTest(test_file)  # Set the path of test dataset\n",
    "ffm_model.setSigmoid()        # Convert output to 0-1\n",
    "\n",
    "# Start to predict\n",
    "# The output result will be stored in output.txt\n",
    "with Timer() as time_predict:\n",
    "    ffm_model.predict(model_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output are the predicted labels (i.e., 1 or 0) for the testing data set. AUC score is calculated to evaluate the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file) as f:\n",
    "    predictions = f.readlines()\n",
    "\n",
    "with open(test_file) as f:\n",
    "    truths = f.readlines()\n",
    "\n",
    "truths = np.array([float(truth.split(' ')[0]) for truth in truths])\n",
    "predictions = np.array([float(prediction.strip('')) for prediction in predictions])\n",
    "\n",
    "auc_score = roc_auc_score(truths, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.record('auc_score', auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training takes {0:.2f}s and predicting takes {1:.2f}s.'.format(time_train.interval, time_predict.interval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the model building/scoring process is fast and the model performance is good. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Hyperparameter tuning of `xlearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following presents a naive approach to tune the parameters of `xlearn`, which is using grid-search of parameter values to find the optimal combinations. It is worth noting that the original [FFM paper](https://www.csie.ntu.edu.tw/~cjlin/papers/ffm.pdf) gave some hints in terms of the impact of parameters on the sampled Criteo dataset. \n",
    "\n",
    "The following are the parameters that can be tuned in the `xlearn` implementation of FM/FFM algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Parameter|Description|Default value|Notes|\n",
    "|-------------|-----------------|------------------|-----------------|\n",
    "|`lr`|Learning rate|0.2|Higher learning rate helps fit a model more efficiently but may also result in overfitting.|\n",
    "|`lambda`|Regularization parameter|0.00002|The value needs to be selected empirically to avoid overfitting.|\n",
    "|`k`|Dimensionality of the latent factors|4|In FFM the effect of k is not that significant as the algorithm itself considers field where `k` can be small to capture the effect of features within each of the fields.|\n",
    "|`init`|Model initialization|0.66|-|\n",
    "|`epoch`|Number of epochs|10|Using a larger epoch size will help converge the model to its optimal point|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = {\n",
    "    \"lr\": [0.0001, 0.001, 0.01],\n",
    "    \"lambda\": [0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "param_grid = generate_param_grid(param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_scores = []\n",
    "\n",
    "with Timer() as time_tune:\n",
    "    for param in param_grid:\n",
    "        ffm_model = xl.create_ffm()       \n",
    "        ffm_model.setTrain(train_file)     \n",
    "        ffm_model.setValidate(valid_file)\n",
    "        ffm_model.fit(param, model_file)\n",
    "\n",
    "        ffm_model.setTest(test_file)  \n",
    "        ffm_model.setSigmoid()        \n",
    "        ffm_model.predict(model_file, output_file)\n",
    "\n",
    "        with open(output_file) as f:\n",
    "            predictions = f.readlines()\n",
    "\n",
    "        with open(test_file) as f:\n",
    "            truths = f.readlines()\n",
    "\n",
    "        truths = np.array([float(truth.split(' ')[0]) for truth in truths])\n",
    "        predictions = np.array([float(prediction.strip('')) for prediction in predictions])\n",
    "\n",
    "        auc_scores.append(roc_auc_score(truths, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tuning by grid search takes {0:.2} min'.format(time_tune.interval / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_scores = [float('%.4f' % x) for x in auc_scores]\n",
    "auc_scores_array = np.reshape(auc_scores, (len(param_dict[\"lr\"]), len(param_dict[\"lambda\"]))) \n",
    "\n",
    "auc_df = pd.DataFrame(\n",
    "    data=auc_scores_array, \n",
    "    index=pd.Index(param_dict[\"lr\"], name=\"LR\"), \n",
    "    columns=pd.Index(param_dict[\"lambda\"], name=\"Lambda\")\n",
    ")\n",
    "auc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(auc_df, cbar=False, annot=True, fmt=\".4g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More advanced tuning methods like Bayesian Optimization can be used for searching for the optimal model efficiently. The benefit of using, for example, `HyperDrive` from Azure Machine Learning Services, for tuning the parameters, is that, the tuning tasks can be distributed across nodes of a cluster and the optimization can be run concurrently to save the total cost.\n",
    "\n",
    "* Details about how to tune hyper parameters by using Azure Machine Learning Services can be found [here](https://github.com/microsoft/recommenders/tree/master/notebooks/04_model_select_and_optimize).\n",
    "* Note, to enable the tuning task on Azure Machine Learning Services by using HyperDrive, one needs a Docker image to containerize the environment where `xlearn` can be run. The Docker file provided [here](https://github.com/microsoft/recommenders/tree/master/docker) can be used for such purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpdir.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='references'></a>\n",
    "1. Rendle, Steffen. \"Factorization machines.\" 2010 IEEE International Conference on Data Mining. IEEE, 2010.\n",
    "2. Juan, Yuchin, et al. \"Field-aware factorization machines for CTR prediction.\" Proceedings of the 10th ACM Conference on Recommender Systems. ACM, 2016.\n",
    "3. Guo, Huifeng, et al. \"DeepFM: a factorization-machine based neural network for CTR prediction.\" arXiv preprint arXiv:1703.04247 (2017).\n",
    "4. Lian, Jianxun, et al. \"xdeepfm: Combining explicit and implicit feature interactions for recommender systems.\" Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM, 2018.\n",
    "5. Qu, Yanru, et al. \"Product-based neural networks for user response prediction.\" 2016 IEEE 16th International Conference on Data Mining (ICDM). IEEE, 2016.\n",
    "6. Zhang, Weinan, Tianming Du, and Jun Wang. \"Deep learning over multi-field categorical data.\" European conference on information retrieval. Springer, Cham, 2016.\n",
    "7. He, Xiangnan, and Tat-Seng Chua. \"Neural factorization machines for sparse predictive analytics.\" Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval. ACM, 2017.\n",
    "8. Cheng, Heng-Tze, et al. \"Wide & deep learning for recommender systems.\" Proceedings of the 1st workshop on deep learning for recommender systems. ACM, 2016.\n",
    "9. Langford, John, Lihong Li, and Alex Strehl. \"Vowpal wabbit online learning project.\" (2007)."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python (reco_base)",
   "language": "python",
   "name": "reco_base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
