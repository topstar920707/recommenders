{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factorization Machine Deep Dive\n",
    "\n",
    "Factorization machine is one of the representative algorithms that are used for building content-based recommender model. The algorithm is powerful in terms of capturing the high-order interactions between input features. In addition, it provides better generalization capability and expressiveness compared to the other classic algorithms such as SVM. The most recent research extends the basic FM algorithms by using deep learning techniques, which achieve remarkable improvement in a few practical use cases.\n",
    "\n",
    "This notebook presents a deep dive into the Factorization Machine algorithm, as well as its variants, and demonstrates some best practices of using the contemporary FM implementations like [`xlearn`](https://github.com/aksnzhy/xlearn) and [`xDeepFM`](https://github.com/microsoft/recommenders) for dealing with tasks like click-through rate prediction by using the Criteo dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Factorization Machine And Its Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Factorization Machine Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 xlearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setups for using `xlearn`.\n",
    "\n",
    "1. `xlearn` is implemented in C++ and has Python bindings, so it can be directly installed as a Python package from PyPI. The installation of `xlearn` is enabled in the [Recommenders repo environment setup script](https://github.com/microsoft/recommenders/blob/master/scripts/generate_conda_file.py). One can follow the general setup steps to install the environment as required, in which `xlearn` is installed as well.\n",
    "2. NOTE `xlearn` may require some base libraries installed as prerequisites in the system, e.g., `cmake`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a succesful creation of the environment, one can load the packages to run `xlearn` in a Jupyter notebook or Python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34) \n",
      "[GCC 7.3.0]\n",
      "Tensorflow version: 1.12.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import os\n",
    "import papermill as pm\n",
    "from tempfile import TemporaryDirectory\n",
    "import xlearn as xl\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "from reco_utils.common.constants import SEED\n",
    "from reco_utils.recommender.deeprec.deeprec_utils import (\n",
    "    download_deeprec_resources, prepare_hparams\n",
    ")\n",
    "from reco_utils.recommender.deeprec.models.xDeepFM import XDeepFMModel\n",
    "from reco_utils.recommender.deeprec.IO.iterator import FFMTextIterator\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Tensorflow version: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the use of `xlearn`, the following example uses a synthetized data set from Bing News for building and evaluating a FFM model built by using `xlearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10.3k/10.3k [00:05<00:00, 2.06kKB/s]\n"
     ]
    }
   ],
   "source": [
    "tmpdir = TemporaryDirectory()\n",
    "\n",
    "data_path = tmpdir.name\n",
    "yaml_file = os.path.join(data_path, r'xDeepFM.yaml')\n",
    "train_file = os.path.join(data_path, r'synthetic_part_0')\n",
    "valid_file = os.path.join(data_path, r'synthetic_part_1')\n",
    "test_file = os.path.join(data_path, r'synthetic_part_2')\n",
    "model_file = os.path.join(data_path, r'model.out')\n",
    "output_file = os.path.join(data_path, r'output.txt')\n",
    "\n",
    "if not os.path.exists(yaml_file):\n",
    "    download_deeprec_resources(r'https://recodatasets.blob.core.windows.net/deeprec/', data_path, 'xdeepfmresources.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following steps are from the [official documentation of `xlearn`](https://xlearn-doc.readthedocs.io/en/latest/index.html) for building a model. To begin with, we do not modify any training parameter values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE, if `xlearn` is run through command line, the training process can be displayed in the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training task\n",
    "ffm_model = xl.create_ffm()        # Use field-aware factorization machine (ffm)\n",
    "ffm_model.setTrain(train_file)     # Set the path of training dataset\n",
    "ffm_model.setValidate(valid_file)  # Set the path of validation dataset\n",
    "\n",
    "# Parameters:\n",
    "#  0. task: binary classification\n",
    "#  1. learning rate: 0.2\n",
    "#  2. regular lambda: 0.002\n",
    "#  3. evaluation metric: accuracy\n",
    "param = {'task':'binary', 'lr':0.2, 'lambda':0.002, 'metric':'auc'}\n",
    "\n",
    "# Start to train\n",
    "# The trained model will be stored in model.out\n",
    "\n",
    "time_start = time.time()\n",
    "\n",
    "ffm_model.fit(param, model_file)\n",
    "\n",
    "time_train = time.time() - time_start\n",
    "\n",
    "# Prediction task\n",
    "ffm_model.setTest(test_file)  # Set the path of test dataset\n",
    "ffm_model.setSigmoid()        # Convert output to 0-1\n",
    "\n",
    "# Start to predict\n",
    "# The output result will be stored in output.txt\n",
    "time_start = time.time()\n",
    "\n",
    "ffm_model.predict(model_file, output_file)\n",
    "\n",
    "time_predict = time.time() - time_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output are the predicted labels (i.e., 1 or 0) for the testing data set. AUC score is calculated to evaluate the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file) as f:\n",
    "    predictions = f.readlines()\n",
    "\n",
    "with open(test_file) as f:\n",
    "    truths = f.readlines()\n",
    "\n",
    "truths = np.array([float(truth.split(' ')[0]) for truth in truths])\n",
    "predictions = np.array([float(prediction.strip('')) for prediction in predictions])\n",
    "\n",
    "auc_score = roc_auc_score(truths, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8680859736493857"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training takes 0.40s and predicting takes 0.03s.\n"
     ]
    }
   ],
   "source": [
    "print('Training takes {0:.2f}s and predicting takes {1:.2f}s.'.format(time_train, time_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the model building/scoring process is fast and the model performance is good. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 xDeepFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS_FOR_SYNTHETIC_RUN = 15\n",
    "BATCH_SIZE_SYNTHETIC = 128\n",
    "RANDOM_SEED = SEED  # Set None for non-deterministic result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('DNN_FIELD_NUM', None), ('FEATURE_COUNT', 1000), ('FIELD_COUNT', 10), ('MODEL_DIR', None), ('PAIR_NUM', None), ('SUMMARIES_DIR', None), ('activation', ['relu', 'relu']), ('attention_activation', None), ('attention_dropout', 0.0), ('attention_layer_sizes', None), ('batch_size', 128), ('cross_activation', 'identity'), ('cross_l1', 0.0), ('cross_l2', 0.0001), ('cross_layer_sizes', [1]), ('cross_layers', None), ('data_format', 'ffm'), ('dim', 10), ('doc_size', None), ('dropout', [0.0, 0.0]), ('dtype', 32), ('embed_l1', 0.0), ('embed_l2', 0.0001), ('enable_BN', False), ('entityEmb_file', None), ('entity_dim', None), ('entity_embedding_method', None), ('entity_size', None), ('epochs', 15), ('fast_CIN_d', 0), ('filter_sizes', None), ('init_method', 'tnormal'), ('init_value', 0.3), ('is_clip_norm', 0), ('iterator_type', None), ('kg_file', None), ('kg_training_interval', 5), ('layer_l1', 0.0), ('layer_l2', 0.0001), ('layer_sizes', [100, 100]), ('learning_rate', 0.001), ('load_model_name', 'you model path'), ('load_saved_model', False), ('loss', 'log_loss'), ('lr_kg', 0.5), ('lr_rs', 1), ('max_grad_norm', 2), ('method', 'classification'), ('metrics', ['auc', 'logloss']), ('model_type', 'xDeepFM'), ('mu', None), ('n_item', None), ('n_item_attr', None), ('n_user', None), ('n_user_attr', None), ('num_filters', None), ('optimizer', 'adam'), ('reg_kg', 0.0), ('save_epoch', 2), ('save_model', False), ('show_step', 200000), ('train_ratio', None), ('transform', None), ('use_CIN_part', False), ('use_DNN_part', False), ('use_FM_part', True), ('use_Linear_part', False), ('user_clicks', None), ('user_dropout', False), ('wordEmb_file', None), ('word_size', None), ('write_tfevents', False)]\n"
     ]
    }
   ],
   "source": [
    "hparams = prepare_hparams(\n",
    "    yaml_file,     \n",
    "    FEATURE_COUNT=1000, \n",
    "    FIELD_COUNT=10, \n",
    "    cross_l2=0.0001, \n",
    "    embed_l2=0.0001, \n",
    "    learning_rate=0.001, \n",
    "    epochs=EPOCHS_FOR_SYNTHETIC_RUN,\n",
    "    use_FM_part=True,\n",
    "    use_CIN_part=False,\n",
    "    use_DNN_part=False,\n",
    "    batch_size=BATCH_SIZE_SYNTHETIC\n",
    ")\n",
    "\n",
    "print(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add FM part.\n",
      "WARNING:tensorflow:From ../../reco_utils/recommender/deeprec/models/xDeepFM.py:140: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "input_creator = FFMTextIterator\n",
    "model = XDeepFMModel(hparams, input_creator, seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 1 train info: auc:0.5308, logloss:0.809 eval info: auc:0.5019, logloss:0.8444\n",
      "at epoch 1 , train time: 2.8 eval time: 3.5\n",
      "at epoch 2 train info: auc:0.5579, logloss:0.7493 eval info: auc:0.5043, logloss:0.8024\n",
      "at epoch 2 , train time: 2.6 eval time: 3.3\n",
      "at epoch 3 train info: auc:0.5863, logloss:0.7104 eval info: auc:0.5067, logloss:0.7764\n",
      "at epoch 3 , train time: 2.6 eval time: 3.3\n",
      "at epoch 4 train info: auc:0.6151, logloss:0.6832 eval info: auc:0.5095, logloss:0.7595\n",
      "at epoch 4 , train time: 2.6 eval time: 3.4\n",
      "at epoch 5 train info: auc:0.6434, logloss:0.6627 eval info: auc:0.5132, logloss:0.7481\n",
      "at epoch 5 , train time: 2.6 eval time: 3.2\n",
      "at epoch 6 train info: auc:0.6705, logloss:0.6461 eval info: auc:0.5178, logloss:0.7402\n",
      "at epoch 6 , train time: 2.6 eval time: 3.3\n",
      "at epoch 7 train info: auc:0.6959, logloss:0.6317 eval info: auc:0.5235, logloss:0.7343\n",
      "at epoch 7 , train time: 2.6 eval time: 3.3\n",
      "at epoch 8 train info: auc:0.7193, logloss:0.6183 eval info: auc:0.5305, logloss:0.7298\n",
      "at epoch 8 , train time: 2.6 eval time: 3.4\n",
      "at epoch 9 train info: auc:0.7407, logloss:0.6052 eval info: auc:0.5389, logloss:0.726\n",
      "at epoch 9 , train time: 2.6 eval time: 3.2\n",
      "at epoch 10 train info: auc:0.7603, logloss:0.5922 eval info: auc:0.5484, logloss:0.7226\n",
      "at epoch 10 , train time: 2.6 eval time: 3.2\n",
      "at epoch 11 train info: auc:0.7781, logloss:0.579 eval info: auc:0.5591, logloss:0.7191\n",
      "at epoch 11 , train time: 2.6 eval time: 3.4\n",
      "at epoch 12 train info: auc:0.7943, logloss:0.5655 eval info: auc:0.5708, logloss:0.7154\n",
      "at epoch 12 , train time: 2.4 eval time: 3.5\n",
      "at epoch 13 train info: auc:0.8092, logloss:0.5518 eval info: auc:0.5834, logloss:0.7113\n",
      "at epoch 13 , train time: 2.6 eval time: 3.3\n",
      "at epoch 14 train info: auc:0.823, logloss:0.5378 eval info: auc:0.5968, logloss:0.7065\n",
      "at epoch 14 , train time: 2.6 eval time: 3.3\n",
      "at epoch 15 train info: auc:0.8356, logloss:0.5236 eval info: auc:0.6106, logloss:0.7011\n",
      "at epoch 15 , train time: 2.6 eval time: 3.4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<reco_utils.recommender.deeprec.models.xDeepFM.XDeepFMModel at 0x7fad5c18a8d0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_file, valid_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'auc': 0.6202, 'logloss': 0.6925}\n"
     ]
    },
    {
     "data": {
      "application/papermill.record+json": {
       "res_syn": {
        "auc": 0.6202,
        "logloss": 0.6925
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res_syn = model.run_eval(test_file)\n",
    "print(res_syn)\n",
    "\n",
    "pm.record(\"res_syn\", res_syn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 FM on criteo data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "reco_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
