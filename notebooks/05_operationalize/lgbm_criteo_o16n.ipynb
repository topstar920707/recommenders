{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "This notebook creates a real-time scoring service for the content-personalization model created in the prior [notebook](../02_model/mmlspark_lightgbm_criteo.ipynb). It is assumed that this notebook is run in an Azure Databricks environment that has had `mmlspark` installed and has been prepared for operationalization. See [Setup instructions](https://github.com/Microsoft/Recommenders/blob/master/SETUP.md) for details.\n",
    "\n",
    "**NOTE**: Please Register Azure Container Instance (ACI) using Azure Portal: https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-manager-supported-services#portal in your subscription before using the SDK to deploy your ML model to ACI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup libraries and variables\n",
    "\n",
    "The next few cells initialize the environment and varibles: we import relevant libraries and set variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import json\n",
    "\n",
    "from azureml.core import Workspace\n",
    "from azureml.core import VERSION as amlversion\n",
    "\n",
    "from azureml.core.model import Model\n",
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "from azureml.core.webservice import Webservice, AciWebservice\n",
    "from azureml.core.image import ContainerImage\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", amlversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These variables are used to construct names of assets:\n",
    "short_uuid = str(uuid.uuid4())[:4]\n",
    "prefix = \"reco\" + short_uuid\n",
    "data = \"criteo\"\n",
    "algo = \"lgbm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure subscription\n",
    "subscription_id = ''\n",
    "\n",
    "# Resource group and workspace\n",
    "resource_group = prefix + \"_\" + data\n",
    "workspace_name = prefix + \"_\"+data+\"_aml\"\n",
    "workspace_region = \"westus2\"\n",
    "print(\"Resource group:\", resource_group)\n",
    "\n",
    "# AzureML\n",
    "#NOTE: The name of a asset must be only letters or numerals, not contain spaces, and under 30 characters\n",
    "model_name = data+\"-\"+algo+\".model\" \n",
    "service_name = data + \"-\" + algo\n",
    "\n",
    "# add a name for the container\n",
    "container_image_name = '-'.join([data, algo])\n",
    "\n",
    "\n",
    "## locations for serializing so it persists. This is a local API URL\n",
    "ws_config_path = '/dbfs/FileStore'\n",
    "## location of model on **dbfs**:\n",
    "model_path = os.path.join('dbfs:/FileStore/dac',model_name)\n",
    "## path to the notebook for modeling. Assumes the entire repository has been imported:\n",
    "modeling_notebook = '../02_model/mmlspark_lightgbm_criteo'\n",
    "\n",
    "## names of other files that are used below\n",
    "my_conda_file = \"deploy_conda.yml\"\n",
    "driver_file = \"score_sparkml.py\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Assets for the Scoring Service\n",
    "\n",
    "Before walking through the steps taken to create a model, it is useful to set some context. In our example, a \"scoring service\" is a function that is executed by a docker container. It takes in some number of records and produces a set of scores for each record (usually predictions of some type) based on a previously estimated model. In our case, we will take the model we estimated earlier that predicts the probability of a click based on some set of numeric and categorical features. In order to create a scoring service, we will do several steps.\n",
    "\n",
    "We will:\n",
    "\n",
    "1. Create an Azure Machine Learning Workspace to simplify all the subsequent steps.\n",
    "2. Make sure we have access to the previously estimated model. If we are working on a spark system, that means we will make sure the model is on the local filesystem (**not** DBFS) and registered with the Azure Machine Learning Service.\n",
    "3. Define a 'driver' script that defines what the system needs to do in order to generate our predictions. This script needs to have an `init` method that does one-time initialization and a `run` method that is executed each time the service is called.\n",
    "4. Define all the pre-requisites that that script requries.\n",
    "5. Use the model, the driver script, and the pre-requisites to create a docker image.\n",
    "6. We will run the docker image on a platform (in our case Azure Container Instance or ACI).\n",
    "7. We will test our service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.create(name = workspace_name,\n",
    "                      subscription_id = subscription_id,\n",
    "                      resource_group = resource_group, \n",
    "                      location = workspace_region,\n",
    "                      exist_ok=True)\n",
    "\n",
    "# persist the subscription id, resource group name, and workspace name in aml_config/config.json.\n",
    "ws.write_config(ws_config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare the Serialized Model\n",
    "\n",
    "First, we will prepare the serialized model. We will make sure the model exists, and if it doesn't, then we will run the notebook to generate the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## if it doesn't exist, run the relevant notebook:\n",
    "if not os.path.exists(model_path.replace('dbfs:','/dbfs')):\n",
    "  print('Model pipeline does not exist. Creating by running {}'.format(modeling_notebook))\n",
    "  dbutils.notebook.run(modeling_notebook, timeout_seconds=600)\n",
    "else:\n",
    "  print('Operationalizing model found at: {}'.format(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy from dbfs to local\n",
    "\n",
    "While you can access files on DBFS with local file APIs, it is better practice to explicitly copy saved models to and from dbfs, because the local file APIs can only access files smaller than 2 GB (see details [here](https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html#access-dbfs-using-local-file-apis)).  \n",
    "\n",
    "Model deployment will always get the model from the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_local = \"file:\" + os.getcwd() + \"/\" + model_name\n",
    "dbutils.fs.cp(model_path, model_local, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register the Model\n",
    "\n",
    "Next, we need to register the model in the Azure Machine Learning Workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Register the model\n",
    "mymodel = Model.register(model_path = model_name, # this points to a local file\n",
    "                       model_name = model_name, # this is the name the model is registered as\n",
    "                       description = \"LightGBM Criteo Model\",\n",
    "                       workspace = ws)\n",
    "\n",
    "print(mymodel.name, mymodel.description, mymodel.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create the Driver Script\n",
    "\n",
    "Next we, need to create the driver script that will be executed when the service is called. The functions that need to be defined for scoring are `init()` and `run()`. The `init()` function is run when the service is created, and the `run()` function is run each time the service is called.\n",
    "\n",
    "In our example, we use the `init()` function to load all the libraries, initialize the spark session, and load the model and pipeline. We use the `run()` method to parse the input json file, generate predictions (in this case the probability of a click), and format for output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_sparkml = \"\"\"\n",
    "\n",
    "import json\n",
    " \n",
    "def init():\n",
    "    # One-time initialization of PySpark and predictive model\n",
    "    import pyspark\n",
    "    from pyspark.ml import PipelineModel\n",
    "    from mmlspark import LightGBMClassifier\n",
    "    from azureml.core.model import Model\n",
    "    from pyspark.ml import PipelineModel\n",
    "    from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "    global trainedModel\n",
    "    global spark\n",
    "    global schema\n",
    "\n",
    "    spark = pyspark.sql.SparkSession.builder.appName(\"LightGBM Criteo Predictions\").getOrCreate()\n",
    "    model_name = \"{model_name}\" \n",
    "    model_path = Model.get_model_path(model_name)\n",
    "    trainedModel = PipelineModel.load(model_path)\n",
    "    \n",
    "def run(input_json):\n",
    "    if isinstance(trainedModel, Exception):\n",
    "        return json.dumps({{\"trainedModel\":str(trainedModel)}})\n",
    "      \n",
    "    try:\n",
    "        sc = spark.sparkContext\n",
    "        input_list = json.loads(input_json)\n",
    "        input_rdd = sc.parallelize(input_list)\n",
    "        input_df = spark.read.json(input_rdd)\n",
    "        \n",
    "        # Compute prediction\n",
    "        predictions = trainedModel.transform(input_df).collect()\n",
    "        #Get probability of a click for each row and conver to a str\n",
    "        click_prob = [str(x.probability[1]) for x in predictions]\n",
    "\n",
    "        # you can return any data type as long as it is JSON-serializable\n",
    "        result = \",\".join(click_prob)\n",
    "        return [result]\n",
    "    except Exception as e:\n",
    "        result = str(e)\n",
    "        return result\n",
    "\"\"\".format(model_name=model_name)\n",
    " \n",
    "exec(score_sparkml)\n",
    " \n",
    "with open(driver_file, \"w\") as file:\n",
    "    file.write(score_sparkml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Dependencies\n",
    "\n",
    "Next, we define the dependencies that are required by driver script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## azureml-sdk is required to load the registered model\n",
    "myconda = CondaDependencies.create(pip_packages=['azureml-sdk'])\n",
    "with open(my_conda_file,\"w\") as f:\n",
    "    f.write(myconda.serialize_to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create the Image\n",
    "\n",
    "We use the `ContainerImage` class to first configure, then to create the docker image used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myimage_config = ContainerImage.image_configuration(execution_script = driver_file, \n",
    "                                                    runtime = \"spark-py\",\n",
    "                                                    conda_file=my_conda_file,\n",
    "                                                    tags={\"runtime\":\"pyspark\", \"algorithm\":\"lightgbm\"})\n",
    "\n",
    "image = ContainerImage.create(name = service_name,\n",
    "                              models = [mymodel],\n",
    "                              image_config = myimage_config,\n",
    "                              workspace = ws)\n",
    "\n",
    "image.wait_for_creation(show_output = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create the Service\n",
    "\n",
    "Once we have created an image, we configure and run it on ACI.\n",
    "\n",
    "**NOTE** You *can* create a service directly from the registered model and image_configuration with the `Webservice.deploy_from_model()` function. We create the image here explicitly and use `deploy_from_image()` for two reasons:\n",
    "\n",
    "1. It provides more transparency in terms of the actual steps that are taking place\n",
    "2. It has potential for faster iteration and for more portability. Once we have an image, we can create a new deployment with the exact same code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configure ACI\n",
    "myaci_config = AciWebservice.deploy_configuration(\n",
    "    cpu_cores = 2, \n",
    "    memory_gb = 2, \n",
    "    tags = {'name':'Azure ML ACI for LightGBM', 'algorithm':'LightGBM'}, \n",
    "    description = 'Light GBM ACI.')\n",
    "\n",
    "# Webservice creation\n",
    "myservice = Webservice.deploy_from_image(\n",
    "  workspace=ws, \n",
    "  name=service_name,\n",
    "  image=image,\n",
    "  deployment_config = myaci_config\n",
    "    )\n",
    "\n",
    "myservice.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for using the Web HTTP API \n",
    "print(myservice.scoring_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test the Service\n",
    "\n",
    "Next, we can use data from the `test` data to test the service.\n",
    "\n",
    "The service expects JSON as its payload, so we take the test data, fill missing values, convert to JSON, then submit to the service endpoint.\n",
    "\n",
    "We have to fill in missing values here to create the data, because the webservice expects that the data coming into the webservice is well-formed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_to_test = 10\n",
    "\n",
    "## load the table created in the other notebook:\n",
    "test=spark.table('test')\n",
    "test_for_service_df = test.drop('features').fillna('M').fillna(0).limit(n_samples_to_test)\n",
    "display(test_for_service_df)\n",
    "test_json = json.dumps(test_for_service_df.toJSON().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Service and Parse the Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The prediction is the predicted probability of a click for that particular record\n",
    "service_out = myservice.run(input_data=test_json)\n",
    "print(service_out)\n",
    "values=json.loads('['+service_out[0]+']')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the Service\n",
    "\n",
    "When you are done, you can delete the service to minimize costs. You can always redeploy from the image using the same command above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment the following line to delete the web service\n",
    "# myservice.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "- See the notebook for model estimation [here](https://github.com/Microsoft/Recommenders/blob/gramhagen/lgbm_scenario/notebooks/02_model/mmlspark_lightgbm_criteo.ipynb).\n",
    "- This notebook is adapted from the notebooks [here](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/azure-databricks/amlsdk/).\n",
    "- See an example of leveraging the image on AKS [here](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/azure-databricks/amlsdk/deploy-to-aks-existingimage-05.ipynb).\n"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "pasha"
   }
  ],
  "kernelspec": {
   "display_name": "Python (reco_base)",
   "language": "python",
   "name": "reco_base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "name": "deploy-to-aci-04",
  "notebookId": 2571086681627427
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
