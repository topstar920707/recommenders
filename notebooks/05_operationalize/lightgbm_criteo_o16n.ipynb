{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying a Real-Time Content Based Personalization Model\n",
    "\n",
    "This notebook provides and example for how a business can use machine learning to automate content based personalization for their customers by using a recommendation system. Azure Databricks is used to train a model that predicts the probability a user will engage with an item. In turn, this estimate can be used to rank items based on the content that a user is most likely to consume.<br><br>\n",
    "This notebook creates a scalable real-time scoring service for the Spark based models such as the Content Based Personalization model trained in the [MMLSpark-LightGBM-Criteo notebook](../02_model/mmlspark_lightgbm_criteo.ipynb).\n",
    "<br><br>\n",
    "### Architecture\n",
    "![architecture](https://recodatasets.blob.core.windows.net/images/lgbm_criteo_arch.svg \"Architecture\")\n",
    "<br>\n",
    "### Components\n",
    "The following components are used in this architecture:<br>\n",
    "- [Azure Blob Storage](https://azure.microsoft.com/en-us/services/storage/blobs/) is a storage service optimized for storing massive amounts of unstructured data. In this case, the input data is stored here.<br>\n",
    "- [Azure Databricks](https://azure.microsoft.com/en-us/services/databricks/) is a managed Apache Spark cluster where model training and evaluating is performed.<br>\n",
    "- [Azure Machine Learning service](https://azure.microsoft.com/en-us/services/machine-learning-service/) is used in this scenario to register the machine learning model. <br>\n",
    "- [Azure Container Registry](https://azure.microsoft.com/en-us/services/container-registry/) is used to package the scoring script as a container image which is used to serve the model in production. <br>\n",
    "- [Azure Kubernetes Service](https://azure.microsoft.com/en-us/services/kubernetes-service/) is used to deploy the trained models to web or app services. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions\n",
    "In order to execute this notebook the following items are assumed:\n",
    "\n",
    "1. A model has previously been trained as shown in the [mmlspark_lightgbm_criteo](../02_model/mmlspark_lightgbm_criteo.ipynb) notebook\n",
    "2. This notebook is running in the same Azure Databricks workspace used to train and save the model\n",
    "3. The Databricks cluster used has been prepared for operationalization (MML Spark and reco_utils are both installed)\n",
    "  - See [Setup](https://github.com/Microsoft/Recommenders/blob/master/SETUP.md) instructions for details\n",
    "4. An Azure Machine Learning Service workspace has been setup in the same region as the Azure Databricks workspace used for model training\n",
    "  - See [Create A Workspace](https://docs.microsoft.com/en-us/azure/machine-learning/service/setup-create-workspace) for more details\n",
    "5. The Azure ML Workspace config.json has been uploaded to databrics at dbfs:/aml_config/config.json\n",
    "  - See [Configure Environment](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-configure-environment) and [Databricks CLI](https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html#access-dbfs-with-the-databricks-cli)\n",
    "6. An Azure Container Instance (ACI) has been registered for use your Azure subscription\n",
    "  - See [Supported Services](https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-manager-supported-services#portal) for more details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Service Steps\n",
    "In this example, a \"scoring service\" is a function that is executed by a docker container. It takes in a post request with JSON formatted payload and produces a score based on a previously estimated model. In our case, we will use the model we estimated earlier that predicts the probability of an user-item interaction based on a set of numeric and categorical features. Because that model was trained using PySpark we will create a Spark session on a single instance (within the docker container) which will use MML Spark Serving to execute the model on the received input data and return the probability of interaction.<br><br>\n",
    "\n",
    "In order to create a scoring service, we will do the following steps:\n",
    "\n",
    "1. Setup and authorize the Azure Machine Learning Workspace\n",
    "2. Serialize the previously trained model and add it to the Azure Model Registry\n",
    "3. Define the 'scoring service' script to execute the model\n",
    "4. Define all the pre-requisites that that script requires\n",
    "5. Use the model, the driver script, and the pre-requisites to create a Azure Container Image\n",
    "6. Deploy the container image on a scalable platform Azure Kubernetes Service\n",
    "7. Test the service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup libraries and variables\n",
    "\n",
    "The next few cells initialize the environment and varibles: we import relevant libraries and set variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Azure ML SDK version: 1.0.21\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "from reco_utils.dataset.criteo import get_spark_schema, load_spark_df\n",
    "\n",
    "from azureml.core import Workspace\n",
    "from azureml.core import VERSION as azureml_version\n",
    "\n",
    "from azureml.core.model import Model\n",
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "from azureml.core.webservice import Webservice, AksWebservice\n",
    "from azureml.core.image import ContainerImage\n",
    "from azureml.core.compute import AksCompute, ComputeTarget\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"Azure ML SDK version: {}\".format(azureml_version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Scoring Service Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_NAME = 'lightgbm_criteo.mml'  # this name must exactly match the name used to save the pipeline model in the estimation notebook\n",
    "MODEL_DESCRIPTION = 'LightGBM Criteo Model'\n",
    "\n",
    "# Setup AzureML assets (names must be lower case alphanumeric without spaces and between 3 and 32 characters)\n",
    "# Azure ML Webservice\n",
    "SERVICE_NAME = 'lightgbm-criteo'\n",
    "# Azure ML Container Image\n",
    "CONTAINER_NAME = 'lightgbm-criteo'\n",
    "CONTAINER_RUN_TIME = 'spark-PY'\n",
    "# Azure AKS Service\n",
    "AKS_NAME = 'predict-aks'\n",
    "\n",
    "# Names of other files that are used below\n",
    "CONDA_FILE = \"deploy_conda.yaml\"\n",
    "DRIVER_FILE = \"mmlspark_serving.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup AzureML Workspace\n",
    "Workspace configuration can be retrieved from the portal and uploaded to Databricks<br>\n",
    "See [AzureML on Databricks](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-configure-environment#azure-databricks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config('/dbfs/aml_config/config.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Serialized Model\n",
    "Spark Serving needs the schema of the raw input data so an additional file is added to the model directory.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### MAKE THIS METHOD PUBLIC WITH DEFAULT HEADER ###\n",
    "raw_schema = get_spark_schema()\n",
    "with open(os.path.join('/dbfs', MODEL_NAME, 'schema.json'), 'w') as f:\n",
    "  f.write(raw_schema.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy from dbfs to local\n",
    "\n",
    "While you can access files on DBFS with local file APIs, it is safer to explicitly copy saved models to and from dbfs, because the local file APIs can only access files smaller than 2 GB (see details [here](https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html#access-dbfs-using-local-file-apis))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">6</span><span class=\"ansired\">]: </span>True\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_local = os.path.join(os.getcwd(), MODEL_NAME)\n",
    "dbutils.fs.cp('dbfs:/' + MODEL_NAME, 'file:' + model_local, recurse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register the Model\n",
    "\n",
    "Next, we need to register the model in the Azure Machine Learning Workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Registering model lightgbm_criteo.mml\n",
       "lightgbm_criteo.mml LightGBM Criteo Model 4\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# First the model directory is compressed to minimize data transfer\n",
    "zip_file = shutil.make_archive(base_name=MODEL_NAME, format='zip', root_dir=model_local)\n",
    "\n",
    "# Register the model\n",
    "model = Model.register(model_path=zip_file,  # this points to a local file\n",
    "                       model_name=MODEL_NAME,  # this is the name the model is registered as\n",
    "                       description=MODEL_DESCRIPTION,\n",
    "                       workspace=ws)\n",
    "\n",
    "print(model.name, model.description, model.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Scoring Script\n",
    "\n",
    "Next we, need to create the driver script that will be executed when the service is called. The functions that need to be defined for scoring are `init()` and `run()`. The `init()` function is run when the service is created, and the `run()` function is run each time the service is called.\n",
    "\n",
    "In our example, we use the `init()` function to load all the libraries, initialize the spark session, start the spark streaming service and load the model pipeline. We use the `run()` method to route the input to the spark streaming service to generate predictions (in this case the probability of an interaction) then return the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "driver_file = '''\n",
    "import os\n",
    "import json\n",
    "from time import sleep\n",
    "from uuid import uuid4\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from azureml.core.model import Model\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType\n",
    "import requests\n",
    "\n",
    "\n",
    "def init():\n",
    "    \"\"\"One time initialization of pyspark and model server\"\"\"\n",
    "\n",
    "    spark = SparkSession.builder.appName(\"Model Server\").getOrCreate()\n",
    "    import mmlspark  # this is needed to load mmlspark libraries\n",
    "\n",
    "    # extract and load model\n",
    "    model_path = Model.get_model_path('{model_name}')\n",
    "    with ZipFile(model_path, 'r') as f:\n",
    "        f.extractall('model')\n",
    "    model = PipelineModel.load('model')\n",
    "\n",
    "    # load data schema saved with model\n",
    "    with open(os.path.join('model', 'schema.json'), 'r') as f:\n",
    "        schema = StructType.fromJson(json.load(f))\n",
    "\n",
    "    input_df = (\n",
    "        spark.readStream.continuousServer()\n",
    "        .address(\"localhost\", 8089, \"predict\")\n",
    "        .load()\n",
    "        .parseRequest(schema)\n",
    "    )\n",
    "\n",
    "    output_df = (\n",
    "        model.transform(input_df)\n",
    "        .makeReply(\"probability\")\n",
    "    )\n",
    "\n",
    "    checkpoint = os.path.join('/tmp', 'checkpoints', uuid4().hex)\n",
    "    server = (\n",
    "        output_df.writeStream.continuousServer()\n",
    "        .trigger(continuous=\"1 second\")\n",
    "        .replyTo(\"predict\")\n",
    "        .queryName(\"prediction\")\n",
    "        .option(\"checkpointLocation\", checkpoint)\n",
    "        .start()\n",
    "    )\n",
    "\n",
    "    # let the server finish starting\n",
    "    sleep(1)\n",
    "\n",
    "\n",
    "def run(input_json):\n",
    "    try:\n",
    "        response = requests.post(data=input_json, url='http://localhost:8089/predict')\n",
    "        result = response.json()['probability']['values'][1]\n",
    "    except Exception as e:\n",
    "        result = str(e)\n",
    "    \n",
    "    return json.dumps({{\"result\": result}})\n",
    "    \n",
    "'''.format(model_name=MODEL_NAME)\n",
    "\n",
    "# check syntax\n",
    "exec(driver_file)\n",
    "\n",
    "with open(DRIVER_FILE, \"w\") as f:\n",
    "    f.write(driver_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dependencies\n",
    "\n",
    "Next, we define the dependencies that are required by driver script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# azureml-sdk is required to load the registered model\n",
    "conda_file = CondaDependencies.create(pip_packages=['azureml-sdk', 'requests']).serialize_to_string()\n",
    "\n",
    "with open(CONDA_FILE, \"w\") as f:\n",
    "    f.write(conda_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Image\n",
    "\n",
    "We use the `ContainerImage` class to first configure the image with the defined driver and dependencies, then to create the image for use later.<br>\n",
    "Building the image allows it to be downloaded and debugged locally using docker, see [troubleshooting instructions](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-troubleshoot-deployment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Creating image\n",
       "Running......................\n",
       "SucceededImage creation operation finished for image lightgbm-criteo:3, operation &quot;Succeeded&quot;\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_config = ContainerImage.image_configuration(execution_script=DRIVER_FILE, \n",
    "                                                  runtime=CONTAINER_RUN_TIME,\n",
    "                                                  conda_file=CONDA_FILE,\n",
    "                                                  tags={\"runtime\":CONTAINER_RUN_TIME, \"model\": MODEL_NAME})\n",
    "\n",
    "image = ContainerImage.create(name=SERVICE_NAME,\n",
    "                              models=[model],\n",
    "                              image_config=image_config,\n",
    "                              workspace=ws)\n",
    "\n",
    "image.wait_for_creation(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Service\n",
    "\n",
    "Once we have created an image, we configure an AKS cluster and deploy the image as a AKS Webservice.\n",
    "\n",
    "**NOTE** You *can* create a service directly from the registered model and image_configuration with the `Webservice.deploy_from_model()` function. We create the image here explicitly and use `deploy_from_image()` for two reasons:\n",
    "\n",
    "1. It provides more transparency in terms of the actual steps that are taking place\n",
    "2. It has potential for faster iteration and for more portability. Once we have an image, we can create a new deployment with the exact same code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create AKS compute first\n",
    "\n",
    "# Use the default configuration (can also provide parameters to customize)\n",
    "prov_config = AksCompute.provisioning_configuration()\n",
    "\n",
    "# Create the cluster\n",
    "aks_target = ComputeTarget.create(\n",
    "  workspace=ws, \n",
    "  name=AKS_NAME, \n",
    "  provisioning_configuration=prov_config\n",
    ")\n",
    "\n",
    "aks_target.wait_for_completion(show_output=True)\n",
    "\n",
    "print(aks_target.provisioning_state)\n",
    "print(aks_target.provisioning_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the web service configuration (using default here)\n",
    "aks_config = AksWebservice.deploy_configuration()\n",
    "\n",
    "# Deploy service using created image\n",
    "aks_service = Webservice.deploy_from_image(\n",
    "  workspace=ws, \n",
    "  name=SERVICE_NAME,\n",
    "  deployment_config=aks_config,\n",
    "  image=image,\n",
    "  deployment_target=aks_target\n",
    ")\n",
    "\n",
    "aks_service.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Service\n",
    "\n",
    "Next, we can use data from the `test` data to test the service.\n",
    "\n",
    "The service expects JSON as its payload, so we take the test data, fill missing values, convert to JSON, then submit to the service endpoint.\n",
    "\n",
    "We have to fill in missing values here to create the data, because the webservice expects that the data coming into the webservice is well-formed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the URI\n",
    "url = aks_service.scoring_uri\n",
    "print('AKS URI: {}'.format(url))\n",
    "\n",
    "# Setup authentication using one of the keys from aks_service\n",
    "headers = dict(Authorization='Bearer {}'.format(aks_service.get_keys()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab some sample data\n",
    "df = load_spark_df(size='sample', spark=spark, dbutils=dbutils)\n",
    "data = df.head().asDict()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send a request to the AKS cluster\n",
    "response = requests.post(url=url, json=data, headers=headers)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the Service\n",
    "\n",
    "When you are done, you can delete the service to minimize costs. You can always redeploy from the image using the same command above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Uncomment the following line to delete the web service\n",
    "# aks_service.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">34</span><span class=\"ansired\">]: </span>&apos;Deleting&apos;\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "aks_service.state"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "pasha"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "name": "deploy-to-aci-04",
  "notebookId": 904892461294324
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
