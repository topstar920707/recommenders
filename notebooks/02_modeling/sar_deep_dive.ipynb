{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAR Deep Dive with Spark and SQL\n",
    "\n",
    "In this example, we will walkthrough each step of the SAR algorithm with an implementation using Spark and SQL.\n",
    "\n",
    "Smart Adaptive Recommendations (SAR) is a fast, scalable, adaptive algorithm for personalized recommendations based on user transaction history and item descriptions. It is powered by understanding the **similarity** between items, and recommending similar items to ones a user has an existing **affinity** for. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Global Variables and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)]\n",
      "Pandas version: 0.23.0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pyspark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-7a66d1d6208a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"System version: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Pandas version: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"PySpark version: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pyspark' is not defined"
     ]
    }
   ],
   "source": [
    "# specify parameters\n",
    "TOP_K=2\n",
    "RECOMMEND_SEEN=True\n",
    "# options are 'jaccard', 'lift' or '' to skip and use item cooccurrence directly\n",
    "SIMILARITY='jaccard'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import heapq\n",
    "import pyspark.sql.functions as F\n",
    "import sys\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, Row, ArrayType, IntegerType, FloatType\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Pandas version: {}\".format(pd.__version__))\n",
    "print(\"PySpark version: {}\".format(pyspark.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Load Data\n",
    "\n",
    "We'll work with a small dataset here containing customer IDs, item IDs, and the customer's rating for the item. SAR requires inputs to be of the following schema: `<User ID>, <Item ID>, <Time>, [<Event Type>], [<Event Weight>]` (we will not use time or event type in the example below, and `rating` will be used as the `Event Weight`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/reco/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# There are two versions of the dataframes - the numeric version and the alphanumeric one:\n",
    "# they both have similar test data for top-2 recommendations and illustrate the indexing approaches to matrix multiplication on SQL\n",
    "d_train = {\n",
    "'customerID': [1,1,1,2,2,3,3],\n",
    "'itemID':     [1,2,3,4,5,6,1],\n",
    "'rating':     [5,5,5,1,1,3,5]\n",
    "}\n",
    "pdf_train = pd.DataFrame(d_train)\n",
    "d_test = {\n",
    "'customerID': [1,1,2,2,3,3],\n",
    "'itemID':     [4,5,1,5,6,1],\n",
    "'rating':     [1,1,5,5,5,5]\n",
    "}\n",
    "pdf_test = pd.DataFrame(d_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5 5 5 0 0 0]\n",
      " [0 0 0 1 1 0]\n",
      " [5 0 0 0 0 3]]\n",
      "(3, 6)\n"
     ]
    }
   ],
   "source": [
    "a_train = np.array([[5,5,5,0,0,0],\\\n",
    "                    [0,0,0,1,1,0],\n",
    "                    [5,0,0,0,0,3]])\n",
    "print(a_train)\n",
    "print(a_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customerID  itemID  rating\n",
       "0           1       4       1\n",
       "1           1       5       1\n",
       "2           2       1       5\n",
       "3           2       5       5\n",
       "4           3       6       5\n",
       "5           3       1       5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_alnum_train = {\n",
    "'customerID': ['ua','ua','ua','ub','ub','uc','uc'],\n",
    "'itemID':     ['ia','ib','ic','id','ie','if','ia'],\n",
    "'rating':     [5,5,5,1,1,3,5]\n",
    "}\n",
    "#pdf_train = pd.DataFrame(d_alnum_train)\n",
    "pdf_train = pd.DataFrame(d_train)\n",
    "d_alnum_test = {\n",
    "'customerID': ['ua','ua','ub','ub','uc','uc'],\n",
    "'itemID':     ['id','ie','ia','ie','if','ia'],\n",
    "'rating':     [1,1,5,5,5,5]\n",
    "}\n",
    "#pdf_test = pd.DataFrame(d_alnum_test)\n",
    "pdf_test = pd.DataFrame(d_test)\n",
    "pdf_test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Spark context\n",
    "\n",
    "The following settings work well for debugging locally on VM - change when running on a cluster. We set up a giant single executor with many threads and specify memory cap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"SAR pySpark\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\")\\\n",
    "    .config(\"spark.executor.cores\", \"32\")\\\n",
    "    .config(\"spark.executor.memory\", \"8g\")\\\n",
    "    .config(\"spark.yarn.executor.memoryOverhead\", \"3g\")\\\n",
    "    .config(\"spark.memory.fraction\", \"0.9\")\\\n",
    "    .config(\"spark.memory.stageFraction\", \"0.3\")\\\n",
    "    .config(\"spark.executor.instances\", 1)\\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"36000s\")\\\n",
    "    .config(\"spark.network.timeout\", \"10000000s\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"50g\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+----+\n",
      "|customerID|itemID|rating|type|\n",
      "+----------+------+------+----+\n",
      "|         1|     1|     5|   1|\n",
      "|         1|     2|     5|   1|\n",
      "|         1|     3|     5|   1|\n",
      "|         2|     4|     1|   1|\n",
      "|         2|     5|     1|   1|\n",
      "|         3|     6|     3|   1|\n",
      "|         3|     1|     5|   1|\n",
      "+----------+------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(pdf_train).withColumn(\"type\", F.lit(1))\n",
    "df_test = spark.createDataFrame(pdf_test).withColumn(\"type\", F.lit(0))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Index the user and item IDs\n",
    "\n",
    "Map user and item alphanumeric IDs to matrix indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+------+------+----+\n",
      "|customerid|row_id|itemid|col_id|rating|type|\n",
      "+----------+------+------+------+------+----+\n",
      "|         1|     1|     1|     1|     5|   1|\n",
      "|         1|     1|     2|     2|     5|   1|\n",
      "|         1|     1|     3|     3|     5|   1|\n",
      "|         1|     1|     4|     4|     1|   0|\n",
      "|         1|     1|     5|     5|     1|   0|\n",
      "|         2|     2|     1|     1|     5|   0|\n",
      "|         2|     2|     4|     4|     1|   1|\n",
      "|         2|     2|     5|     5|     1|   1|\n",
      "|         2|     2|     5|     5|     5|   0|\n",
      "|         3|     3|     1|     1|     5|   1|\n",
      "|         3|     3|     1|     1|     5|   0|\n",
      "|         3|     3|     6|     6|     3|   1|\n",
      "|         3|     3|     6|     6|     5|   0|\n",
      "+----------+------+------+------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_train = df.count()\n",
    "df_all = df.union(df_test)\n",
    "df_all.createOrReplaceTempView(\"df_all\")\n",
    "query = \"\"\"\n",
    "SELECT customerid,\n",
    "       Dense_rank()\n",
    "         OVER(\n",
    "           partition BY 1\n",
    "           ORDER BY customerid) AS row_id,\n",
    "       itemid,\n",
    "       Dense_rank()\n",
    "         OVER(\n",
    "           partition BY 1\n",
    "           ORDER BY itemid)     AS col_id,\n",
    "       rating,\n",
    "       type\n",
    "FROM   df_all \n",
    "\"\"\"\n",
    "df_all = spark.sql(query)\n",
    "df_all.createOrReplaceTempView(\"df_all\")\n",
    "customer_index2ID = dict(df_all.select([\"row_id\", \"customerID\"]).rdd.reduceByKey(lambda k, v: v).collect())\n",
    "item_index2ID = dict(df_all.select([\"col_id\", \"itemID\"]).rdd.reduceByKey(lambda k, v: v).collect())\n",
    "df_all.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Compute Item Co-occurrence\n",
    "\n",
    "Central to how SAR defines similarity is an item-to-item ***co-occurrence matrix***. Co-occurrence is defined as the number of times two items appear together for a given user.  We can represent the co-occurrence of all items as a $mxm$ matrix $C$, where $c_{i,j}$   is the number of times item $i$ occurred with item $j$.\n",
    "\n",
    "The co-occurence matric $C$ has the following properties:\n",
    "- It is symmetric, so $c_{i,j} = c_{j,i}$\n",
    "- It is nonnegative: $c_{i,j} >= 0$\n",
    "- The occurrences are at least as large as the co-occurrences. I.e, the largest element for each row (and column) is on the main diagonal: $∀(i,j) C_{i,i},C_{j,j}>=C_{i,j}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+\n",
      "|row_id|col_id|rating|\n",
      "+------+------+------+\n",
      "|     1|     1|     5|\n",
      "|     1|     2|     5|\n",
      "|     1|     3|     5|\n",
      "|     2|     4|     1|\n",
      "|     2|     5|     1|\n",
      "|     3|     1|     5|\n",
      "|     3|     6|     3|\n",
      "+------+------+------+\n",
      "\n",
      "+------+------+------+\n",
      "|row_id|col_id|rating|\n",
      "+------+------+------+\n",
      "|     1|     1|     5|\n",
      "|     2|     1|     5|\n",
      "|     3|     1|     5|\n",
      "|     4|     2|     1|\n",
      "|     5|     2|     1|\n",
      "|     1|     3|     5|\n",
      "|     6|     3|     3|\n",
      "+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT row_id,\n",
    "       col_id,\n",
    "       rating\n",
    "FROM   df_all\n",
    "WHERE  type = 1 \n",
    "\"\"\"\n",
    "df = spark.sql(query)\n",
    "df.createOrReplaceTempView(\"df_train\")\n",
    "df.show()\n",
    "df_transpose = spark.sql(\"select col_id as row_id, row_id as col_id, rating from df_train\")\n",
    "df_transpose.createOrReplaceTempView(\"df_train_transpose\")\n",
    "df_transpose.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----+\n",
      "|row_item_id|col_item_id|value|\n",
      "+-----------+-----------+-----+\n",
      "|          6|          1|    1|\n",
      "|          3|          1|    1|\n",
      "|          2|          2|    1|\n",
      "|          2|          3|    1|\n",
      "|          1|          2|    1|\n",
      "|          1|          1|    2|\n",
      "|          1|          3|    1|\n",
      "|          5|          4|    1|\n",
      "|          3|          3|    1|\n",
      "|          2|          1|    1|\n",
      "|          3|          2|    1|\n",
      "|          4|          4|    1|\n",
      "|          6|          6|    1|\n",
      "|          1|          6|    1|\n",
      "|          4|          5|    1|\n",
      "|          5|          5|    1|\n",
      "+-----------+-----------+-----+\n",
      "\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT A.row_id AS row_item_id,\n",
    "       B.col_id AS col_item_id,\n",
    "       Sum(1)   AS value\n",
    "FROM   df_train_transpose A\n",
    "       INNER JOIN df_train B\n",
    "               ON A.col_id = B.row_id\n",
    "GROUP  BY A.row_id,\n",
    "          B.col_id\n",
    "\"\"\"\n",
    "item_cooccurrence = spark.sql(query)\n",
    "item_cooccurrence.createOrReplaceTempView(\"item_cooccurrence\")\n",
    "item_cooccurrence.show()\n",
    "print(item_cooccurrence.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1 1 0 0 1]\n",
      " [1 1 1 0 0 0]\n",
      " [1 1 1 0 0 0]\n",
      " [0 0 0 1 1 0]\n",
      " [0 0 0 1 1 0]\n",
      " [1 0 0 0 0 1]]\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "indicator = a_train.copy()\n",
    "indicator[indicator>0]=1\n",
    "item_cooccurrence = indicator.T.dot(indicator)\n",
    "print (item_cooccurrence)\n",
    "print ((item_cooccurrence>0).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Compute Item Similarity\n",
    "\n",
    "\n",
    "Once we have a co-occurrence matrix, an ***item similarity matrix*** $S$ can be obtained by rescaling the co-occurrences according to a given metric. Options for the metric include Jaccard, lift, and counts (meaning no rescaling).\n",
    "\n",
    "The rescaling formula for Jaccard is $s_{ij}=c_{ij} / (c_{ii}+c_{jj}-c_{ij})$\n",
    "\n",
    "and that for lift is $s_{ij}=c_{ij}/(c_{ii}*c_{jj})$\n",
    "\n",
    "where $c_{ii}$ and $c_{jj}$ are the $i$th and $j$th diagonal elements of $C$. In general, using counts as a similarity metric favours predictability, meaning that the most popular items will be recommended most of the time. Lift by contrast favours discoverability/serendipity: an item that is less popular overall but highly favoured by a small subset of users is more likely to be recommended. Jaccard is a compromise between the two.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'item_cooccurrence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-bfc2c6878d44>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# show to who to compute Jaccard\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdiag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitem_cooccurrence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiagonal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdiag_rows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdiag_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# this essentially does vstack(diag_rows).T + vstack(diag_rows) - cooccurrence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'item_cooccurrence' is not defined"
     ]
    }
   ],
   "source": [
    "# show to who to compute Jaccard\n",
    "diag = item_cooccurrence.diagonal()\n",
    "diag_rows = np.expand_dims(diag, axis=0)\n",
    "diag_cols = np.expand_dims(diag, axis=1)\n",
    "# this essentially does vstack(diag_rows).T + vstack(diag_rows) - cooccurrence\n",
    "denom = diag_rows + diag_cols - item_cooccurrence\n",
    "jaccard = item_cooccurrence / denom\n",
    "print (\"Jaccard\")\n",
    "print (jaccard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SIMILARITY is 'jaccard' or SIMILARITY is 'lift':\n",
    "    query = \"\"\"\n",
    "    SELECT A.row_item_id AS i,\n",
    "           A.value       AS d\n",
    "    FROM   item_cooccurrence A\n",
    "    WHERE  A.row_item_id = A.col_item_id \n",
    "    \"\"\"\n",
    "    diagonal = spark.sql(query)\n",
    "    diagonal.createOrReplaceTempView(\"diagonal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----+\n",
      "|row_item_id|col_item_id|value|\n",
      "+-----------+-----------+-----+\n",
      "|          1|          1|  1.0|\n",
      "|          6|          1|  0.5|\n",
      "|          3|          1|  0.5|\n",
      "|          2|          1|  0.5|\n",
      "|          1|          6|  0.5|\n",
      "|          6|          6|  1.0|\n",
      "|          1|          3|  0.5|\n",
      "|          3|          3|  1.0|\n",
      "|          2|          3|  1.0|\n",
      "|          5|          5|  1.0|\n",
      "|          4|          5|  1.0|\n",
      "|          5|          4|  1.0|\n",
      "|          4|          4|  1.0|\n",
      "|          1|          2|  0.5|\n",
      "|          3|          2|  1.0|\n",
      "|          2|          2|  1.0|\n",
      "+-----------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "similarity = None\n",
    "if SIMILARITY is \"jaccard\":\n",
    "    query = \"\"\"\n",
    "    SELECT A.row_item_id,\n",
    "           A.col_item_id,\n",
    "           ( A.value / ( B.d + C.d - A.value ) ) AS value\n",
    "    FROM   item_cooccurrence AS A,\n",
    "           diagonal AS B,\n",
    "           diagonal AS C\n",
    "    WHERE  A.row_item_id = B.i\n",
    "           AND A.col_item_id = C.i \n",
    "    \"\"\"\n",
    "    similarity = spark.sql(query)\n",
    "elif SIMILARITY is 'lift':\n",
    "    query = \"\"\"\n",
    "    SELECT A.row_item_id,\n",
    "           A.col_item_id,\n",
    "           ( A.value / ( B.d * C.d ) ) AS value\n",
    "    FROM   item_cooccurrence AS A,\n",
    "           diagonal AS B,\n",
    "           diagonal AS C\n",
    "    WHERE  A.row_item_id = B.i\n",
    "           AND A.col_item_id = C.i \n",
    "    \"\"\"\n",
    "    similarity = spark.sql(query)\n",
    "else:\n",
    "    similarity = item_cooccurrence\n",
    "similarity.createOrReplaceTempView(\"item_similarity\")\n",
    "similarity.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Compute User Affinity Scores\n",
    "\n",
    "The affinity matrix in SAR captures the strength of the relationship between each individual user and each item. The event types and weights are used in computing this matrix: different event types (such as “rate” vs “view”) should be allowed to have an impact on a user’s affinity for an item. Similarly, the time of a transaction should have an impact; an event that takes place in the distant past can be thought of as being less important in determining the affinity.\n",
    "\n",
    "Combining these effects gives us an expression for user-item affinity:\n",
    "$a_{ij}=Σ_k (w_k exp[-log_2((t_0-t_k)/T)] $\n",
    "\n",
    "where the affinity for user $i$ and item $j$ is the sum of all events involving user $i$ and item $j$, and $w_k$ is the weight of event $k$. The presence of the  $log_{2}$ factor means that the parameter $T$ in the exponential decay term can be treated as a half-life: events this far before the reference date $t_0$ will be given half the weight as those taking place at $t_0$. \n",
    "\n",
    "Repeating this computation for all $n$ users and $m$ items results in an $nxm$ matrix $A$.\n",
    "Simplifications of the above expression can be obtained by setting all the weights equal to 1 (effectively ignoring event types), or by setting the half-life parameter $T$ to infinity (ignoring transaction times).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----+\n",
      "|row_user_id|col_item_id|score|\n",
      "+-----------+-----------+-----+\n",
      "|          3|          1|  6.5|\n",
      "|          1|          2| 12.5|\n",
      "|          1|          1| 10.0|\n",
      "|          1|          3| 12.5|\n",
      "|          2|          5|  2.0|\n",
      "|          3|          3|  2.5|\n",
      "|          2|          4|  2.0|\n",
      "|          3|          6|  5.5|\n",
      "|          3|          2|  2.5|\n",
      "|          1|          6|  2.5|\n",
      "+-----------+-----------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT A.row_id                AS row_user_id,\n",
    "       B.col_item_id,\n",
    "       Sum(A.rating * B.value) AS score\n",
    "FROM   df_train A\n",
    "       INNER JOIN item_similarity B\n",
    "               ON A.col_id = B.row_item_id\n",
    "GROUP  BY A.row_id,\n",
    "          B.col_item_id \n",
    "\"\"\"\n",
    "scores = spark.sql(query)\n",
    "scores.show()\n",
    "scores.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Remove Seen Items\n",
    "\n",
    "Optionally we remove items which have already been seen in the training set, i.e. don't recommend items which have been previously bought by the user again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keeping seen items\n",
      "+-----------+-----------+------+\n",
      "|row_user_id|col_item_id|rating|\n",
      "+-----------+-----------+------+\n",
      "|          3|          1|   6.5|\n",
      "|          1|          2|  12.5|\n",
      "|          1|          1|  10.0|\n",
      "|          1|          3|  12.5|\n",
      "|          2|          5|   2.0|\n",
      "|          3|          3|   2.5|\n",
      "|          2|          4|   2.0|\n",
      "|          3|          6|   5.5|\n",
      "|          3|          2|   2.5|\n",
      "|          1|          6|   2.5|\n",
      "+-----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not RECOMMEND_SEEN:\n",
    "    print (\"Removing seen items\")\n",
    "    masked_scores = scores\\\n",
    "        .join(df, (scores.row_user_id == df.row_id) & (scores.col_item_id == df.col_id), \"left_outer\")    \n",
    "    masked_scores.show()\n",
    "    # now since training set is smaller, we have nulls under its value column, i.e. item is not in the\n",
    "    # training set\n",
    "    masked_scores = \\\n",
    "        masked_scores.withColumn(\"rating\", F.when(F.col('rating').isNull(), F.col('score')).otherwise(0))\n",
    "else:\n",
    "    print (\"Keeping seen items\")\n",
    "    scores.createOrReplaceTempView(\"scores\")\n",
    "    masked_scores = spark.sql(\"select row_user_id, col_item_id, score as rating from scores\")\n",
    "masked_scores.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Top-K Item Calculation\n",
    "\n",
    "The personalized recommendations for a set of users can then be obtained by multiplying the affinity matrix by the similarity matrix. The result is an recommendation score matrix, with one row per user / item pair; higher scores correspond to more strongly recommended items.\n",
    "\n",
    "This is the unoptimized way of performing top-K on Spark - although this is very readable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------+---+\n",
      "|row_user_id|col_item_id|rating|top|\n",
      "+-----------+-----------+------+---+\n",
      "|          1|          2|  12.5|  1|\n",
      "|          1|          3|  12.5|  2|\n",
      "|          3|          1|   6.5|  1|\n",
      "|          3|          6|   5.5|  2|\n",
      "|          2|          5|   2.0|  1|\n",
      "|          2|          4|   2.0|  2|\n",
      "+-----------+-----------+------+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window = Window.partitionBy(masked_scores[\"row_user_id\"]).orderBy(masked_scores[\"rating\"].desc())\n",
    "#top_scores =\\\n",
    "#    masked_scores.select(\"*\", F.rank().over(window).alias(\"top\")).filter(F.col(\"top\")<=TOP_K)\n",
    "top_scores =\\\n",
    "    masked_scores.select(\"*\", F.row_number().over(window).alias(\"top\")).filter(F.col(\"top\")<=TOP_K)\n",
    "top_scores.show()\n",
    "top_scores.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Optimized Top-K Item Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|row_user_id|              tuples|\n",
      "+-----------+--------------------+\n",
      "|          1|[[2, 12.5], [1, 1...|\n",
      "|          3|[[1, 6.5], [3, 2....|\n",
      "|          2|[[5, 2.0], [4, 2.0]]|\n",
      "+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pivot the ratings by user and item\n",
    "pivoted_scores = masked_scores.withColumn(\"combined\", F.struct(\"col_item_ID\", \"rating\"))\\\n",
    "    .groupBy(\"row_user_id\").agg(F.collect_list(F.col(\"combined\")).alias(\"tuples\"))\n",
    "pivoted_scores.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the items by their scores\n",
    "def wrapped_fun(tuples, params):\n",
    "    \"\"\"\n",
    "    Use heapq to sort the items by ratings for each user - complexity analysis provided here:\n",
    "    \n",
    "    \"\"\"\n",
    "    # TODO: can add params here if needed\n",
    "    n, sort_key = params\n",
    "    print(n, sort_key)\n",
    "    return heapq.nlargest(n, tuples, key = lambda l: l[sort_key])\n",
    "\n",
    "# wraps the above function so that we can pass in parameters in UDF\n",
    "def udf_wrapper_fun(params):\n",
    "    # notice that if needed, this can also pass in user_ID to the create_random_ratings function\n",
    "    schema = ArrayType(StructType((StructField(\"ItemID\", StringType()),\n",
    "                                   StructField(\"Rating\", FloatType()))))\n",
    "\n",
    "    return F.udf(lambda tuples: wrapped_fun(tuples, params), schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|row_user_id|              tuples|\n",
      "+-----------+--------------------+\n",
      "|          1|[[2, 12.5], [3, 1...|\n",
      "|          3|[[1, 6.5], [6, 5.5]]|\n",
      "|          2|[[5, 2.0], [4, 2.0]]|\n",
      "+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# top 2 items and 1 as sort key index\n",
    "params = (TOP_K, 1)\n",
    "sorted_pivoted_scores = pivoted_scores.withColumn(\"tuples\", udf_wrapper_fun(params)(F.col(\"tuples\")))\n",
    "sorted_pivoted_scores.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+------+\n",
      "|row_user_id|ItemID|Rating|\n",
      "+-----------+------+------+\n",
      "|          1|     2|  12.5|\n",
      "|          1|     3|  12.5|\n",
      "|          3|     1|   6.5|\n",
      "|          3|     6|   5.5|\n",
      "|          2|     5|   2.0|\n",
      "|          2|     4|   2.0|\n",
      "+-----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exploded_df = sorted_pivoted_scores.select(\"*\", F.explode(\"tuples\").alias(\"exploded_tuples\"))\n",
    "exploded_df = exploded_df\\\n",
    ".withColumn(\"ItemID\", F.col(\"exploded_tuples\").getItem(\"ItemID\"))\\\n",
    ".withColumn(\"Rating\", F.col(\"exploded_tuples\").getItem(\"Rating\"))\n",
    "\n",
    "top_scores = exploded_df.drop(\"tuples\").drop(\"exploded_tuples\")\n",
    "top_scores.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Method\n",
    "\n",
    "Please note that certain Machine Learning frameworks require a recommendation algorithm to have a .predict method. SAR is a ranker - it uses internally-generated SAR scores to rank the items for each user. We can output these SAR scores as a proxy for the rating - the higher the better - however we obviously cannot directly interpret those as ratings.\n",
    "\n",
    "The proxy is simple - we just subset the full set of scores on user-item pairs found in the test set. This is easy to do on Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+\n",
      "|row_id|col_id|rating|\n",
      "+------+------+------+\n",
      "|     1|     4|     1|\n",
      "|     1|     5|     1|\n",
      "|     2|     1|     5|\n",
      "|     2|     5|     5|\n",
      "|     3|     1|     5|\n",
      "|     3|     6|     5|\n",
      "+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT row_id,\n",
    "       col_id,\n",
    "       rating\n",
    "FROM   df_all\n",
    "WHERE  type = 0 \n",
    "\"\"\"\n",
    "df_test = spark.sql(query)\n",
    "df_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = df_test.join(\n",
    "    masked_scores, \n",
    "    (df_test.row_id==masked_scores.row_user_id) & (df_test.col_id==masked_scores.col_item_id),\n",
    "    \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+-----------+-----------+------+\n",
      "|row_id|col_id|rating|row_user_id|col_item_id|rating|\n",
      "+------+------+------+-----------+-----------+------+\n",
      "|     3|     1|     5|          3|          1|   6.5|\n",
      "|     2|     5|     5|          2|          5|   2.0|\n",
      "|     3|     6|     5|          3|          6|   5.5|\n",
      "+------+------+------+-----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (reco)",
   "language": "python",
   "name": "reco"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
