{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAR Deep Dive with Spark and SQL\n",
    "\n",
    "In this example, we will walkthrough each step of the SAR algorithm with an implementation using Spark and SQL.\n",
    "\n",
    "Smart Adaptive Recommendations (SAR) is a fast, scalable, adaptive algorithm for personalized recommendations based on user transaction history and item descriptions. It is powered by understanding the **similarity** between items, and recommending similar items to ones a user has an existing **affinity** for. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Global Variables and Imports"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 2,
>>>>>>> upstream/staging
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "System version: 3.6.2 |Anaconda, Inc.| (default, Sep 21 2017, 18:29:43) \n",
      "[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]\n",
      "Pandas version: 0.20.3\n",
      "PySpark version: 2.3.2\n"
=======
      "System version: 3.6.5 |Anaconda, Inc.| (default, Apr 29 2018, 16:14:56) \n",
      "[GCC 7.2.0]\n",
      "Pandas version: 0.23.4\n",
      "PySpark version: 2.3.1\n"
>>>>>>> upstream/staging
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import heapq\n",
    "import os\n",
    "import pyspark.sql.functions as F\n",
    "import sys\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, Row, ArrayType, IntegerType, FloatType\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pysarplus import SARPlus\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Pandas version: {}\".format(pd.__version__))\n",
    "print(\"PySpark version: {}\".format(pyspark.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify parameters\n",
    "TOP_K=2\n",
    "RECOMMEND_SEEN=True\n",
    "# options are 'jaccard', 'lift' or '' to skip and use item cooccurrence directly\n",
    "SIMILARITY='jaccard'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Load Data\n",
    "\n",
    "We'll work with a small dataset here containing customer IDs, item IDs, and the customer's rating for the item. SAR requires inputs to be of the following schema: `<User ID>, <Item ID>, <Time>, [<Event Type>], [<Event Weight>]` (we will not use time or event type in the example below, and `rating` will be used as the `Event Weight`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are two versions of the dataframes - the numeric version and the alphanumeric one:\n",
    "# they both have similar test data for top-2 recommendations and illustrate the indexing approaches to matrix multiplication on SQL\n",
    "d_train = {\n",
    "'customerID': [1,1,1,2,2,3,3],\n",
    "'itemID':     [1,2,3,4,5,6,1],\n",
    "'rating':     [5,5,5,1,1,3,5]\n",
    "}\n",
    "pdf_train = pd.DataFrame(d_train)\n",
    "d_test = {\n",
    "'customerID': [1,1,2,2,3,3],\n",
    "'itemID':     [4,5,1,5,6,1],\n",
    "'rating':     [1,1,5,5,5,5]\n",
    "}\n",
    "pdf_test = pd.DataFrame(d_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5 5 5 0 0 0]\n",
      " [0 0 0 1 1 0]\n",
      " [5 0 0 0 0 3]]\n",
      "(3, 6)\n"
     ]
    }
   ],
   "source": [
    "a_train = np.array([[5,5,5,0,0,0],\\\n",
    "                    [0,0,0,1,1,0],\n",
    "                    [5,0,0,0,0,3]])\n",
    "print(a_train)\n",
    "print(a_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customerID  itemID  rating\n",
       "0           1       4       1\n",
       "1           1       5       1\n",
       "2           2       1       5\n",
       "3           2       5       5\n",
       "4           3       6       5\n",
       "5           3       1       5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_alnum_train = {\n",
    "'customerID': ['ua','ua','ua','ub','ub','uc','uc'],\n",
    "'itemID':     ['ia','ib','ic','id','ie','if','ia'],\n",
    "'rating':     [5,5,5,1,1,3,5]\n",
    "}\n",
    "#pdf_train = pd.DataFrame(d_alnum_train)\n",
    "pdf_train = pd.DataFrame(d_train)\n",
    "d_alnum_test = {\n",
    "'customerID': ['ua','ua','ub','ub','uc','uc'],\n",
    "'itemID':     ['id','ie','ia','ie','if','ia'],\n",
    "'rating':     [1,1,5,5,5,5]\n",
    "}\n",
    "#pdf_test = pd.DataFrame(d_alnum_test)\n",
    "pdf_test = pd.DataFrame(d_test)\n",
    "pdf_test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Spark context\n",
    "\n",
    "The following settings work well for debugging locally on VM - change when running on a cluster. We set up a giant single executor with many threads and specify memory cap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMIT_ARGS = \"--packages eisber:sarplus:0.2.2 pyspark-shell\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = SUBMIT_ARGS\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"SAR pySpark\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"memory\", \"4G\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"2\") \\\n",
    "    .config(\"spark.sql.crossJoin.enabled\", True) \\\n",
    "    .config(\"spark.ui.enabled\", False) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>rating</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customerID  itemID  rating  type\n",
       "0           1       1       5     1\n",
       "1           1       2       5     1\n",
       "2           1       3       5     1\n",
       "3           2       4       1     1\n",
       "4           2       5       1     1\n",
       "5           3       6       3     1\n",
       "6           3       1       5     1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame(pdf_train).withColumn(\"type\", F.lit(1))\n",
    "df_test = spark.createDataFrame(pdf_test).withColumn(\"type\", F.lit(0))\n",
<<<<<<< HEAD
    "df.toPandas()"
=======
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Index the user and item IDs\n",
    "\n",
    "Map user and item alphanumeric IDs to matrix indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+------+------+----+\n",
      "|customerid|row_id|itemid|col_id|rating|type|\n",
      "+----------+------+------+------+------+----+\n",
      "|         1|     1|     1|     1|     5|   1|\n",
      "|         2|     2|     1|     1|     5|   0|\n",
      "|         3|     3|     1|     1|     5|   1|\n",
      "|         3|     3|     1|     1|     5|   0|\n",
      "|         1|     1|     2|     2|     5|   1|\n",
      "|         1|     1|     3|     3|     5|   1|\n",
      "|         1|     1|     4|     4|     1|   0|\n",
      "|         2|     2|     4|     4|     1|   1|\n",
      "|         1|     1|     5|     5|     1|   0|\n",
      "|         2|     2|     5|     5|     1|   1|\n",
      "|         2|     2|     5|     5|     5|   0|\n",
      "|         3|     3|     6|     6|     3|   1|\n",
      "|         3|     3|     6|     6|     5|   0|\n",
      "+----------+------+------+------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_train = df.count()\n",
    "df_all = df.union(df_test)\n",
    "df_all.createOrReplaceTempView(\"df_all\")\n",
    "query = \"\"\"\n",
    "SELECT customerid,\n",
    "       Dense_rank()\n",
    "         OVER(\n",
    "           partition BY 1\n",
    "           ORDER BY customerid) AS row_id,\n",
    "       itemid,\n",
    "       Dense_rank()\n",
    "         OVER(\n",
    "           partition BY 1\n",
    "           ORDER BY itemid)     AS col_id,\n",
    "       rating,\n",
    "       type\n",
    "FROM   df_all \n",
    "\"\"\"\n",
    "df_all = spark.sql(query)\n",
    "df_all.createOrReplaceTempView(\"df_all\")\n",
    "customer_index2ID = dict(df_all.select([\"row_id\", \"customerID\"]).rdd.reduceByKey(lambda k, v: v).collect())\n",
    "item_index2ID = dict(df_all.select([\"col_id\", \"itemID\"]).rdd.reduceByKey(lambda k, v: v).collect())\n",
    "df_all.show()"
>>>>>>> upstream/staging
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Compute Item Co-occurrence and Item Similarity\n",
    "\n",
    "Central to how SAR defines similarity is an item-to-item ***co-occurrence matrix***. Co-occurrence is defined as the number of times two items appear together for a given user.  We can represent the co-occurrence of all items as a $mxm$ matrix $C$, where $c_{i,j}$   is the number of times item $i$ occurred with item $j$.\n",
    "\n",
    "The co-occurence matric $C$ has the following properties:\n",
    "- It is symmetric, so $c_{i,j} = c_{j,i}$\n",
    "- It is nonnegative: $c_{i,j} >= 0$\n",
<<<<<<< HEAD
    "- The occurrences are at least as large as the co-occurrences. I.e, the largest element for each row (and column) is on the main diagonal: $∀(i,j) C_{i,i},C_{j,j}>=C_{i,j}$.\n",
=======
    "- The occurrences are at least as large as the co-occurrences. I.e, the largest element for each row (and column) is on the main diagonal: $∀(i,j) C_{i,i},C_{j,j}>=C_{i,j}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+\n",
      "|row_id|col_id|rating|\n",
      "+------+------+------+\n",
      "|     1|     1|     5|\n",
      "|     3|     1|     5|\n",
      "|     1|     2|     5|\n",
      "|     1|     3|     5|\n",
      "|     2|     4|     1|\n",
      "|     2|     5|     1|\n",
      "|     3|     6|     3|\n",
      "+------+------+------+\n",
      "\n",
      "+------+------+------+\n",
      "|row_id|col_id|rating|\n",
      "+------+------+------+\n",
      "|     1|     1|     5|\n",
      "|     1|     3|     5|\n",
      "|     2|     1|     5|\n",
      "|     3|     1|     5|\n",
      "|     4|     2|     1|\n",
      "|     5|     2|     1|\n",
      "|     6|     3|     3|\n",
      "+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT row_id,\n",
    "       col_id,\n",
    "       rating\n",
    "FROM   df_all\n",
    "WHERE  type = 1 \n",
    "\"\"\"\n",
    "df = spark.sql(query)\n",
    "df.createOrReplaceTempView(\"df_train\")\n",
    "df.show()\n",
    "df_transpose = spark.sql(\"select col_id as row_id, row_id as col_id, rating from df_train\")\n",
    "df_transpose.createOrReplaceTempView(\"df_train_transpose\")\n",
    "df_transpose.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----+\n",
      "|row_item_id|col_item_id|value|\n",
      "+-----------+-----------+-----+\n",
      "|          6|          1|    1|\n",
      "|          3|          1|    1|\n",
      "|          2|          2|    1|\n",
      "|          2|          3|    1|\n",
      "|          1|          2|    1|\n",
      "|          1|          1|    2|\n",
      "|          1|          3|    1|\n",
      "|          5|          4|    1|\n",
      "|          3|          3|    1|\n",
      "|          2|          1|    1|\n",
      "|          3|          2|    1|\n",
      "|          4|          4|    1|\n",
      "|          6|          6|    1|\n",
      "|          1|          6|    1|\n",
      "|          4|          5|    1|\n",
      "|          5|          5|    1|\n",
      "+-----------+-----------+-----+\n",
      "\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT A.row_id AS row_item_id,\n",
    "       B.col_id AS col_item_id,\n",
    "       Sum(1)   AS value\n",
    "FROM   df_train_transpose A\n",
    "       INNER JOIN df_train B\n",
    "               ON A.col_id = B.row_id\n",
    "GROUP  BY A.row_id,\n",
    "          B.col_id\n",
    "\"\"\"\n",
    "item_cooccurrence = spark.sql(query)\n",
    "item_cooccurrence.createOrReplaceTempView(\"item_cooccurrence\")\n",
    "item_cooccurrence.show()\n",
    "print(item_cooccurrence.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1 1 0 0 1]\n",
      " [1 1 1 0 0 0]\n",
      " [1 1 1 0 0 0]\n",
      " [0 0 0 1 1 0]\n",
      " [0 0 0 1 1 0]\n",
      " [1 0 0 0 0 1]]\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "indicator = a_train.copy()\n",
    "indicator[indicator>0]=1\n",
    "item_cooccurrence = indicator.T.dot(indicator)\n",
    "print(item_cooccurrence)\n",
    "print((item_cooccurrence>0).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Compute Item Similarity\n",
    "\n",
>>>>>>> upstream/staging
    "\n",
    "Once we have a co-occurrence matrix, an ***item similarity matrix*** $S$ can be obtained by rescaling the co-occurrences according to a given metric. Options for the metric include Jaccard, lift, and counts (meaning no rescaling).\n",
    "\n",
    "The rescaling formula for Jaccard is $s_{ij}=c_{ij} / (c_{ii}+c_{jj}-c_{ij})$\n",
    "\n",
    "and that for lift is $s_{ij}=c_{ij}/(c_{ii}*c_{jj})$\n",
    "\n",
<<<<<<< HEAD
    "where $c_{ii}$ and $c_{jj}$ are the $i$th and $j$th diagonal elements of $C$. In general, using counts as a similarity metric favours predictability, meaning that the most popular items will be recommended most of the time. Lift by contrast favours discoverability/serendipity: an item that is less popular overall but highly favoured by a small subset of users is more likely to be recommended. Jaccard is a compromise between the two."
=======
    "where $c_{ii}$ and $c_{jj}$ are the $i$th and $j$th diagonal elements of $C$. In general, using counts as a similarity metric favours predictability, meaning that the most popular items will be recommended most of the time. Lift by contrast favours discoverability/serendipity: an item that is less popular overall but highly favoured by a small subset of users is more likely to be recommended. Jaccard is a compromise between the two.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard\n",
      "[[1.  0.5 0.5 0.  0.  0.5]\n",
      " [0.5 1.  1.  0.  0.  0. ]\n",
      " [0.5 1.  1.  0.  0.  0. ]\n",
      " [0.  0.  0.  1.  1.  0. ]\n",
      " [0.  0.  0.  1.  1.  0. ]\n",
      " [0.5 0.  0.  0.  0.  1. ]]\n"
     ]
    }
   ],
   "source": [
    "# show to who to compute Jaccard\n",
    "diag = item_cooccurrence.diagonal()\n",
    "diag_rows = np.expand_dims(diag, axis=0)\n",
    "diag_cols = np.expand_dims(diag, axis=1)\n",
    "# this essentially does vstack(diag_rows).T + vstack(diag_rows) - cooccurrence\n",
    "denom = diag_rows + diag_cols - item_cooccurrence\n",
    "jaccard = item_cooccurrence / denom\n",
    "print (\"Jaccard\")\n",
    "print (jaccard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SIMILARITY is 'jaccard' or SIMILARITY is 'lift':\n",
    "    query = \"\"\"\n",
    "    SELECT A.row_item_id AS i,\n",
    "           A.value       AS d\n",
    "    FROM   item_cooccurrence A\n",
    "    WHERE  A.row_item_id = A.col_item_id \n",
    "    \"\"\"\n",
    "    diagonal = spark.sql(query)\n",
    "    diagonal.createOrReplaceTempView(\"diagonal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----+\n",
      "|row_item_id|col_item_id|value|\n",
      "+-----------+-----------+-----+\n",
      "|          1|          1|  1.0|\n",
      "|          6|          1|  0.5|\n",
      "|          3|          1|  0.5|\n",
      "|          2|          1|  0.5|\n",
      "|          1|          6|  0.5|\n",
      "|          6|          6|  1.0|\n",
      "|          1|          3|  0.5|\n",
      "|          3|          3|  1.0|\n",
      "|          2|          3|  1.0|\n",
      "|          5|          5|  1.0|\n",
      "|          4|          5|  1.0|\n",
      "|          5|          4|  1.0|\n",
      "|          4|          4|  1.0|\n",
      "|          1|          2|  0.5|\n",
      "|          3|          2|  1.0|\n",
      "|          2|          2|  1.0|\n",
      "+-----------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "similarity = None\n",
    "if SIMILARITY is \"jaccard\":\n",
    "    query = \"\"\"\n",
    "    SELECT A.row_item_id,\n",
    "           A.col_item_id,\n",
    "           ( A.value / ( B.d + C.d - A.value ) ) AS value\n",
    "    FROM   item_cooccurrence AS A,\n",
    "           diagonal AS B,\n",
    "           diagonal AS C\n",
    "    WHERE  A.row_item_id = B.i\n",
    "           AND A.col_item_id = C.i \n",
    "    \"\"\"\n",
    "    similarity = spark.sql(query)\n",
    "elif SIMILARITY is 'lift':\n",
    "    query = \"\"\"\n",
    "    SELECT A.row_item_id,\n",
    "           A.col_item_id,\n",
    "           ( A.value / ( B.d * C.d ) ) AS value\n",
    "    FROM   item_cooccurrence AS A,\n",
    "           diagonal AS B,\n",
    "           diagonal AS C\n",
    "    WHERE  A.row_item_id = B.i\n",
    "           AND A.col_item_id = C.i \n",
    "    \"\"\"\n",
    "    similarity = spark.sql(query)\n",
    "else:\n",
    "    similarity = item_cooccurrence\n",
    "similarity.createOrReplaceTempView(\"item_similarity\")\n",
    "similarity.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Compute User Affinity Scores\n",
    "\n",
    "The affinity matrix in SAR captures the strength of the relationship between each individual user and each item. The event types and weights are used in computing this matrix: different event types (such as “rate” vs “view”) should be allowed to have an impact on a user’s affinity for an item. Similarly, the time of a transaction should have an impact; an event that takes place in the distant past can be thought of as being less important in determining the affinity.\n",
    "\n",
    "Combining these effects gives us an expression for user-item affinity:\n",
    "$a_{ij}=Σ_k (w_k exp[-log_2((t_0-t_k)/T)] $\n",
    "\n",
    "where the affinity for user $i$ and item $j$ is the sum of all events involving user $i$ and item $j$, and $w_k$ is the weight of event $k$. The presence of the  $log_{2}$ factor means that the parameter $T$ in the exponential decay term can be treated as a half-life: events this far before the reference date $t_0$ will be given half the weight as those taking place at $t_0$. \n",
    "\n",
    "Repeating this computation for all $n$ users and $m$ items results in an $nxm$ matrix $A$.\n",
    "Simplifications of the above expression can be obtained by setting all the weights equal to 1 (effectively ignoring event types), or by setting the half-life parameter $T$ to infinity (ignoring transaction times).\n"
>>>>>>> upstream/staging
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
=======
   "execution_count": 16,
>>>>>>> upstream/staging
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sarplus:sarplus.fit 1/2: compute item cooccurences...\n",
      "INFO:sarplus:sarplus.fit 2/2: compute similiarity metric jaccard...\n"
     ]
    },
    {
     "data": {
<<<<<<< HEAD
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i1</th>\n",
       "      <th>i2</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    i1  i2  value\n",
       "0    1   2    0.5\n",
       "1    1   6    0.5\n",
       "2    1   1    1.0\n",
       "3    1   3    0.5\n",
       "4    3   3    1.0\n",
       "5    3   2    1.0\n",
       "6    3   1    0.5\n",
       "7    6   6    1.0\n",
       "8    6   1    0.5\n",
       "9    2   2    1.0\n",
       "10   2   3    1.0\n",
       "11   2   1    0.5\n",
       "12   4   4    1.0\n",
       "13   4   5    1.0\n",
       "14   5   5    1.0\n",
       "15   5   4    1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
=======
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT A.row_id                AS row_user_id,\n",
    "       B.col_item_id,\n",
    "       Sum(A.rating * B.value) AS score\n",
    "FROM   df_train A\n",
    "       INNER JOIN item_similarity B\n",
    "               ON A.col_id = B.row_item_id\n",
    "GROUP  BY A.row_id,\n",
    "          B.col_item_id \n",
    "\"\"\"\n",
    "scores = spark.sql(query)\n",
    "scores.show()\n",
    "scores.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Remove Seen Items\n",
    "\n",
    "Optionally we remove items which have already been seen in the training set, i.e. don't recommend items which have been previously bought by the user again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keeping seen items\n",
      "+-----------+-----------+------+\n",
      "|row_user_id|col_item_id|rating|\n",
      "+-----------+-----------+------+\n",
      "|          3|          1|   6.5|\n",
      "|          1|          2|  12.5|\n",
      "|          1|          1|  10.0|\n",
      "|          1|          3|  12.5|\n",
      "|          2|          5|   2.0|\n",
      "|          3|          3|   2.5|\n",
      "|          2|          4|   2.0|\n",
      "|          3|          6|   5.5|\n",
      "|          3|          2|   2.5|\n",
      "|          1|          6|   2.5|\n",
      "+-----------+-----------+------+\n",
      "\n"
     ]
>>>>>>> upstream/staging
    }
   ],
   "source": [
    "model = SARPlus(spark, col_user='customerID', col_item='itemID', col_rating='rating')\n",
    "model.fit(df, similarity_type=SIMILARITY)\n",
    "\n",
    "model.item_similarity.toPandas()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 20,
=======
   "execution_count": 18,
>>>>>>> upstream/staging
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customerID  itemID  rating\n",
       "0           2       4     1.0\n",
       "1           2       5     1.0\n",
       "2           1       1     5.0\n",
       "3           1       2     5.0\n",
       "4           1       3     5.0\n",
       "5           3       1     5.0\n",
       "6           3       6     3.0"
      ]
     },
<<<<<<< HEAD
     "execution_count": 20,
=======
     "execution_count": 18,
>>>>>>> upstream/staging
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_user_affinity(df_test).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "# 5 Compute User Affinity Scores\n",
=======
    "### 7.1 Optimized Top-K Item Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|row_user_id|              tuples|\n",
      "+-----------+--------------------+\n",
      "|          1|[[2, 12.5], [1, 1...|\n",
      "|          3|[[1, 6.5], [3, 2....|\n",
      "|          2|[[5, 2.0], [4, 2.0]]|\n",
      "+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pivot the ratings by user and item\n",
    "pivoted_scores = masked_scores.withColumn(\"combined\", F.struct(\"col_item_ID\", \"rating\"))\\\n",
    "    .groupBy(\"row_user_id\").agg(F.collect_list(F.col(\"combined\")).alias(\"tuples\"))\n",
    "pivoted_scores.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the items by their scores\n",
    "def wrapped_fun(tuples, params):\n",
    "    \"\"\"\n",
    "    Use heapq to sort the items by ratings for each user - complexity analysis provided here:\n",
    "    \n",
    "    \"\"\"\n",
    "    # TODO: can add params here if needed\n",
    "    n, sort_key = params\n",
    "    print(n, sort_key)\n",
    "    return heapq.nlargest(n, tuples, key = lambda l: l[sort_key])\n",
>>>>>>> upstream/staging
    "\n",
    "The affinity matrix in SAR captures the strength of the relationship between each individual user and each item. The event types and weights are used in computing this matrix: different event types (such as “rate” vs “view”) should be allowed to have an impact on a user’s affinity for an item. Similarly, the time of a transaction should have an impact; an event that takes place in the distant past can be thought of as being less important in determining the affinity.\n",
    "\n",
<<<<<<< HEAD
    "Combining these effects gives us an expression for user-item affinity:\n",
    "$a_{ij}=Σ_k (w_k exp[-log_2((t_0-t_k)/T)] $\n",
=======
    "    return F.udf(lambda tuples: wrapped_fun(tuples, params), schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|row_user_id|              tuples|\n",
      "+-----------+--------------------+\n",
      "|          1|[[2, 12.5], [3, 1...|\n",
      "|          3|[[1, 6.5], [6, 5.5]]|\n",
      "|          2|[[5, 2.0], [4, 2.0]]|\n",
      "+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# top 2 items and 1 as sort key index\n",
    "params = (TOP_K, 1)\n",
    "sorted_pivoted_scores = pivoted_scores.withColumn(\"tuples\", udf_wrapper_fun(params)(F.col(\"tuples\")))\n",
    "sorted_pivoted_scores.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+------+\n",
      "|row_user_id|ItemID|Rating|\n",
      "+-----------+------+------+\n",
      "|          1|     2|  12.5|\n",
      "|          1|     3|  12.5|\n",
      "|          3|     1|   6.5|\n",
      "|          3|     6|   5.5|\n",
      "|          2|     5|   2.0|\n",
      "|          2|     4|   2.0|\n",
      "+-----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exploded_df = sorted_pivoted_scores.select(\"*\", F.explode(\"tuples\").alias(\"exploded_tuples\"))\n",
    "exploded_df = exploded_df\\\n",
    ".withColumn(\"ItemID\", F.col(\"exploded_tuples\").getItem(\"ItemID\"))\\\n",
    ".withColumn(\"Rating\", F.col(\"exploded_tuples\").getItem(\"Rating\"))\n",
>>>>>>> upstream/staging
    "\n",
    "where the affinity for user $i$ and item $j$ is the sum of all events involving user $i$ and item $j$, and $w_k$ is the weight of event $k$. The presence of the  $log_{2}$ factor means that the parameter $T$ in the exponential decay term can be treated as a half-life: events this far before the reference date $t_0$ will be given half the weight as those taking place at $t_0$. \n",
    "\n",
    "Repeating this computation for all $n$ users and $m$ items results in an $nxm$ matrix $A$.\n",
    "Simplifications of the above expression can be obtained by setting all the weights equal to 1 (effectively ignoring event types), or by setting the half-life parameter $T$ to infinity (ignoring transaction times).\n",
    "\n",
    "\n",
    "# 6 Remove Seen Items\n",
    "\n",
    "Optionally we remove items which have already been seen in the training set, i.e. don't recommend items which have been previously bought by the user again.\n",
    "\n",
    "# 7 Top-K Item Calculation\n",
    "\n",
    "The personalized recommendations for a set of users can then be obtained by multiplying the affinity matrix by the similarity matrix. The result is an recommendation score matrix, with one row per user / item pair; higher scores correspond to more strongly recommended items.\n",
    "\n",
    "This is the unoptimized way of performing top-K on Spark - although this is very readable:"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 15,
=======
   "execution_count": 23,
>>>>>>> upstream/staging
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "INFO:sarplus:sarplus.recommend_k_items 1/3: create item index\n",
      "INFO:sarplus:sarplus.recommend_k_items 2/3: prepare similarity matrix\n",
      "INFO:sarplus:sarplus.recommend_k_items 3/3: compute recommendations\n"
     ]
    },
=======
      "+------+------+------+\n",
      "|row_id|col_id|rating|\n",
      "+------+------+------+\n",
      "|     2|     1|     5|\n",
      "|     3|     1|     5|\n",
      "|     1|     4|     1|\n",
      "|     1|     5|     1|\n",
      "|     2|     5|     5|\n",
      "|     3|     6|     5|\n",
      "+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT row_id,\n",
    "       col_id,\n",
    "       rating\n",
    "FROM   df_all\n",
    "WHERE  type = 0 \n",
    "\"\"\"\n",
    "df_test = spark.sql(query)\n",
    "df_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = df_test.join(\n",
    "    masked_scores, \n",
    "    (df_test.row_id==masked_scores.row_user_id) & (df_test.col_id==masked_scores.col_item_id),\n",
    "    \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
>>>>>>> upstream/staging
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customerID  itemID  score\n",
       "0           1       6    2.5\n",
       "1           3       2    2.5\n",
       "2           3       3    2.5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.recommend_k_items(df_test, cache_path='sar_deep_dive_cache', top_k=2)\\\n",
    "    .toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
<<<<<<< HEAD
   "display_name": "Python (recommender)",
   "language": "python",
   "name": "recommender"
=======
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
>>>>>>> upstream/staging
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.6.2"
=======
   "version": "3.6.5"
>>>>>>> upstream/staging
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
