{"cells":[{"cell_type":"markdown","source":["# Building a Real-time Recommendation API\n\nThis reference architecture shows how to train a recommendation model using Azure Databricks and deploy it as an API by using Azure Cosmos DB, Azure Machine Learning, and Azure Kubernetes Service. This architecture can be generalized for most recommendation engine scenarios, including recommendations for products, movies, and news. \n\nScenario: A media organization wants to provide movie or video recommendations to its users. By providing personalized recommendations, the organization meets several business goals, including increased click-through rates, increased engagement on site, and higher user satisfaction.\n\nIn this reference, we train and deploy a real-time recommender service API that can provide the top-10 movie recommendations for a given user. \n\n### Architecture\n![architecture](https://raw.githubusercontent.com/Microsoft/Recommenders/staging/reco-arch.png?token=AD1gMqtKoLXMAdpihRg68hpCeLikr9CZks5cEHVIwA%3D%3D)\n\nThis architecture consists of the following components:\n* [Azure Databricks](https://docs.microsoft.com/en-us/azure/azure-databricks/what-is-azure-databricks), a development environment used to prepare input data and train the recommender model on a Spark cluster. Azure Databricks also provides an interactive workspace to run and collaborate on notebooks for any data processing or machine learning tasks. \n* [Azure Cosmos DB](https://docs.microsoft.com/en-us/azure/cosmos-db/introduction), a managed service for globally distributed data storage. In the recommendation use case, it is used to store the top-10 recommended movies for each user. Azure Cosmos DB is ideal for this scenario as it provides a guaranteed low latency of 10 ms to read the top recommended items for a given user. \n* [Azure Kubernetes Service](https://docs.microsoft.com/en-us/azure/aks/intro-kubernetes), used to deploy and operationalize a machine learning model service API on a Kubernetes cluster.\n* [Azure Machine Learning Service](https://docs.microsoft.com/en-us/azure/machine-learning/service/), a service used to track and manage machine learning models, and then package and deploy these models to a scalable Azure Kubernetes Service environment.\n\n\n### Table of Contents.\n1. Service Creation\n1. Training\n1. Scoring\n1. Operationalization"],"metadata":{}},{"cell_type":"markdown","source":["### Setup\n* Create a new Databricks cluster, using this configuration: `DB 4.1, Spark 2.3.0, Python3`.\n* Attach the following PyPi libraries to the custer. [See here for help adding a library.](https://docs.databricks.com/user-guide/libraries.html).\n    * Add Azure-cli via pypi: `azure-cli`.\n    * Add AzureML via Pypi: `azureml-sdk[databricks]`.\n    * Add pydocumentdb via Pypi: `pydocumentdb`.\n* Attach CosmosDB uber jar to the library. The jar can downloaded at https://search.maven.org/artifact/com.microsoft.azure/azure-cosmosdb-spark_2.3.0_2.11/1.2.2/jar. Make sure you download the **uber** version.\n* Add this repository utilies to the cluster as detailed in the [SETUP.md](../../SETUP.md##setup-guide-for-azure-databricks)."],"metadata":{}},{"cell_type":"markdown","source":["## 0. File Imports"],"metadata":{}},{"cell_type":"code","source":["import numpy as np\nimport os\nimport pandas as pd\nimport pprint\nimport shutil\nimport time, timeit\nimport urllib\nimport yaml\nimport json\nimport uuid\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nfrom azure.common.client_factory import get_client_from_cli_profile\nfrom azure.mgmt.compute import ComputeManagementClient\nimport azure.mgmt.cosmosdb\nimport azureml.core\nfrom azureml.core import Workspace\nfrom azureml.core.run import Run\nfrom azureml.core.experiment import Experiment\nfrom azureml.core.model import Model\nfrom azureml.core.image import ContainerImage\nfrom azureml.core.compute import AksCompute, ComputeTarget\nfrom azureml.core.webservice import Webservice, AksWebservice\n\n\nimport pydocumentdb\nimport pydocumentdb.document_client as document_client\n\nimport pyspark\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.recommendation import ALS\nfrom pyspark.sql import Row\nfrom pyspark.sql.types import StructType, StructField\nfrom pyspark.sql.types import StringType, FloatType, IntegerType, LongType\n\nfrom reco_utils.dataset import movielens\nfrom reco_utils.dataset.cosmos_cli import find_collection, read_collection, read_database, find_database\nfrom reco_utils.dataset.spark_splitters import spark_random_split\nfrom reco_utils.evaluation.spark_evaluation import SparkRatingEvaluation, SparkRankingEvaluation\n\nprint(\"PySpark version:\", pyspark.__version__)\nprint(\"Azure SDK version:\", azureml.core.VERSION)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["## I. Service Creation\nModify the **Subscription ID** to the subscription you would like to deploy to.\n\n#### Services created by this notebook:\n1. [Azure ML Service](https://docs.databricks.com/user-guide/libraries.html)\n1. [Azure Cosmos DB](https://azure.microsoft.com/en-us/services/cosmos-db/)\n1. [Azure Container Registery](https://docs.microsoft.com/en-us/azure/container-registry/)\n1. [Azure Container Instances](https://docs.microsoft.com/en-us/azure/container-instances/)\n1. [Azure Application Insights](https://azure.microsoft.com/en-us/services/monitor/)\n1. [Azure Storage](https://docs.microsoft.com/en-us/azure/storage/common/storage-account-overview)\n1. [Azure Key Vault](https://azure.microsoft.com/en-us/services/key-vault/)\n1. [Azure Kubernetes Service (AKS)](https://azure.microsoft.com/en-us/services/kubernetes-service/)"],"metadata":{}},{"cell_type":"code","source":["# Select the services names\nshort_uuid = str(uuid.uuid4())[:4]\nprefix = \"reco\" + short_uuid\ndata = \"mvl\"\nalgo = \"als\"\n\n# Add your subscription ID\nsubscription_id = \"\"\n"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# Resource group and workspace\nresource_group = prefix + \"_\" + data\nworkspace_name = prefix + \"_\"+data+\"_aml\"\nworkspace_region = \"westus2\"\nprint(\"Resource group:\", resource_group)\n\n# Columns\nuserCol = \"UserId\"\nitemCol = \"MovieId\"\nratingCol = \"Rating\"\n\n# CosmosDB\nlocation = workspace_region\naccount_name = prefix + \"-\" + data + \"-ds-sql\"\nDOCUMENTDB_DATABASE = \"recommendations\"\nDOCUMENTDB_COLLECTION = \"user_recommendations_\" + algo\n\n# AzureML\nhistory_name = 'spark-ml-notebook'\nmodel_name = data+\"-\"+algo+\"-reco.mml\" #NOTE: The name of a asset must be only letters or numerals, not contain spaces, and under 30 characters\nservice_name = data + \"-\" + algo\nexperiment_name = data + \"_\"+ algo +\"_Experiment\"\naks_name = prefix + '-aks'\n\ntrain_data_path = data + \"Train\"\ntest_data_path = data + \"Test\""],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["#### 1.1 Import or create the AzureML Workspace. \nThis command will check if the AML Workspace exists or not, and will create the workspace if it doesn't exist."],"metadata":{}},{"cell_type":"code","source":["ws = Workspace.create(name = workspace_name,\n                      subscription_id = subscription_id,\n                      resource_group = resource_group, \n                      location = workspace_region,\n                      exist_ok=True)\n\n# persist the subscription id, resource group name, and workspace name in aml_config/config.json.\nws.write_config()"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["#### 1.2 Create a Cosmos DB resource to store recommendation results:"],"metadata":{}},{"cell_type":"code","source":["client = get_client_from_cli_profile(azure.mgmt.cosmosdb.CosmosDB)\n\nasync_cosmosdb_create = client.database_accounts.create_or_update(\n    resource_group,\n    account_name,\n    {\n        'location': location,\n        'locations': [{\n            'location_name': location\n        }]\n    }\n)\naccount = async_cosmosdb_create.result()\n\nmy_keys = client.database_accounts.list_keys(\n    resource_group,\n    account_name\n)\n\nmaster_key = my_keys.primary_master_key\nendpoint = \"https://\" + account_name + \".documents.azure.com:443/\"\n\n#db client\nclient = document_client.DocumentClient(endpoint, {'masterKey': master_key})\n\nif find_database(client, DOCUMENTDB_DATABASE) == False:\n    db = client.CreateDatabase({ 'id': DOCUMENTDB_DATABASE })\nelse:\n    db = read_database(client, DOCUMENTDB_DATABASE)\n# Create collection options\noptions = {\n    'offerThroughput': 11000\n}\n\n# Create a collection\ncollection_definition = { 'id': DOCUMENTDB_COLLECTION, 'partitionKey': {'paths': ['/id'],'kind': 'Hash'} }\nif find_collection(client,DOCUMENTDB_DATABASE,  DOCUMENTDB_COLLECTION) ==False:\n    collection = client.CreateCollection(db['_self'], collection_definition, options)\nelse:\n    collection = read_collection(client, DOCUMENTDB_DATABASE, DOCUMENTDB_COLLECTION)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["secrets = {\n  \"Endpoint\": endpoint,\n  \"Masterkey\": master_key,\n  \"Database\": DOCUMENTDB_DATABASE,\n  \"Collection\": DOCUMENTDB_COLLECTION,\n  \"Upsert\": \"true\"\n}\nwith open(\"secrets.json\", \"w\") as file:\n    json.dump(secrets, file)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["## 2. Training\n\nNext, we will train an [Alternating Least Squares model](https://spark.apache.org/docs/2.2.0/ml-collaborative-filtering.html) is trained using the [MovieLens](https://grouplens.org/datasets/movielens/) dataset."],"metadata":{}},{"cell_type":"code","source":["# top k items to recommend\nTOP_K = 10\n\n# Select Movielens data size: 100k, 1m, 10m, or 20m\nMOVIELENS_DATA_SIZE = '100k'"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["#### 2.1. Download the MovieLens dataset"],"metadata":{}},{"cell_type":"code","source":["# Note: The DataFrame-based API for ALS currently only supports integers for user and item ids.\nschema = StructType(\n    (\n        StructField(\"UserId\", IntegerType()),\n        StructField(\"MovieId\", IntegerType()),\n        StructField(\"Rating\", FloatType()),\n        StructField(\"Timestamp\", LongType()),\n    )\n)\n\ndata = movielens.load_spark_df(spark, size=MOVIELENS_DATA_SIZE, schema=schema, dbutils=dbutils)\ndata.show()"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["#### 2.2 Split the data into train, test\nThere are several ways of splitting the data: random, chronological, stratified, etc., each of which favors a different real-world evaluation use case. We will split randomly in this example – for more details on which splitter to choose, consult [this guide](https://github.com/Microsoft/Recommenders/blob/master/notebooks/01_data/data_split.ipynb)."],"metadata":{}},{"cell_type":"code","source":["train, test = spark_random_split(data, ratio=0.75, seed=123)\nprint (\"N train\", train.cache().count())\nprint (\"N test\", test.cache().count())"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["#### 2.3. Train the ALS model on the training data, and get the top-k recommendations for our testing data\nTo predict movie ratings, we use the rating data in the training set as users' explicit feedbacks. The hyper parameters used in building the model are referenced from [here](http://mymedialite.net/examples/datasets.html)."],"metadata":{}},{"cell_type":"code","source":["header = {\n    \"userCol\": \"UserId\",\n    \"itemCol\": \"MovieId\",\n    \"ratingCol\": \"Rating\",\n}\n\n\nals = ALS(\n    rank=10,\n    maxIter=15,\n    implicitPrefs=False,\n    alpha=0.1,\n    regParam=0.05,\n    coldStartStrategy='drop',\n    nonnegative=True,\n    **header\n)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["model = als.fit(train)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["In the movie recommendation use case, recommending movies that have been rated by the users do not make sense. Therefore, the rated movies are removed from the recommended items.\n\nIn order to achieve this, we recommend all movies to all users, and then remove the user-movie pairs that exist in the training datatset."],"metadata":{}},{"cell_type":"code","source":["# Get the cross join of all user-item pairs and score them.\nusers = train.select('UserId').distinct()\nitems = train.select('MovieId').distinct()\nuser_item = users.crossJoin(items)\ndfs_pred = model.transform(user_item)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["dfs_pred.show()"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["# Remove seen items.\ndfs_pred_exclude_train = dfs_pred.alias(\"pred\").join(\n    train.alias(\"train\"),\n    (dfs_pred['UserId'] == train['UserId']) & (dfs_pred['MovieId'] == train['MovieId']),\n    how='outer'\n)\n\ntop_all = dfs_pred_exclude_train.filter(dfs_pred_exclude_train[\"train.Rating\"].isNull()) \\\n    .select('pred.' + 'UserId', 'pred.' + 'MovieId', 'pred.' + \"prediction\")\n\ntop_all.show()"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["#### 2.4 Evaluate how well ALS performs\n\nEvaluate model performance using metrics such as Precision@K, Recall@K, [MAP](https://en.wikipedia.org/wiki/Evaluation_measures_\\(information_retrieval\\)) or [nDCG](https://en.wikipedia.org/wiki/Discounted_cumulative_gain). For a full guide on what metrics to evaluate your recommender with, consult [this guide](https://github.com/Microsoft/Recommenders/blob/master/notebooks/03_evaluate/evaluation.ipynb)."],"metadata":{}},{"cell_type":"code","source":["test.show()"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["rank_eval = SparkRankingEvaluation(test, top_all, k = TOP_K, col_user=\"UserId\", col_item=\"MovieId\", \n                                    col_rating=\"Rating\", col_prediction=\"prediction\", \n                                    relevancy_method=\"top_k\")"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["# Evaluate Ranking Metrics\n\nprint(\"Model:\\tALS\",\n      \"Top K:\\t%d\" % rank_eval.k,\n      \"MAP:\\t%f\" % rank_eval.map_at_k(),\n      \"NDCG:\\t%f\" % rank_eval.ndcg_at_k(),\n      \"Precision@K:\\t%f\" % rank_eval.precision_at_k(),\n      \"Recall@K:\\t%f\" % rank_eval.recall_at_k(), sep='\\n')"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["# Evaluate Rating Metrics\n\nprediction = model.transform(test)\nrating_eval = SparkRatingEvaluation(test, prediction, col_user=\"UserId\", col_item=\"MovieId\", \n                                    col_rating=\"Rating\", col_prediction=\"prediction\")\n\nprint(\"Model:\\tALS rating prediction\",\n      \"RMSE:\\t%.2f\" % rating_eval.rmse(),\n      \"MAE:\\t%f\" % rating_eval.mae(),\n      \"Explained variance:\\t%f\" % rating_eval.exp_var(),\n      \"R squared:\\t%f\" % rating_eval.rsquared(), sep='\\n')"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["#### 2.5 Save the model"],"metadata":{}},{"cell_type":"code","source":["model.write().overwrite().save(model_name)\nmodel_local = \"file:\" + os.getcwd() + \"/\" + model_name\ndbutils.fs.cp(model_name, model_local, True)"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["## 3. Operationalize the Recommender Service\nOnce the model is built with desirable performance, it will be operationalized to run as a REST endpoint to be utilized by a real time service. We will utilize [Azure Cosmos DB](https://azure.microsoft.com/en-us/services/cosmos-db/), [Azure Machine Learning Service](https://azure.microsoft.com/en-us/services/machine-learning-service/), and [Azure Kubernetes Service](https://docs.microsoft.com/en-us/azure/aks/intro-kubernetes) to operationalize the recommender service."],"metadata":{}},{"cell_type":"markdown","source":["#### 3.1 Create a look-up for Recommendations in Cosmos DB\n\nFirst, the Top-10 recommendations for each user as predicted by the model are stored as a lookup table in Cosmos DB. At runtime, the service will return the Top-10 recommendations as precomputed and stored in Cosmos DB:"],"metadata":{}},{"cell_type":"code","source":["with open('secrets.json') as json_data:\n    writeConfig = json.load(json_data)\n    recs = model.recommendForAllUsers(10)\n    recs.withColumn(\"id\",recs[userCol].cast(\"string\")).select(\"id\", \"recommendations.\"+ itemCol)\\\n    .write.format(\"com.microsoft.azure.cosmosdb.spark\").mode('overwrite').options(**writeConfig).save()"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["#### 3.2 Configure Azure Machine Learning\n\nNext, Azure Machine Learning Service is used to create a model scoring image and deploy it to Azure Kubernetes Service as a scalable containerized service. To achieve this, a **scoring script** and an **environment config** should be created. The following shows the content of the two files.  \n\nIn the scoring script, we make a call to Cosmos DB to lookup the top 10 movies to recommend given an input User ID:"],"metadata":{}},{"cell_type":"code","source":["#%%writefile score_sparkml.py\n\nscore_sparkml = \"\"\"\n\nimport json\ndef init(local=False):\n    global client, collection\n    try:\n      # Query them in SQL\n      import pydocumentdb.document_client as document_client\n\n      MASTER_KEY = '{key}'\n      HOST = '{endpoint}'\n      DATABASE_ID = \"{database}\"\n      COLLECTION_ID = \"{collection}\"\n      database_link = 'dbs/' + DATABASE_ID\n      collection_link = database_link + '/colls/' + COLLECTION_ID\n      \n      client = document_client.DocumentClient(HOST, {'masterKey': MASTER_KEY})\n      collection = client.ReadCollection(collection_link=collection_link)\n    except Exception as e:\n      collection = e\ndef run(input_json):      \n\n    try:\n      import json\n\n      id = json.loads(json.loads(input_json)[0])['id']\n      query = {'query': 'SELECT * FROM c WHERE c.id = \"' + str(id) +'\"' } #+ str(id)\n\n      options = {}\n\n      result_iterable = client.QueryDocuments(collection['_self'], query, options)\n      result = list(result_iterable);\n  \n    except Exception as e:\n        result = str(e)\n    return json.dumps(str(result)) #json.dumps({{\"result\":result}})\n\"\"\"\n\n\nwith open('secrets.json') as json_data:\n    writeConfig = json.load(json_data)\n    score_sparkml = score_sparkml.replace(\"{key}\",writeConfig['Masterkey']).replace(\"{endpoint}\",writeConfig['Endpoint']).replace(\"{database}\",writeConfig['Database']).replace(\"{collection}\",writeConfig['Collection'])\n\n    exec(score_sparkml)\n\n    with open(\"score_sparkml.py\", \"w\") as file:\n        file.write(score_sparkml)"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":["Next, create a environment config file with the dependencies needed:"],"metadata":{}},{"cell_type":"code","source":["%%writefile myenv_sparkml.yml\n\nname: myenv\nchannels:\n  - defaults\ndependencies:\n  - pip:\n    - numpy==1.14.2\n    - scikit-learn==0.19.1\n    - pandas\n    # Required packages for AzureML execution, history, and data preparation.\n    - --extra-index-url https://azuremlsdktestpypi.azureedge.net/sdk-release/Preview/E7501C02541B433786111FE8E140CAA1\n    - azureml-core\n    - pydocumentdb"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":["Register your model:"],"metadata":{}},{"cell_type":"code","source":["mymodel = Model.register(model_path = model_name, # this points to a local file\n                       model_name = model_name, # this is the name the model is registered as, am using same name for both path and name.                 \n                       description = \"ADB trained model\",\n                       workspace = ws)\n\nprint(mymodel.name, mymodel.description, mymodel.version)"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":["#### 3.3 Deploy the model as a Service on AKS"],"metadata":{}},{"cell_type":"code","source":["mymodel = Model.register(model_path = model_name, # this points to a local file\n                       model_name = model_name, # this is the name the model is registered as, am using same name for both path and name.                 \n                       description = \"ADB trained model\",\n                       workspace = ws)\n\nprint(mymodel.name, mymodel.description, mymodel.version)"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":["Create a container for your model service:"],"metadata":{}},{"cell_type":"code","source":["# Create Image for Web Service\nmodels = [mymodel]\nruntime = \"spark-py\"\nconda_file = 'myenv_sparkml.yml'\ndriver_file = \"score_sparkml.py\"\n\n# image creation\nfrom azureml.core.image import ContainerImage\nmyimage_config = ContainerImage.image_configuration(execution_script = driver_file, \n                                    runtime = runtime, \n                                    conda_file = conda_file)\n\nimage = ContainerImage.create(name = \"news-als\",\n                                # this is the model object\n                                models = [mymodel],\n                                image_config = myimage_config,\n                                workspace = ws)\n\n# Wait for the create process to complete\nimage.wait_for_creation(show_output = True)"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":["Create an AKS Cluster to run your container (this may take 20-25 minutes):"],"metadata":{}},{"cell_type":"code","source":["from azureml.core.compute import AksCompute, ComputeTarget\n\n# Use the default configuration (can also provide parameters to customize)\nprov_config = AksCompute.provisioning_configuration()\n\n# Create the cluster\naks_target = ComputeTarget.create(workspace = ws, \n                                  name = aks_name, \n                                  provisioning_configuration = prov_config)\n\naks_target.wait_for_completion(show_output = True)\n\nprint(aks_target.provisioning_state)\nprint(aks_target.provisioning_errors)"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":["Deploy the container image to AKS:"],"metadata":{}},{"cell_type":"code","source":["#Set the web service configuration (using default here with app insights)\naks_config = AksWebservice.deploy_configuration(enable_app_insights=True)\n\n# Webservice creation using single command, there is a variant to use image directly as well.\ntry:\n    aks_service = Webservice.deploy_from_image(\n      workspace=ws, \n      name=service_name,\n      deployment_config = aks_config,\n      image = image,\n      deployment_target = aks_target\n      )\n    aks_service.wait_for_deployment(show_output=True)\nexcept Exception:\n    aks_service = Webservice.list(ws)[0]"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"markdown","source":["####3.4 Call the AKS model service\nAfter the deployment, the service can be called with a user ID – the service will then look up the top 10 recommendations for that user in Cosmos DB and send back the results.\nThe following script demonstrates how to call the recommendation service API and view the result for the given user ID:"],"metadata":{}},{"cell_type":"code","source":["scoring_url = aks_service.scoring_uri\n\ninput_data = '[\"{\\\\\"id\\\\\":\\\\\"496\\\\\"}\"]'.encode()\n\nreq = urllib.request.Request(scoring_url,data=input_data)\nreq.add_header(\"Content-Type\",\"application/json\")\n\ntic = time.time()\nwith urllib.request.urlopen(req) as result:\n    res = result.readlines()\n    print(res)\n    \ntoc = time.time()\nt2 = toc - tic\nprint(\"Full run took %.2f seconds\" % (toc - tic))"],"metadata":{},"outputs":[],"execution_count":51}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.7","nbconvert_exporter":"python","file_extension":".py"},"name":"ALS_Movie_Example","notebookId":3793436040750096},"nbformat":4,"nbformat_minor":0}
