{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning (Spark based recommender)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning for Spark based recommender algorithm is important to select a model with the optimal performance. This notebook introduces good practices in performing hyperparameter tuning for building recommender models with the utility functions provided in the [Microsoft/Recommenders](https://github.com/Microsoft/Recommenders.git) repository.\n",
    "\n",
    "Three different approaches are introduced and comparatively studied.\n",
    "* Spark native constructs (`ParamGridBuilder`, `TrainValidationSplit`).\n",
    "* `hyperopt` package with Tree of Parzen Estimator algorithm. \n",
    "* Parallelized random search of parameter values sampled with pre-defined space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Global settings and import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.6.0 | packaged by conda-forge | (default, Feb  9 2017, 14:36:55) \n",
      "[GCC 4.8.2 20140120 (Red Hat 4.8.2-15)]\n",
      "Pandas version: 0.23.4\n",
      "PySpark version: 2.3.1\n"
     ]
    }
   ],
   "source": [
    "# set the environment path to find Recommenders\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll.base import scope\n",
    "from hyperopt.pyll.stochastic import sample\n",
    "\n",
    "from reco_utils.common.spark_utils import start_or_get_spark\n",
    "from reco_utils.evaluation.spark_evaluation import SparkRankingEvaluation, SparkRatingEvaluation\n",
    "from reco_utils.dataset.movielens import load_spark_df\n",
    "from reco_utils.dataset.spark_splitters import spark_random_split\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Pandas version: {}\".format(pd.__version__))\n",
    "print(\"PySpark version: {}\".format(pyspark.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYSPARK_PYTHON=/anaconda/envs/recommender/bin/python\n",
      "env: PYSPARK_DRIVER_PYTHON=/anaconda/envs/recommender/bin/python\n"
     ]
    }
   ],
   "source": [
    "%env PYSPARK_PYTHON=/anaconda/envs/recommender/bin/python\n",
    "%env PYSPARK_DRIVER_PYTHON=/anaconda/envs/recommender/bin/python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "COL_USER = \"UserId\"\n",
    "COL_ITEM = \"MovieId\"\n",
    "COL_TIMESTAMP = \"Timestamp\"\n",
    "COL_RATING = \"Rating\"\n",
    "COL_PREDICTION = \"prediction\"\n",
    "\n",
    "HEADER = {\n",
    "    \"col_user\": COL_USER,\n",
    "    \"col_item\": COL_ITEM,\n",
    "    \"col_rating\": COL_RATING,\n",
    "    \"col_prediction\": COL_PREDICTION,\n",
    "}\n",
    "\n",
    "HEADER_ALS = {\n",
    "    \"userCol\": COL_USER,\n",
    "    \"itemCol\": COL_ITEM,\n",
    "    \"ratingCol\": COL_RATING\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For illustration purpose, data used for demonstrating the hyperparameter tuning practices is the Movielens 100k dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"Hyperopt\")\n",
    "    .master('local[*]')\n",
    "    .config(\"spark.jars.packages\", \"Azure:mmlspark:0.15\")\n",
    "    .config(\"spark.driver.memory\", '16g')\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Movielens 100k dataset is used for running the demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_spark_df(spark, size='100k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is split into 3 subsets randomly with a given split ratio. The hyperparameter tuning is performed on the training and the validating data, and then the optimal recommender selected is evaluated on the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = spark_random_split(data, ratio=[3, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Hyper parameter tuning with Azure Machine Learning Services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `hyperdrive` module in the [Azure Machine Learning Services](https://azure.microsoft.com/en-us/services/machine-learning-service/) is supposed to run [hyperparameter tuning and optimizing for machine learning model selection](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters). At the moment, the service supports running hyperparameter tuning on heterogenous computing targets such as cluster of commodity compute nodes with or without GPU devices (see detailed documentation [here](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-set-up-training-targets)). It is feasible to run parameter tuning on a cluster of VM nodes. In this case, the service containerizes individual and independent Spark session on each node of the cluster to run the parameter tuning job in parallel, instead of inside a single Spark session where the training is executed in a distributed manner.  \n",
    "\n",
    "It is worth mentioning that tuning hyperparameter with `hyperdrive` in Azure Machine Learning services will be a feature in the future version. \n",
    "\n",
    "Detailed instructions of tuning hyperparameter of non-Spark workloads by using Azure Machine Learning Services can be found in [this](https://github.com/Microsoft/Recommenders.git) notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Hyper parameter tuning with Spark ML constructs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Spark native construct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark ML lib implements modules such as `CrossValidator` and `TrainValidationSplit` for tuning hyperparameters (see [here](https://spark.apache.org/docs/2.2.0/ml-tuning.html)). However, by default, it does not support custom machine learning algorithms, data splitting methods, and evaluation metrics, like what are offered as utility functions in the Recommenders repository. \n",
    "\n",
    "For example, the Spark native constuct can be used for tuning a recommender against the `rmse` metric which is one of the available regression metrics in Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, a Spark ALS object needs to be created. In this case, for illustration purpose, it is an ALS model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE the parameters of interest, rank and regParam, are left unset, \n",
    "# because their values will be assigned in the parameter grid builder.\n",
    "als = ALS(\n",
    "    maxIter=15,\n",
    "    implicitPrefs=False,\n",
    "    alpha=0.1,\n",
    "    coldStartStrategy='drop',\n",
    "    nonnegative=False,\n",
    "    **HEADER_ALS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, a parameter grid can be defined as follows. Without loss of generity, only `rank` and `regParam` are considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(als.rank, [10, 20]) \\\n",
    "    .addGrid(als.regParam, [ 0.1, 0.01, 0.001, 0.0001, 0.00001]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the settings above, a `TrainValidationSplit` constructor can be created for fitting the best model in the given parameter range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvs = TrainValidationSplit(\n",
    "    estimator=als,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    # A regression evaluation method is used. \n",
    "    evaluator=RegressionEvaluator(labelCol='Rating'),\n",
    "    # 80% of the data will be used for training, 20% for validation.\n",
    "    # NOTE here the splitting is random. The Spark splitting utilities (e.g. chrono splitter)\n",
    "    # are therefore not available here. \n",
    "    trainRatio=0.75\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "# Run TrainValidationSplit, and choose the best set of parameters.\n",
    "# NOTE train and valid is union because in Spark TrainValidationSplit does splitting by itself.\n",
    "model = tvs.fit(train.union(valid))\n",
    "\n",
    "time_spark = time.time() - time_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model parameters in the grid and the best metrics can be then returned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0:\n",
      "\tValidation Metric: 0.9421215576763626\n",
      "\tParam(parent='ALS_4a6d87a44ea816ff815b', name='rank', doc='rank of the factorization'): 10\n",
      "\tParam(parent='ALS_4a6d87a44ea816ff815b', name='regParam', doc='regularization parameter (>= 0).'): 0.1\n",
      "Run 1:\n",
      "\tValidation Metric: 1.1935485782392687\n",
      "\tParam(parent='ALS_4a6d87a44ea816ff815b', name='rank', doc='rank of the factorization'): 10\n",
      "\tParam(parent='ALS_4a6d87a44ea816ff815b', name='regParam', doc='regularization parameter (>= 0).'): 0.01\n",
      "Run 2:\n",
      "\tValidation Metric: 1.536557434836125\n",
      "\tParam(parent='ALS_4a6d87a44ea816ff815b', name='rank', doc='rank of the factorization'): 10\n",
      "\tParam(parent='ALS_4a6d87a44ea816ff815b', name='regParam', doc='regularization parameter (>= 0).'): 0.001\n",
      "Run 3:\n",
      "\tValidation Metric: 2.108594860151324\n",
      "\tParam(parent='ALS_4a6d87a44ea816ff815b', name='rank', doc='rank of the factorization'): 10\n",
      "\tParam(parent='ALS_4a6d87a44ea816ff815b', name='regParam', doc='regularization parameter (>= 0).'): 0.0001\n",
      "Run 4:\n",
      "\tValidation Metric: 2.6397158225049893\n",
      "\tParam(parent='ALS_4a6d87a44ea816ff815b', name='rank', doc='rank of the factorization'): 10\n",
      "\tParam(parent='ALS_4a6d87a44ea816ff815b', name='regParam', doc='regularization parameter (>= 0).'): 1e-05\n",
      "Run 5:\n",
      "\tValidation Metric: 0.9418574694424408\n",
      "\tParam(parent='ALS_4a6d87a44ea816ff815b', name='rank', doc='rank of the factorization'): 20\n",
      "\tParam(parent='ALS_4a6d87a44ea816ff815b', name='regParam', doc='regularization parameter (>= 0).'): 0.1\n",
      "Run 6:\n",
      "\tValidation Metric: 1.2875617383316302\n",
      "\tParam(parent='ALS_4a6d87a44ea816ff815b', name='rank', doc='rank of the factorization'): 20\n",
      "\tParam(parent='ALS_4a6d87a44ea816ff815b', name='regParam', doc='regularization parameter (>= 0).'): 0.01\n",
      "Run 7:\n",
      "\tValidation Metric: 1.6891997782841075\n",
      "\tParam(parent='ALS_4a6d87a44ea816ff815b', name='rank', doc='rank of the factorization'): 20\n",
      "\tParam(parent='ALS_4a6d87a44ea816ff815b', name='regParam', doc='regularization parameter (>= 0).'): 0.001\n",
      "Run 8:\n",
      "\tValidation Metric: 2.215253494308962\n",
      "\tParam(parent='ALS_4a6d87a44ea816ff815b', name='rank', doc='rank of the factorization'): 20\n",
      "\tParam(parent='ALS_4a6d87a44ea816ff815b', name='regParam', doc='regularization parameter (>= 0).'): 0.0001\n",
      "Run 9:\n",
      "\tValidation Metric: 3.1110708371552813\n",
      "\tParam(parent='ALS_4a6d87a44ea816ff815b', name='rank', doc='rank of the factorization'): 20\n",
      "\tParam(parent='ALS_4a6d87a44ea816ff815b', name='regParam', doc='regularization parameter (>= 0).'): 1e-05\n"
     ]
    }
   ],
   "source": [
    "for idx, item in enumerate(model.getEstimatorParamMaps()):\n",
    "    print('Run {}:'.format(idx))\n",
    "    print('\\tValidation Metric: {}'.format(model.validationMetrics[idx]))\n",
    "    for key, value in item.items():\n",
    "        print('\\t{0}: {1}'.format(repr(key), value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9421215576763626,\n",
       " 1.1935485782392687,\n",
       " 1.536557434836125,\n",
       " 2.108594860151324,\n",
       " 2.6397158225049893,\n",
       " 0.9418574694424408,\n",
       " 1.2875617383316302,\n",
       " 1.6891997782841075,\n",
       " 2.215253494308962,\n",
       " 3.1110708371552813]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.validationMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the best model, just do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_best_spark = model.bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Custom `Estimator`, `Transformer`, and `Evaluator` for Spark ALS\n",
    "\n",
    "One can also customize Spark modules to allow tuning hyperparameters for a desired model. For instance, the native Spark ALS does not allow tuning hyperparameters for ranking metrics such as precision@k, recall@k, etc. This can be done by creating custom `Estimator`, `Transformer` and `Evaluator`. The benefit is that, after the customization, the tuning process can make use of `trainValidSplit` directly, which distributes the tuning in a Spark session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customized `Estimator` and `Transformer` for top k recommender based on Spark ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ALSTopK(\n",
    "    ALS,\n",
    "    Estimator,\n",
    "    HasInputCol,\n",
    "    HasPredictionCol\n",
    "):    \n",
    "    rank = Param(Params._dummy(), \"rank\", \"rank of the factorization\",\n",
    "                 typeConverter=TypeConverters.toInt)\n",
    "    numUserBlocks = Param(Params._dummy(), \"numUserBlocks\", \"number of user blocks\",\n",
    "                          typeConverter=TypeConverters.toInt)\n",
    "    numItemBlocks = Param(Params._dummy(), \"numItemBlocks\", \"number of item blocks\",\n",
    "                          typeConverter=TypeConverters.toInt)\n",
    "    implicitPrefs = Param(Params._dummy(), \"implicitPrefs\", \"whether to use implicit preference\",\n",
    "                          typeConverter=TypeConverters.toBoolean)\n",
    "    alpha = Param(Params._dummy(), \"alpha\", \"alpha for implicit preference\",\n",
    "                  typeConverter=TypeConverters.toFloat)\n",
    "    userCol = Param(Params._dummy(), \"userCol\", \"column name for user ids. Ids must be within \" +\n",
    "                    \"the integer value range.\", typeConverter=TypeConverters.toString)\n",
    "    itemCol = Param(Params._dummy(), \"itemCol\", \"column name for item ids. Ids must be within \" +\n",
    "                    \"the integer value range.\", typeConverter=TypeConverters.toString)\n",
    "    ratingCol = Param(Params._dummy(), \"ratingCol\", \"column name for ratings\",\n",
    "                      typeConverter=TypeConverters.toString)\n",
    "    nonnegative = Param(Params._dummy(), \"nonnegative\",\n",
    "                        \"whether to use nonnegative constraint for least squares\",\n",
    "                        typeConverter=TypeConverters.toBoolean)\n",
    "    intermediateStorageLevel = Param(Params._dummy(), \"intermediateStorageLevel\",\n",
    "                                     \"StorageLevel for intermediate datasets. Cannot be 'NONE'.\",\n",
    "                                     typeConverter=TypeConverters.toString)\n",
    "    finalStorageLevel = Param(Params._dummy(), \"finalStorageLevel\",\n",
    "                              \"StorageLevel for ALS model factors.\",\n",
    "                              typeConverter=TypeConverters.toString)\n",
    "    coldStartStrategy = Param(Params._dummy(), \"coldStartStrategy\", \"strategy for dealing with \" +\n",
    "                              \"unknown or new users/items at prediction time. This may be useful \" +\n",
    "                              \"in cross-validation or production scenarios, for handling \" +\n",
    "                              \"user/item ids the model has not seen in the training data. \" +\n",
    "                              \"Supported values: 'nan', 'drop'.\",\n",
    "                              typeConverter=TypeConverters.toString)\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(\n",
    "        self,\n",
    "        rank=10, maxIter=10, regParam=0.1, numUserBlocks=10, numItemBlocks=10,\n",
    "        implicitPrefs=False, alpha=1.0, userCol=\"user\", itemCol=\"item\", seed=None, k=10,\n",
    "        ratingCol=\"rating\", nonnegative=False, checkpointInterval=10,\n",
    "        intermediateStorageLevel=\"MEMORY_AND_DISK\",\n",
    "        finalStorageLevel=\"MEMORY_AND_DISK\", coldStartStrategy=\"nan\"\n",
    "    ):\n",
    "        super(ALS, self).__init__()\n",
    "        self._java_obj = self._new_java_obj(\"org.apache.spark.ml.recommendation.ALS\", self.uid)\n",
    "        self._setDefault(rank=10, maxIter=10, regParam=0.1, numUserBlocks=10, numItemBlocks=10,\n",
    "                         implicitPrefs=False, alpha=1.0, userCol=\"user\", itemCol=\"item\",\n",
    "                         ratingCol=\"rating\", nonnegative=False, checkpointInterval=10,\n",
    "                         intermediateStorageLevel=\"MEMORY_AND_DISK\",\n",
    "                         finalStorageLevel=\"MEMORY_AND_DISK\", coldStartStrategy=\"nan\")\n",
    "\n",
    "        kwargs = self._input_kwargs \n",
    "        kwargs = {x: kwargs[x] for x in kwargs if x not in {'k'}}\n",
    "        self.setParams(**kwargs)\n",
    "        \n",
    "        # The manually added parameter is not present in ALS Java implementation. \n",
    "        self.k = k\n",
    "        \n",
    "    def setRank(self, value):\n",
    "        \"\"\"\n",
    "        Sets the value of :py:attr:`rank`.\n",
    "        \"\"\"\n",
    "        return self._set(rank=value)\n",
    "\n",
    "    def getRank(self):\n",
    "        \"\"\"\n",
    "        Gets the value of rank or its default value.\n",
    "        \"\"\"\n",
    "        return self.getOrDefault(self.rank)\n",
    "    \n",
    "    def setParams(self, rank=10, maxIter=10, regParam=0.1, numUserBlocks=10, numItemBlocks=10,\n",
    "                  implicitPrefs=False, alpha=1.0, userCol=\"user\", itemCol=\"item\", seed=None,\n",
    "                  ratingCol=\"rating\", nonnegative=False, checkpointInterval=10,\n",
    "                  intermediateStorageLevel=\"MEMORY_AND_DISK\",\n",
    "                  finalStorageLevel=\"MEMORY_AND_DISK\", coldStartStrategy=\"nan\"):\n",
    "        \"\"\"\n",
    "        setParams(self, rank=10, maxIter=10, regParam=0.1, numUserBlocks=10, numItemBlocks=10, \\\n",
    "                 implicitPrefs=False, alpha=1.0, userCol=\"user\", itemCol=\"item\", seed=None, \\\n",
    "                 ratingCol=\"rating\", nonnegative=False, checkpointInterval=10, \\\n",
    "                 intermediateStorageLevel=\"MEMORY_AND_DISK\", \\\n",
    "                 finalStorageLevel=\"MEMORY_AND_DISK\", coldStartStrategy=\"nan\")\n",
    "        Sets params for ALS.\n",
    "        \"\"\"\n",
    "        kwargs = self._input_kwargs\n",
    "        kwargs = {x: kwargs[x] for x in kwargs if x not in {'k'}}\n",
    "        return self._set(**kwargs)\n",
    "        \n",
    "    def _fit(self, dataset):\n",
    "        kwargs = self._input_kwargs    \n",
    "        # Exclude k as it is not a parameter for ALS.\n",
    "        kwargs = {x: kwargs[x] for x in kwargs if x not in {'k'}}\n",
    "        kwargs['rank'] = self.getRank()\n",
    "        kwargs['regParam'] = self.getOrDefault(self.regParam)\n",
    "        als = ALS(\n",
    "            **kwargs\n",
    "        )\n",
    "        als_model = als.fit(dataset)\n",
    "        \n",
    "        user_col = kwargs['userCol']\n",
    "        item_col = kwargs['itemCol']\n",
    "        \n",
    "        k = self.k\n",
    "                     \n",
    "        topk_model = ALSTopKModel()\n",
    "        topk_model.setParams(\n",
    "            als_model,\n",
    "            user_col, \n",
    "            item_col, \n",
    "            k\n",
    "        )\n",
    "        \n",
    "        return topk_model\n",
    "    \n",
    "    \n",
    "class ALSTopKModel(\n",
    "    Model,\n",
    "    HasInputCol,\n",
    "    HasPredictionCol,\n",
    "    HasLabelCol\n",
    "):    \n",
    "    def setParams(self, model, userCol, itemCol, k):\n",
    "        self.model = model\n",
    "        self.userCol = userCol\n",
    "        self.itemCol = itemCol\n",
    "        self.k = k\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        predictionCol = self.getPredictionCol()\n",
    "        labelCol = self.getLabelCol()\n",
    "        \n",
    "        users = dataset.select(self.userCol).distinct()\n",
    "        topk_recommendation = self.model.recommendForUserSubset(users, self.k)  \n",
    "        \n",
    "        extract_value = F.udf((lambda x: [y[0] for y in x]), ArrayType(IntegerType()))\n",
    "        topk_recommendation = topk_recommendation.withColumn(predictionCol, extract_value(F.col(\"recommendations\")))        \n",
    "        \n",
    "        dataset = (\n",
    "            dataset\n",
    "            .groupBy(self.userCol)\n",
    "            .agg(F.collect_list(F.col(self.itemCol)).alias(labelCol))\n",
    "        )\n",
    "            \n",
    "        topk_recommendation_all = dataset.join(\n",
    "            topk_recommendation, \n",
    "            on=self.userCol,\n",
    "            how=\"outer\"\n",
    "        )\n",
    "        \n",
    "        return topk_recommendation_all.select(self.userCol, labelCol, predictionCol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customized precision@k evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import Evaluator\n",
    "from pyspark.mllib.evaluation import RegressionMetrics, RankingMetrics\n",
    "\n",
    "# Define a custom Evaulator. Here precision@k is used.\n",
    "class PrecisionAtKEvaluator(Evaluator):\n",
    "\n",
    "    def __init__(self, predictionCol=\"prediction\", labelCol=\"label\", k=10):\n",
    "        self.predictionCol = predictionCol\n",
    "        self.labelCol = labelCol\n",
    "        self.k = k\n",
    "\n",
    "    def _evaluate(self, dataset):\n",
    "        \"\"\"\n",
    "        Returns a random number. \n",
    "        Implement here the true metric\n",
    "        \"\"\"      \n",
    "        # Drop Nulls.\n",
    "        dataset = dataset.na.drop()\n",
    "        metrics = RankingMetrics(dataset.select(self.predictionCol, self.labelCol).rdd)\n",
    "        return metrics.precisionAt(self.k)\n",
    "\n",
    "    def isLargerBetter(self):\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new ALS top-k recommender and find the optimal model with a train-valid splitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "alstopk = ALSTopK(\n",
    "    userCol=\"UserId\",\n",
    "    itemCol=\"MovieId\",\n",
    "    ratingCol=\"Rating\",\n",
    "    k=10\n",
    ")\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(alstopk.rank, [10, 20]) \\\n",
    "    .addGrid(alstopk.regParam, [0.1, 0.01, 0.001, 0.0001, 0.00001]) \\\n",
    "    .build()\n",
    "\n",
    "tvs = TrainValidationSplit(\n",
    "    estimator=alstopk,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    # A regression evaluation method is used. \n",
    "    evaluator=PrecisionAtKEvaluator(),\n",
    "    # 80% of the data will be used for training, 20% for validation.\n",
    "    # NOTE here the splitting is random. The Spark splitting utilities (e.g. chrono splitter)\n",
    "    # are therefore not available here. \n",
    "    trainRatio=0.75\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'userCol': 'UserId', 'itemCol': 'MovieId', 'ratingCol': 'Rating', 'rank': 10, 'regParam': 0.1}\n",
      "{'userCol': 'UserId', 'itemCol': 'MovieId', 'ratingCol': 'Rating', 'rank': 10, 'regParam': 0.01}\n",
      "{'userCol': 'UserId', 'itemCol': 'MovieId', 'ratingCol': 'Rating', 'rank': 10, 'regParam': 0.001}\n",
      "{'userCol': 'UserId', 'itemCol': 'MovieId', 'ratingCol': 'Rating', 'rank': 10, 'regParam': 0.0001}\n",
      "{'userCol': 'UserId', 'itemCol': 'MovieId', 'ratingCol': 'Rating', 'rank': 10, 'regParam': 1e-05}\n",
      "{'userCol': 'UserId', 'itemCol': 'MovieId', 'ratingCol': 'Rating', 'rank': 20, 'regParam': 0.1}\n",
      "{'userCol': 'UserId', 'itemCol': 'MovieId', 'ratingCol': 'Rating', 'rank': 20, 'regParam': 0.01}\n",
      "{'userCol': 'UserId', 'itemCol': 'MovieId', 'ratingCol': 'Rating', 'rank': 20, 'regParam': 0.001}\n",
      "{'userCol': 'UserId', 'itemCol': 'MovieId', 'ratingCol': 'Rating', 'rank': 20, 'regParam': 0.0001}\n",
      "{'userCol': 'UserId', 'itemCol': 'MovieId', 'ratingCol': 'Rating', 'rank': 20, 'regParam': 1e-05}\n",
      "{'userCol': 'UserId', 'itemCol': 'MovieId', 'ratingCol': 'Rating', 'rank': 20, 'regParam': 0.1}\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "# Run TrainValidationSplit, and choose the best set of parameters.\n",
    "# NOTE train and valid is union because in Spark TrainValidationSplit does splitting by itself.\n",
    "model = tvs.fit(train.union(valid))\n",
    "\n",
    "time_spark = time.time() - time_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98.03197407722473"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{Param(parent='ALSTopK_4fee9bc0af0538c7d0f3', name='rank', doc='rank of the factorization'): 10,\n",
       "  Param(parent='ALSTopK_4fee9bc0af0538c7d0f3', name='regParam', doc='regularization parameter (>= 0).'): 0.1},\n",
       " {Param(parent='ALSTopK_4fee9bc0af0538c7d0f3', name='rank', doc='rank of the factorization'): 10,\n",
       "  Param(parent='ALSTopK_4fee9bc0af0538c7d0f3', name='regParam', doc='regularization parameter (>= 0).'): 0.01},\n",
       " {Param(parent='ALSTopK_4fee9bc0af0538c7d0f3', name='rank', doc='rank of the factorization'): 10,\n",
       "  Param(parent='ALSTopK_4fee9bc0af0538c7d0f3', name='regParam', doc='regularization parameter (>= 0).'): 0.001},\n",
       " {Param(parent='ALSTopK_4fee9bc0af0538c7d0f3', name='rank', doc='rank of the factorization'): 10,\n",
       "  Param(parent='ALSTopK_4fee9bc0af0538c7d0f3', name='regParam', doc='regularization parameter (>= 0).'): 0.0001},\n",
       " {Param(parent='ALSTopK_4fee9bc0af0538c7d0f3', name='rank', doc='rank of the factorization'): 10,\n",
       "  Param(parent='ALSTopK_4fee9bc0af0538c7d0f3', name='regParam', doc='regularization parameter (>= 0).'): 1e-05},\n",
       " {Param(parent='ALSTopK_4fee9bc0af0538c7d0f3', name='rank', doc='rank of the factorization'): 20,\n",
       "  Param(parent='ALSTopK_4fee9bc0af0538c7d0f3', name='regParam', doc='regularization parameter (>= 0).'): 0.1},\n",
       " {Param(parent='ALSTopK_4fee9bc0af0538c7d0f3', name='rank', doc='rank of the factorization'): 20,\n",
       "  Param(parent='ALSTopK_4fee9bc0af0538c7d0f3', name='regParam', doc='regularization parameter (>= 0).'): 0.01},\n",
       " {Param(parent='ALSTopK_4fee9bc0af0538c7d0f3', name='rank', doc='rank of the factorization'): 20,\n",
       "  Param(parent='ALSTopK_4fee9bc0af0538c7d0f3', name='regParam', doc='regularization parameter (>= 0).'): 0.001},\n",
       " {Param(parent='ALSTopK_4fee9bc0af0538c7d0f3', name='rank', doc='rank of the factorization'): 20,\n",
       "  Param(parent='ALSTopK_4fee9bc0af0538c7d0f3', name='regParam', doc='regularization parameter (>= 0).'): 0.0001},\n",
       " {Param(parent='ALSTopK_4fee9bc0af0538c7d0f3', name='rank', doc='rank of the factorization'): 20,\n",
       "  Param(parent='ALSTopK_4fee9bc0af0538c7d0f3', name='regParam', doc='regularization parameter (>= 0).'): 1e-05}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.getEstimatorParamMaps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_param(model, is_larger_better=True):\n",
    "    if is_larger_better:\n",
    "        best_metric = max(model.validationMetrics)\n",
    "    else:\n",
    "        best_metric = min(model.validationMetrics)\n",
    "        \n",
    "    parameters = model.getEstimatorParamMaps()[model.validationMetrics.index(best_metric)]\n",
    "     \n",
    "    return list(parameters.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = best_param(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+\n",
      "|UserId|               label|          prediction|\n",
      "+------+--------------------+--------------------+\n",
      "|   148|[7, 8, 78, 127, 1...|[408, 173, 169, 5...|\n",
      "|   463|[1, 7, 16, 19, 10...|[613, 116, 19, 88...|\n",
      "|   471|[50, 99, 404, 420...|[1242, 102, 477, ...|\n",
      "|   496|[28, 50, 53, 133,...|[320, 1639, 114, ...|\n",
      "|   833|[23, 26, 72, 92, ...|[1597, 1187, 1070...|\n",
      "|   243|[13, 22, 83, 194,...|[1642, 1398, 1449...|\n",
      "|   392|[98, 99, 174, 189...|[1463, 408, 483, ...|\n",
      "|   540|[7, 20, 25, 257, ...|[1449, 1019, 745,...|\n",
      "|   623|[163, 211, 234, 2...|[483, 496, 318, 5...|\n",
      "|   737|[89, 169, 174, 18...|[1405, 119, 1103,...|\n",
      "|   858|     [323, 334, 515]|[276, 1642, 1193,...|\n",
      "|   897|[11, 23, 66, 68, ...|[1643, 1169, 64, ...|\n",
      "|    31|[124, 299, 319, 3...|[1463, 175, 1643,...|\n",
      "|   516|[194, 204, 214, 431]|[1062, 1137, 1642...|\n",
      "|    85|[10, 13, 14, 30, ...|[1463, 408, 1398,...|\n",
      "|   137|[183, 210, 257, 3...|[1019, 1169, 1643...|\n",
      "|   251|[24, 60, 79, 117,...|[127, 357, 318, 6...|\n",
      "|   451|[262, 268, 269, 2...|[313, 1019, 253, ...|\n",
      "|   580|[15, 100, 121, 12...|[1137, 267, 1639,...|\n",
      "|   808|     [262, 312, 872]|[408, 50, 613, 11...|\n",
      "+------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.bestModel.transform(valid).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0:\n",
      "\tValidation Metric: 0.030286928799149855\n",
      "\tParam(parent='ALSTopK_4fee9bc0af0538c7d0f3', name='rank', doc='rank of the factorization'): 10\n",
      "\tParam(parent='ALSTopK_4fee9bc0af0538c7d0f3', name='regParam', doc='regularization parameter (>= 0).'): 0.1\n",
      "Run 1:\n",
      "\tValidation Metric: 0.011477151965993623\n",
      "\tParam(parent='ALSTopK_4fee9bc0af0538c7d0f3', name='rank', doc='rank of the factorization'): 10\n",
      "\tParam(parent='ALSTopK_4fee9bc0af0538c7d0f3', name='regParam', doc='regularization parameter (>= 0).'): 0.01\n",
      "Run 2:\n",
      "\tValidation Metric: 0.00563230605738576\n",
      "\tParam(parent='ALSTopK_4fee9bc0af0538c7d0f3', name='rank', doc='rank of the factorization'): 10\n",
      "\tParam(parent='ALSTopK_4fee9bc0af0538c7d0f3', name='regParam', doc='regularization parameter (>= 0).'): 0.001\n",
      "Run 3:\n",
      "\tValidation Metric: 0.004994686503719443\n",
      "\tParam(parent='ALSTopK_4fee9bc0af0538c7d0f3', name='rank', doc='rank of the factorization'): 10\n",
      "\tParam(parent='ALSTopK_4fee9bc0af0538c7d0f3', name='regParam', doc='regularization parameter (>= 0).'): 0.0001\n",
      "Run 4:\n",
      "\tValidation Metric: 0.005313496280552603\n",
      "\tParam(parent='ALSTopK_4fee9bc0af0538c7d0f3', name='rank', doc='rank of the factorization'): 10\n",
      "\tParam(parent='ALSTopK_4fee9bc0af0538c7d0f3', name='regParam', doc='regularization parameter (>= 0).'): 1e-05\n",
      "Run 5:\n",
      "\tValidation Metric: 0.04027630180658873\n",
      "\tParam(parent='ALSTopK_4fee9bc0af0538c7d0f3', name='rank', doc='rank of the factorization'): 20\n",
      "\tParam(parent='ALSTopK_4fee9bc0af0538c7d0f3', name='regParam', doc='regularization parameter (>= 0).'): 0.1\n",
      "Run 6:\n",
      "\tValidation Metric: 0.02890541976620616\n",
      "\tParam(parent='ALSTopK_4fee9bc0af0538c7d0f3', name='rank', doc='rank of the factorization'): 20\n",
      "\tParam(parent='ALSTopK_4fee9bc0af0538c7d0f3', name='regParam', doc='regularization parameter (>= 0).'): 0.01\n",
      "Run 7:\n",
      "\tValidation Metric: 0.01785334750265674\n",
      "\tParam(parent='ALSTopK_4fee9bc0af0538c7d0f3', name='rank', doc='rank of the factorization'): 20\n",
      "\tParam(parent='ALSTopK_4fee9bc0af0538c7d0f3', name='regParam', doc='regularization parameter (>= 0).'): 0.001\n",
      "Run 8:\n",
      "\tValidation Metric: 0.010945802337938364\n",
      "\tParam(parent='ALSTopK_4fee9bc0af0538c7d0f3', name='rank', doc='rank of the factorization'): 20\n",
      "\tParam(parent='ALSTopK_4fee9bc0af0538c7d0f3', name='regParam', doc='regularization parameter (>= 0).'): 0.0001\n",
      "Run 9:\n",
      "\tValidation Metric: 0.00924548352816153\n",
      "\tParam(parent='ALSTopK_4fee9bc0af0538c7d0f3', name='rank', doc='rank of the factorization'): 20\n",
      "\tParam(parent='ALSTopK_4fee9bc0af0538c7d0f3', name='regParam', doc='regularization parameter (>= 0).'): 1e-05\n"
     ]
    }
   ],
   "source": [
    "for idx, item in enumerate(model.getEstimatorParamMaps()):\n",
    "    print('Run {}:'.format(idx))\n",
    "    print('\\tValidation Metric: {}'.format(model.validationMetrics[idx]))\n",
    "    for key, value in item.items():\n",
    "        print('\\t{0}: {1}'.format(repr(key), value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 `mmlspark`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmlspark import HyperparamBuilder\n",
    "from mmlspark import RangeHyperParam\n",
    "from mmlspark import DiscreteHyperParam\n",
    "from mmlspark import RandomSpace\n",
    "\n",
    "alstopk = ALSTopK(\n",
    "    userCol=\"UserId\",\n",
    "    itemCol=\"MovieId\",\n",
    "    ratingCol=\"Rating\",\n",
    "    k=10\n",
    ")\n",
    "\n",
    "paramBuilder = (\n",
    "    HyperparamBuilder()\n",
    "    .addHyperparam(alstopk, alstopk.rank, DiscreteHyperParam([10, 15]))\n",
    "    .addHyperparam(alstopk, alstopk.regParam, RangeHyperParam(0.1, 0.3, isDouble=True))\n",
    ")\n",
    "\n",
    "paramSpace = paramBuilder.build()\n",
    "randomSpace = RandomSpace(paramBuilder.build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'RandomSpace' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-477d1089ef79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda/envs/recommender/lib/python3.6/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/anaconda/envs/recommender/lib/python3.6/site-packages/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0mest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrDefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0mepm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrDefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimatorParamMaps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m         \u001b[0mnumModels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0meva\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrDefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0mtRatio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrDefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'RandomSpace' has no len()"
     ]
    }
   ],
   "source": [
    "tvs = TrainValidationSplit(\n",
    "    estimator=alstopk,\n",
    "    estimatorParamMaps=randomSpace,\n",
    "    # A regression evaluation method is used. \n",
    "    evaluator=PrecisionAtKEvaluator(),\n",
    "    # 80% of the data will be used for training, 20% for validation.\n",
    "    # NOTE here the splitting is random. The Spark splitting utilities (e.g. chrono splitter)\n",
    "    # are therefore not available here. \n",
    "    trainRatio=0.75\n",
    ")\n",
    "\n",
    "tvs.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2206.fit.\n: java.lang.ClassCastException: org.apache.spark.ml.recommendation.ALS cannot be cast to [Lorg.apache.spark.ml.Estimator;\n\tat com.microsoft.ml.spark.TuneHyperparameters.getModels(TuneHyperparameters.scala:43)\n\tat com.microsoft.ml.spark.TuneHyperparameters.fit(TuneHyperparameters.scala:119)\n\tat com.microsoft.ml.spark.TuneHyperparameters.fit(TuneHyperparameters.scala:33)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-760367503698>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mparamSpace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandomSpace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m ).fit(train)\n\u001b[0m",
      "\u001b[0;32m/anaconda/envs/recommender/lib/python3.6/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/anaconda/envs/recommender/lib/python3.6/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/recommender/lib/python3.6/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \"\"\"\n\u001b[1;32m    284\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/recommender/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/recommender/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/recommender/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2206.fit.\n: java.lang.ClassCastException: org.apache.spark.ml.recommendation.ALS cannot be cast to [Lorg.apache.spark.ml.Estimator;\n\tat com.microsoft.ml.spark.TuneHyperparameters.getModels(TuneHyperparameters.scala:43)\n\tat com.microsoft.ml.spark.TuneHyperparameters.fit(TuneHyperparameters.scala:119)\n\tat com.microsoft.ml.spark.TuneHyperparameters.fit(TuneHyperparameters.scala:33)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "from mmlspark import TuneHyperparameters\n",
    "\n",
    "bestModel = TuneHyperparameters(\n",
    "    evaluationMetric=\"accuracy\", \n",
    "    models=alstopk, \n",
    "    numFolds=2,\n",
    "    numRuns=1, \n",
    "    parallelism=1,\n",
    "    paramSpace=randomSpace.space(), \n",
    "    seed=0\n",
    ").fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Hyperparameter tuning with `hyperopt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`hyperopt` is an open source Python package that is designed for tuning parameters for generic function with any pre-defined loss. More information about `hyperopt` can be found [here](https://github.com/hyperopt/hyperopt). `hyperopt` supports parallelization on MongoDB but not Spark. In our case, the tuning is performed in a sequential mode on a local computer.\n",
    "\n",
    "In `hyperopt`, an *objective* function is defined for optimizing the hyper parameters. In this case, the objective is similar to that in the Spark native construct situation, which is *to the RMSE metric for an ALS recommender*. Parameters of `rank` and `regParam` are used as hyperparameters. \n",
    "\n",
    "The objective function shown below demonstrates a RMSE loss for an ALS recommender. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize an objective function\n",
    "def objective(params):\n",
    "    rank = params['rank']\n",
    "    reg = params['reg']\n",
    "    train = params['train'] \n",
    "    valid = params['valid'] \n",
    "    col_user = params['col_user'] \n",
    "    col_item = params['col_item']\n",
    "    col_rating = params['col_rating'] \n",
    "    col_prediction = params['col_prediction'] \n",
    "    k = params['k']\n",
    "    relevancy_method = params['relevancy_method']\n",
    "    \n",
    "    als = ALS(\n",
    "        rank=rank,\n",
    "        maxIter=15,\n",
    "        implicitPrefs=False,\n",
    "        alpha=0.1,\n",
    "        regParam=reg,\n",
    "        coldStartStrategy='drop',\n",
    "        nonnegative=False,\n",
    "        seed=0,\n",
    "        **HEADER_ALS\n",
    "    )\n",
    "    \n",
    "    model = als.fit(train) \n",
    "    prediction = model.transform(valid)\n",
    "\n",
    "    rating_eval = SparkRatingEvaluation(\n",
    "        valid, \n",
    "        prediction, \n",
    "        **HEADER\n",
    "    )\n",
    "    \n",
    "    rmse = rating_eval.rmse()\n",
    "    \n",
    "    # Return the objective function result.\n",
    "    return {\n",
    "        'loss': rmse,\n",
    "        'status': STATUS_OK,\n",
    "        'eval_time': time.time()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A search space is usually defined for hyperparameter exploration. Design of search space is empirical, and depends on the understanding of how distribution of parameter of interest affects the model performance measured by the loss function. \n",
    "\n",
    "In the ALS algorithm, the two hyper parameters, rank and reg, affect model performance in a way that\n",
    "* The higher the rank, the better the model performance but also the higher risk of overfitting.\n",
    "* The reg parameter is subject to rank and it prevents overfitting in certain way. \n",
    "\n",
    "Therefore, in this case, a uniform distribution and a lognormal distribution sampling spaces are used for rank and reg, respectively. A narrow search space is used for illustration purpose, that is, the range of rank is from 10 to 20, while that of reg is from $e^{-5}$ to $e^{-1}$. Together with the randomly sampled hyper parameters, other parameters use for building / evaluating the recommender, like `k`, column names, data, etc., are kept as constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a search space\n",
    "space = {\n",
    "    'rank': hp.quniform('rank', 10, 20, 5),\n",
    "    'reg': hp.loguniform('reg', -5, -1),\n",
    "    'train': train, \n",
    "    'valid': valid, \n",
    "    'col_user': \"UserId\", \n",
    "    'col_item': \"MovieId\", \n",
    "    'col_rating': \"Rating\", \n",
    "    'col_prediction': \"prediction\", \n",
    "    'k': 10,\n",
    "    'relevancy_method': \"top_k\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fmin` of `hyperopt` is used for running the trials for searching optimal hyper parameters. In `hyperopt`, there are different strategies for intelligently optimize hyper parameters. For example, `hyperopt` avails [Tree of Parzen Estimators (TPE) method](https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf) for searching optimal parameters, which is more efficient than random search or grid search (details can be found [here](http://hyperopt.github.io/hyperopt/)). It is claimed in the project website that Bayesian based optimization method is planned but so far it has not yet been implemented. \n",
    "\n",
    "The following runs the trials with the pre-defined objective function and search space. TPE is used as the optimization method. Totally there will be 10 evaluations run for searching the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "# Trials for recording each iteration of the hyperparameter searching.\n",
    "trials = Trials()\n",
    "\n",
    "best = fmin(\n",
    "    fn=objective,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    trials=trials,\n",
    "    max_evals=10\n",
    ")\n",
    "                  \n",
    "time_hyperopt = time.time() - time_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials.best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = ['rank', 'reg']\n",
    "cols = len(parameters)\n",
    "f, axes = plt.subplots(nrows=1, ncols=cols, figsize=(15,5))\n",
    "cmap = plt.cm.jet\n",
    "for i, val in enumerate(parameters):\n",
    "    xs = np.array([t['misc']['vals'][val] for t in trials.trials]).ravel()\n",
    "    ys = [t['result']['loss'] for t in trials.trials]\n",
    "    xs, ys = zip(*sorted(zip(xs, ys)))\n",
    "    ys = np.array(ys)\n",
    "    axes[i].scatter(xs, ys, s=20, linewidth=0.01, alpha=0.75, c=cmap(float(i)/len(parameters)))\n",
    "    axes[i].set_title(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen from the above plot that\n",
    "* The actual impact of rank is in line with the intuition - the larger the value the better the result.\n",
    "* It is interesting to see that the optimal value of reg is ~0.1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "als = ALS(\n",
    "    rank=best[\"rank\"],\n",
    "    regParam=best[\"reg\"],\n",
    "    maxIter=15,\n",
    "    implicitPrefs=False,\n",
    "    alpha=0.1,\n",
    "    coldStartStrategy='drop',\n",
    "    nonnegative=False,\n",
    "    seed=0,\n",
    "    **HEADER_ALS\n",
    ")\n",
    "    \n",
    "model_best_hyperopt = als.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning prameters against other metrics can be simply done by modifying the `objective` function. The following shows an objective function of how to tune \"precision@k\". Since `fmin` in `hyperopt` only supports minimization while the actual objective of the loss is to maximize \"precision@k\", `-precision` instead of `precision` is used in the returned value of the `objective` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize an objective function\n",
    "def objective(params):\n",
    "    rank = params['rank']\n",
    "    reg = params['reg']\n",
    "    train = params['train'] \n",
    "    valid = params['valid'] \n",
    "    col_user = params['col_user'] \n",
    "    col_item = params['col_item']\n",
    "    col_rating = params['col_rating'] \n",
    "    col_prediction = params['col_prediction'] \n",
    "    k = params['k']\n",
    "    relevancy_method = params['relevancy_method']\n",
    "    \n",
    "    header = {\n",
    "        \"userCol\": col_user,\n",
    "        \"itemCol\": col_item,\n",
    "        \"ratingCol\": col_rating,\n",
    "    }\n",
    "    \n",
    "    als = ALS(\n",
    "        rank=rank,\n",
    "        maxIter=15,\n",
    "        implicitPrefs=False,\n",
    "        alpha=0.1,\n",
    "        regParam=reg,\n",
    "        coldStartStrategy='drop',\n",
    "        nonnegative=False,\n",
    "        seed=0,\n",
    "        **header\n",
    "    )\n",
    "    \n",
    "    model = als.fit(train)\n",
    "    \n",
    "    users = train.select(col_user).distinct()\n",
    "    items = train.select(col_item).distinct()\n",
    "    user_item = users.crossJoin(items)\n",
    "    dfs_pred = model.transform(user_item)\n",
    "\n",
    "    # Remove seen items.\n",
    "    dfs_pred_exclude_train = dfs_pred.alias(\"pred\").join(\n",
    "        train.alias(\"train\"),\n",
    "        (dfs_pred[col_user] == train[col_user]) & (dfs_pred[col_item] == train[col_item]),\n",
    "        how='outer'\n",
    "    )\n",
    "\n",
    "    top_all = dfs_pred_exclude_train.filter(dfs_pred_exclude_train[\"train.Rating\"].isNull()) \\\n",
    "        .select('pred.' + col_user, 'pred.' + col_item, 'pred.' + \"prediction\")\n",
    "    \n",
    "    top_all.cache().count()\n",
    "\n",
    "    rank_eval = SparkRankingEvaluation(\n",
    "        valid, \n",
    "        top_all, \n",
    "        k=k, \n",
    "        col_user=col_user, \n",
    "        col_item=col_item, \n",
    "        col_rating=\"Rating\", \n",
    "        col_prediction=\"prediction\", \n",
    "        relevancy_method=relevancy_method\n",
    "    )\n",
    "    \n",
    "    precision = rank_eval.precision_at_k()\n",
    "    \n",
    "    # Return the objective function result.\n",
    "    return {\n",
    "        'loss': -precision,\n",
    "        'status': STATUS_OK,\n",
    "        'eval_time': time.time()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Hyperparameter tuning with `hyperopt` sampling methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though `hyperopt` works well in a single node machine, its features (e.g., `Trials` module) do not support Spark environment. A good practice is to use `hyperopt` for sampling parameter values from the defined sampling space, and then parallelize the model training onto Spark cluster with the sampled parameter combinations.\n",
    "\n",
    "The downside of this method is that the intelligent searching algorithm (i.e., TPE) of `hyperopt` cannot be used. The approach introduced here is therefore equivalent to random search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample the parameters used for model building from the pre-defined space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperopt.pyll.stochastic\n",
    "\n",
    "time_start = time.time()\n",
    "\n",
    "sample_params = [hyperopt.pyll.stochastic.sample(space) for x in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the master node of the Spark cluster has multiple cores, a local parallelization with Python `map` function can be used for running model building in parallel. Note here the Spark `flatMap` function cannot be used, because parallelizing Spark sessions in a Spark session is forbidden.\n",
    "\n",
    "The following runs model building on the sampled parameter values with the pre-defined objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_map = list(map(lambda x: objective(x), sample_params))\n",
    "\n",
    "time_sample = time.time() - time_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_metrics = np.array([x['loss'] for x in results_map])\n",
    "best_loss = np.where(loss_metrics == min(loss_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_param = sample_params[best_loss[0].item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "als = ALS(\n",
    "    rank=best_param[\"rank\"],\n",
    "    regParam=best_param[\"reg\"],\n",
    "    maxIter=15,\n",
    "    implicitPrefs=False,\n",
    "    alpha=0.1,\n",
    "    coldStartStrategy='drop',\n",
    "    nonnegative=False,\n",
    "    seed=0,\n",
    "    **HEADER_ALS\n",
    ")\n",
    "    \n",
    "model_best_sample = als.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Evaluation on testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal parameters can then be used for building a recommender, which is then evaluated on the testing data.\n",
    "\n",
    "To compare the aforementioned three approaches, the evaluation metrics (loss) and the elapsed time to run each of them are recorded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prediction results with the optimal modesl from different approaches.\n",
    "prediction_spark = model_best_spark.transform(test)\n",
    "prediction_hyperopt = model_best_hyperopt.transform(test)\n",
    "prediction_sample = model_best_sample.transform(test)\n",
    "\n",
    "predictions = [prediction_spark, prediction_hyperopt, prediction_sample]\n",
    "elapsed = [time_spark, time_hyperopt, time_sample]\n",
    "\n",
    "approaches = ['spark', 'hyperopt', 'sample']\n",
    "comparison = pd.DataFrame()\n",
    "for ind, approach in enumerate(approaches):    \n",
    "    rating_eval = SparkRatingEvaluation(\n",
    "        test, \n",
    "        predictions[ind],\n",
    "        **HEADER\n",
    "    )\n",
    "    \n",
    "    result = pd.DataFrame({\n",
    "        'Approach': approach,\n",
    "        'RMSE': rating_eval.rmse(),\n",
    "        'MAE': rating_eval.mae(),\n",
    "        'Explained variance': rating_eval.exp_var(),\n",
    "        'R squared': rating_eval.rsquared(),\n",
    "        'Elapsed': elapsed[ind]\n",
    "    }, index=[0])\n",
    "    \n",
    "    comparison = comparison.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results, it can be seen that, *with the same number of iterations*, Spark native construct based approach takes the least amount of time, and not suprisingly, sample based approach takes the most amount of time. Intuitively this is because Spark native constructs leverage the underlying Java codes for running the actual analytics with high performance efficiency. There is a tradeoff in choosing between `hyperopt` TPE based approach and sampling approach in a for-loop. The former uses TPE for searching optimal parameters intelligently but runs the tuning iterations sequentially, while the latter simple samples parameter values from the pre-defined space but parallelizes the runs with Python `map` function. Run-time performance of the two approaches therefore depends on acutal use cases.  \n",
    "\n",
    "The three approaches use the same RMSE loss. In this measure, the native Spark construct performs the best. The `hyperopt` based approach performs the second best. This may be owing to the limited number of iterations used in the TPE searching algorithm. Note the differences in the RMSE metrics may also come from the randomness of the intermediate steps in parameter tuning process. In practice, multiple runs are required for generating statistically robust comparison results. We have tried 5 times for running the same comparison codes above. The results aligned well with each other in terms of objective metric values and elapsed time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, there are mainly three different approaches for running hyperparameter tuning for Spark based recommendation algorithm. The three different approaches are compared as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Approach|Distributed (on Spark)|Param sampling|Advanced hyperparam searching algo|Custom evaluation metrics|Custom data split|\n",
    "|---------|-------------|--------------|--------------------------|--------------|------------|\n",
    "|AML Services|Parallelizing Spark sessions on multi-node cluster or single Spark session on one VM node.)|Random, Grid, Bayesian sampling for discrete and continuous variables.|Bandit policy, Median stopping policy, and truncation selection policy.|Yes|Yes|\n",
    "|Spark native construct|Distributed in single-node standalone Spark environment or multi-node Spark cluster.|No|No|Need to re-engineer Spark modules|Need to re-engineer Spark modules.|\n",
    "|`hyperopt`|No (only support parallelization on MongoDB)|Random sampling for discrete and continuous variables.|Tree Parzen Estimator|Yes|Yes|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "* Azure Machine Learning Services, url: https://azure.microsoft.com/en-us/services/machine-learning-service/\n",
    "* Lisa Li, *et al*, Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization, The Journal of Machine Learning Research, Volume 18 Issue 1, pp 6765-6816, January 2017.\n",
    "* James Bergstrat *et al*, Algorithms for Hyper-Parameter Optimization, Procs 25th NIPS 2011. \n",
    "* `hyperopt`, url: http://hyperopt.github.io/hyperopt/.\n",
    "* Bergstra, J., Yamins, D., Cox, D. D. (2013) Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures. Proc. of the 30th International Conference on Machine Learning (ICML 2013).\n",
    "* Kris Wright, \"Hyper parameter tuning with hyperopt\", url:https://districtdatalabs.silvrback.com/parameter-tuning-with-hyperopt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (recommender)",
   "language": "python",
   "name": "recommender"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
