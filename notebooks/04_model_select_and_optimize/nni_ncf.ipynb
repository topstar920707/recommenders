{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.<br>\n",
    "Licensed under the MIT License.</i>\n",
    "<br>\n",
    "# Model Comparison between SVD and NCF Using the Neural Network Intelligence Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to use the **[Neural Network Intelligence](https://nni.readthedocs.io/en/latest/) toolkit (NNI)** for tuning hyperparameters for the Neural Collaborative Filtering Model and Surprise SVD model.\n",
    "\n",
    "To learn about each tuner NNI offers you can read about it [here](https://nni.readthedocs.io/en/latest/Tuner/BuiltinTuner.html). To see how each tuner performs on the Surprise SVD model, visit [this notebook instead](./nni_surprise_svd.ipynb). \n",
    "\n",
    "NNI is a toolkit to help users design and tune machine learning models (e.g., hyperparameters), neural network architectures, or complex system’s parameters, in an efficient and automatic way. NNI has several appealing properties: ease of use, scalability, flexibility and efficiency. . NNI can be executed in a distributed way on a local machine, a remote server, or a large scale training platform such as OpenPAI or Kubernetes. \n",
    "\n",
    "In this notebook, we can see how NNI works with two different model types and the differences between their hyperparameter search spaces, yaml config file, and training scripts.\n",
    "\n",
    "- [Surprise SVD Training Script](../../reco_utils/nni/svd_training.py)\n",
    "- [NCF Training Script](../../reco_utils/nni/ncf_training.py)\n",
    "\n",
    "In all experiments, we maximize precision@10. \n",
    "\n",
    "For this notebook we use a _local machine_ as the training platform (this can be any machine running the `reco_base` conda environment). In this case, NNI uses the available processors of the machine to parallelize the trials, subject to the value of `trialConcurrency` we specify in the configuration. Our runs and the results we report were obtained on a [Standard_D16_v3 virtual machine](https://docs.microsoft.com/en-us/azure/virtual-machines/windows/sizes-general#dv3-series-1) with 16 vcpus and 64 GB memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Global Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/recommenders_gpu/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/anaconda/envs/recommenders_gpu/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/anaconda/envs/recommenders_gpu/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/anaconda/envs/recommenders_gpu/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/anaconda/envs/recommenders_gpu/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/anaconda/envs/recommenders_gpu/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.6.10 |Anaconda, Inc.| (default, Mar 25 2020, 23:51:54) \n",
      "[GCC 7.3.0]\n",
      "Surprise version: 1.1.0\n",
      "NNI version: 1.5\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import json\n",
    "import os\n",
    "import surprise\n",
    "import papermill as pm\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import subprocess\n",
    "import tensorflow as tf\n",
    "import yaml\n",
    "import pkg_resources\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "import reco_utils\n",
    "from reco_utils.common.timer import Timer\n",
    "from reco_utils.dataset import movielens\n",
    "from reco_utils.dataset.python_splitters import python_chrono_split\n",
    "from reco_utils.evaluation.python_evaluation import rmse, precision_at_k, ndcg_at_k\n",
    "from reco_utils.tuning.nni.nni_utils import (check_experiment_status, check_stopped, check_metrics_written, get_trials,\n",
    "                                      stop_nni, start_nni)\n",
    "from reco_utils.recommender.ncf.dataset import Dataset as NCFDataset\n",
    "from reco_utils.recommender.ncf.ncf_singlenode import NCF\n",
    "from reco_utils.recommender.surprise.surprise_utils import predict, compute_ranking_predictions\n",
    "# from reco_utils.evaluation.python_evaluation import (rmse, mae, rsquared, exp_var, map_at_k, ndcg_at_k, precision_at_k, \n",
    "#                                                      recall_at_k, get_top_k_items)\n",
    "from reco_utils.common.constants import SEED as DEFAULT_SEED\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Surprise version: {}\".format(surprise.__version__))\n",
    "print(\"NNI version: {}\".format(pkg_resources.get_distribution(\"nni\").version))\n",
    "\n",
    "tmp_dir = TemporaryDirectory()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prepare Dataset\n",
    "1. Download data and split into training, validation and test sets\n",
    "2. Store the data sets to a local directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters used by papermill\n",
    "# Select Movielens data size: 100k, 1m\n",
    "MOVIELENS_DATA_SIZE = '100k'\n",
    "SURPRISE_READER = 'ml-100k'\n",
    "TMP_DIR = tmp_dir.name\n",
    "NUM_EPOCHS = 30\n",
    "MAX_TRIAL_NUM = 30\n",
    "# time (in seconds) to wait for each tuning experiment to complete\n",
    "WAITING_TIME = 20\n",
    "MAX_RETRIES = MAX_TRIAL_NUM*4 # it is recommended to have MAX_RETRIES>=4*MAX_TRIAL_NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.81k/4.81k [00:00<00:00, 11.0kKB/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3.0</td>\n",
       "      <td>881250949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3.0</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1.0</td>\n",
       "      <td>878887116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2.0</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1.0</td>\n",
       "      <td>886397596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userID  itemID  rating  timestamp\n",
       "0     196     242     3.0  881250949\n",
       "1     186     302     3.0  891717742\n",
       "2      22     377     1.0  878887116\n",
       "3     244      51     2.0  880606923\n",
       "4     166     346     1.0  886397596"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: The NCF model can incorporate\n",
    "df = movielens.load_pandas_df(\n",
    "    size=MOVIELENS_DATA_SIZE,\n",
    "    header=[\"userID\", \"itemID\", \"rating\", \"timestamp\"]\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validation, test = python_chrono_split(df, [0.7, 0.15, 0.15])\n",
    "train = train.drop(['timestamp'], axis=1)\n",
    "validation = validation.drop(['timestamp'], axis=1)\n",
    "test = test.drop(['timestamp'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LOG_DIR = os.path.join(TMP_DIR, \"experiments\")\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "DATA_DIR = os.path.join(TMP_DIR, \"data\") \n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "TRAIN_FILE_NAME = \"movielens_\" + MOVIELENS_DATA_SIZE + \"_train.pkl\"\n",
    "train.to_pickle(os.path.join(DATA_DIR, TRAIN_FILE_NAME))\n",
    "\n",
    "VAL_FILE_NAME = \"movielens_\" + MOVIELENS_DATA_SIZE + \"_val.pkl\"\n",
    "validation.to_pickle(os.path.join(DATA_DIR, VAL_FILE_NAME))\n",
    "\n",
    "TEST_FILE_NAME = \"movielens_\" + MOVIELENS_DATA_SIZE + \"_test.pkl\"\n",
    "test.to_pickle(os.path.join(DATA_DIR, TEST_FILE_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prepare Hyperparameter Tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run an experiment on NNI we require a general training script for our model of choice.\n",
    "A general framework for a training script utilizes the following components\n",
    "1. Argument Parse for the fixed parameters (dataset location, metrics to use)\n",
    "2. Data preprocessing steps specific to the model\n",
    "3. Fitting the model on the train set\n",
    "4. Evaluating the model on the validation set on each metric (ranking and rating)\n",
    "5. Save metrics and model\n",
    "\n",
    "To utilize NNI we also require a hypeyparameter search space. Only the hyperparameters we want to tune are required in the dictionary. NNI supports different methods of [hyperparameter sampling](https://nni.readthedocs.io/en/latest/Tutorial/SearchSpaceSpec.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `script_params` below are the parameters of the training script that are fixed (unlike `hyper_params` which are tuned). In particular, `VERBOSE, BIASED, RANDOM_STATE, NUM_EPOCHS` are parameters used in the [SVD method](../02_model/surprise_svd_deep_dive.ipynb) and `REMOVE_SEEN` removes the training data from the recommended items. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRIMARY_METRIC = \"precision_at_k\"\n",
    "RATING_METRICS = [\"rmse\"]\n",
    "RANKING_METRICS = [\"precision_at_k\", \"ndcg_at_k\"]  \n",
    "USERCOL = \"userID\"\n",
    "ITEMCOL = \"itemID\"\n",
    "REMOVE_SEEN = True\n",
    "K = 10\n",
    "RANDOM_STATE = 42\n",
    "VERBOSE = True\n",
    "BIASED = True\n",
    "\n",
    "script_params = \" \".join([\n",
    "    \"--datastore\", DATA_DIR,\n",
    "    \"--train-datapath\", TRAIN_FILE_NAME,\n",
    "    \"--validation-datapath\", VAL_FILE_NAME,\n",
    "    \"--surprise-reader\", SURPRISE_READER,\n",
    "    \"--rating-metrics\", \" \".join(RATING_METRICS),\n",
    "    \"--ranking-metrics\", \" \".join(RANKING_METRICS),\n",
    "    \"--usercol\", USERCOL,\n",
    "    \"--itemcol\", ITEMCOL,\n",
    "    \"--k\", str(K),\n",
    "    \"--random-state\", str(RANDOM_STATE),\n",
    "    \"--epochs\", str(NUM_EPOCHS),\n",
    "    \"--primary-metric\", PRIMARY_METRIC\n",
    "])\n",
    "\n",
    "if BIASED:\n",
    "    script_params += \" --biased\"\n",
    "if VERBOSE:\n",
    "    script_params += \" --verbose\"\n",
    "if REMOVE_SEEN:\n",
    "    script_params += \" --remove-seen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters search space\n",
    "# We do not set 'lr_all' and 'reg_all' because they will be overriden by the other lr_ and reg_ parameters\n",
    "\n",
    "svd_hyper_params = {\n",
    "    'n_factors': {\"_type\": \"choice\", \"_value\": [10, 50, 100, 150, 200]},\n",
    "    'init_mean': {\"_type\": \"uniform\", \"_value\": [-0.5, 0.5]},\n",
    "    'init_std_dev': {\"_type\": \"uniform\", \"_value\": [0.01, 0.2]},\n",
    "    'lr_bu': {\"_type\": \"uniform\", \"_value\": [1e-6, 0.1]}, \n",
    "    'lr_bi': {\"_type\": \"uniform\", \"_value\": [1e-6, 0.1]}, \n",
    "    'lr_pu': {\"_type\": \"uniform\", \"_value\": [1e-6, 0.1]}, \n",
    "    'lr_qi': {\"_type\": \"uniform\", \"_value\": [1e-6, 0.1]}, \n",
    "    'reg_bu': {\"_type\": \"uniform\", \"_value\": [1e-6, 1]},\n",
    "    'reg_bi': {\"_type\": \"uniform\", \"_value\": [1e-6, 1]}, \n",
    "    'reg_pu': {\"_type\": \"uniform\", \"_value\": [1e-6, 1]}, \n",
    "    'reg_qi': {\"_type\": \"uniform\", \"_value\": [1e-6, 1]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(TMP_DIR, 'search_space_svd.json'), 'w') as fp:\n",
    "    json.dump(svd_hyper_params, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also create a yaml file for the configuration of the trials and the tuning algorithm to be used (in this experiment we use the [TPE tuner](https://nni.readthedocs.io/en/latest/hyperoptTuner.html)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"authorName\": \"default\",\n",
    "    \"experimentName\": \"surprise_svd\",\n",
    "    \"trialConcurrency\": 8,\n",
    "    \"maxExecDuration\": \"1h\",\n",
    "    \"maxTrialNum\": MAX_TRIAL_NUM,\n",
    "    \"trainingServicePlatform\": \"local\",\n",
    "    # The path to Search Space\n",
    "    \"searchSpacePath\": \"search_space_svd.json\",\n",
    "    \"useAnnotation\": False,\n",
    "    \"logDir\": LOG_DIR,\n",
    "    \"tuner\": {\n",
    "        \"builtinTunerName\": \"TPE\",\n",
    "        \"classArgs\": {\n",
    "            #choice: maximize, minimize\n",
    "            \"optimize_mode\": \"maximize\"\n",
    "        }\n",
    "    },\n",
    "    # The path and the running command of trial\n",
    "    \"trial\":  {\n",
    "      \"command\": sys.prefix + \"/bin/python svd_training.py\" + \" \" + script_params,\n",
    "      \"codeDir\": os.path.join(os.path.split(os.path.abspath(reco_utils.__file__))[0], \"tuning\", \"nni\"),\n",
    "      \"gpuNum\": 0\n",
    "    }\n",
    "}\n",
    " \n",
    "with open(os.path.join(TMP_DIR, \"config_svd.yml\"), \"w\") as fp:\n",
    "    fp.write(yaml.dump(config, default_flow_style=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specify the search space for the NCF hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncf_hyper_params = {\n",
    "    'n_factors': {\"_type\": \"choice\", \"_value\": [8, 12, 16, 24, 40]},\n",
    "    'learning_rate': {\"_type\": \"uniform\", \"_value\": [1e-6, 1]},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(TMP_DIR, 'search_space_ncf.json'), 'w') as fp:\n",
    "    json.dump(ncf_hyper_params, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our NCF config.yml file follows the same structure as the SVD config.yml. The only differences are the\n",
    "- Experiment name\n",
    "- Hyperparameter Search Space\n",
    "- The executed command for the trial (Note: The script parameters have been configured to work for both training scripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"authorName\": \"default\",\n",
    "    \"experimentName\": \"tensorflow_ncf\",\n",
    "    \"trialConcurrency\": 8,\n",
    "    \"maxExecDuration\": \"1h\",\n",
    "    \"maxTrialNum\": MAX_TRIAL_NUM,\n",
    "    \"trainingServicePlatform\": \"local\",\n",
    "    # The path to Search Space\n",
    "    \"searchSpacePath\": \"search_space_ncf.json\",\n",
    "    \"useAnnotation\": False,\n",
    "    \"logDir\": LOG_DIR,\n",
    "    \"tuner\": {\n",
    "        \"builtinTunerName\": \"TPE\",\n",
    "        \"classArgs\": {\n",
    "            #choice: maximize, minimize\n",
    "            \"optimize_mode\": \"maximize\"\n",
    "        }\n",
    "    },\n",
    "    # The path and the running command of trial\n",
    "    \"trial\":  {\n",
    "      \"command\": sys.prefix + \"/bin/python ncf_training.py\" + \" \" + script_params,\n",
    "      \"codeDir\": os.path.join(os.path.split(os.path.abspath(reco_utils.__file__))[0], \"tuning\", \"nni\"),\n",
    "      \"gpuNum\": 0\n",
    "    }\n",
    "}\n",
    " \n",
    "with open(os.path.join(TMP_DIR, \"config_ncf.yml\"), \"w\") as fp:\n",
    "    fp.write(yaml.dump(config, default_flow_style=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Execute NNI Trials\n",
    "\n",
    "The conda environment comes with NNI installed, which includes the command line tool `nnictl` for controlling and getting information about NNI experiments. <br>\n",
    "To start the NNI tuning trials from the command line, execute the following command: <br>\n",
    "`nnictl create --config <path of config.yml>` <br>\n",
    "\n",
    "\n",
    "The `start_nni` function will run the `nnictl create` command. To find the URL for an active experiment you can run `nnictl webui url` on your terminal.\n",
    "\n",
    "In this notebook the SVD and NCF models are trained sequentially on different NNI experiments. While NNI can run two separate experiments simultaneously by adding the `--port <port_num>` flag to `nnictl create`, the total training time will probably be the same as running the experiments sequentially since these are CPU bound processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_nni()\n",
    "config_path_svd = os.path.join(TMP_DIR, 'config_svd.yml')\n",
    "with Timer() as time_svd:\n",
    "    start_nni(config_path_svd, wait=WAITING_TIME, max_retries=MAX_RETRIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_metrics_written(wait=WAITING_TIME, max_retries=MAX_RETRIES)\n",
    "trials_svd, best_metrics_svd, best_params_svd, best_trial_path_svd = get_trials('maximize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rmse': 1.0153957155486038,\n",
       " 'ndcg_at_k': 0.027899886410413542,\n",
       " 'precision_at_k': 0.024708377518557794}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_metrics_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'parameter_id': 22,\n",
       " 'parameter_source': 'algorithm',\n",
       " 'parameters': {'n_factors': 100,\n",
       "  'init_mean': 0.11174254678268791,\n",
       "  'init_std_dev': 0.1977986447321912,\n",
       "  'lr_bu': 0.008056394852767297,\n",
       "  'lr_bi': 0.0008200509033991312,\n",
       "  'lr_pu': 0.006790668636338857,\n",
       "  'lr_qi': 0.09214174394023733,\n",
       "  'reg_bu': 0.7866760462023896,\n",
       "  'reg_bi': 0.23022204903863294,\n",
       "  'reg_pu': 0.9432527716016627,\n",
       "  'reg_qi': 0.9808218483424879},\n",
       " 'parameter_index': 0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_nni()\n",
    "config_path_ncf = os.path.join(TMP_DIR, 'config_ncf.yml')\n",
    "with Timer() as time_ncf:\n",
    "    start_nni(config_path_ncf, wait=WAITING_TIME, max_retries=MAX_RETRIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_metrics_written(wait=WAITING_TIME, max_retries=MAX_RETRIES)\n",
    "trials_ncf, best_metrics_ncf, best_params_ncf, best_trial_path_ncf = get_trials('maximize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_metrics_ncf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_ncf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Show Results\n",
    "\n",
    "The metrics for each model type is reported on the validation set. At this point we can compare the metrics for each model and select the one with the best score on the primary metric(s) of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_metrics_dicts(*metrics):\n",
    "    df = pd.DataFrame(metrics[0], index=[0])\n",
    "    for metric in metrics[1:]:\n",
    "        df = df.append(pd.DataFrame(metric, index=[0]))\n",
    "    return df\n",
    "\n",
    "best_metrics_svd['name'] = 'svd'\n",
    "best_metrics_ncf['name'] = 'ncf'\n",
    "combine_metrics_dicts(best_metrics_svd, best_metrics_ncf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we select our model based on the validation set, we can test the model's performance on the test set using the best hyperparameters for the best model (in this case we will simply choose the NCF model, your results may differ depending on your own tests). We will do so by retraining the model on both the train and validation sets to predict on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_validation = train.append(validation).reset_index(drop=True)\n",
    "data = NCFDataset(train_and_validation, test, seed=DEFAULT_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NCF(\n",
    "    n_users=data.n_users, \n",
    "    n_items=data.n_items,\n",
    "    model_type=\"NeuMF\",\n",
    "    n_factors=best_params_ncf[\"parameters\"][\"n_factors\"],\n",
    "    n_epochs=NUM_EPOCHS,\n",
    "    learning_rate=best_params_ncf[\"parameters\"][\"learning_rate\"],\n",
    "    verbose=True,\n",
    "    seed=DEFAULT_SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_test_results(model, train, test):\n",
    "    test_results = {}\n",
    "    \n",
    "    # Rating Metrics\n",
    "    predictions = [[row.userID, row.itemID, model.predict(row.userID, row.itemID)]\n",
    "           for (_, row) in test.iterrows()]\n",
    "\n",
    "    predictions = pd.DataFrame(predictions, columns=['userID', 'itemID', 'prediction'])\n",
    "    predictions = predictions.astype({'userID': 'int64', 'itemID': 'int64', 'prediction': 'float64'})\n",
    "\n",
    "    for metric in RATING_METRICS:\n",
    "        test_results[metric] = eval(metric)(test, predictions)\n",
    "        \n",
    "    # Ranking Metrics\n",
    "    users, items, preds = [], [], []\n",
    "    item = list(train.itemID.unique())\n",
    "    for user in train.userID.unique():\n",
    "        user = [user] * len(item) \n",
    "        users.extend(user)\n",
    "        items.extend(item)\n",
    "        preds.extend(list(model.predict(user, item, is_list=True)))\n",
    "\n",
    "    all_predictions = pd.DataFrame(data={\"userID\": users, \"itemID\": items, \"prediction\": preds})\n",
    "\n",
    "    merged = pd.merge(train, all_predictions, on=[\"userID\", \"itemID\"], how=\"outer\")\n",
    "    all_predictions = merged[merged.rating.isnull()].drop('rating', axis=1)\n",
    "\n",
    "    for metric in RANKING_METRICS:\n",
    "        test_results[metric] = eval(metric)(test, all_predictions, col_prediction='prediction', k=K)\n",
    "        \n",
    "    return test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = compute_test_results(model, train_and_validation, test)\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see in the table above, _annealing_ performs best with respect to the primary metric (precision@10) that all the tuners optimized. Also the best NDCG@10 is obtained for annealing and correlates well with precision@10. RMSE on the other hand does not correlate well and is not optimized for annealing, since finding the top k recommendations in the right order is a different task from predicting ratings (high and low) accurately.     \n",
    "We have also observed that the above ranking of the tuners is not consistent and may change when trying these experiments multiple times. Since some of these tuners rely heavily on randomized sampling, a larger number of trials is required to get more consistent metrics.\n",
    "In addition, some of the tuning algorithms themselves come with parameters, which can affect their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the NNI experiment \n",
    "stop_nni()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_dir.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Concluding Remarks\n",
    "\n",
    "In this notebook we showed how to use the NNI framework on different models. By inspection of the training scripts, the differences between the two should help you identify what components would need to be modified to run another model with NNI.\n",
    "\n",
    "In practice, an AutoML framework like NNI is just a tool to help you explore a large space of hyperparameters quickly with a pre-described level of randomization. It is recommended that in addition to using NNI one trains baseline models using typical hyperparamter choices (learning rate of 0.005, 0.001 or regularization rates of 0.05, 0.01, etc.) to draw  more meaningful comparisons between model performances. This may help determine if a model is overfitting from the tuner or if there is a statistically significant improvement.\n",
    "\n",
    "Another thing to note is the added computational cost required to train models using an AutoML framework. In this case, it takes about 1 minute to train each of the models on a [Standard_NC6 VM](https://docs.microsoft.com/en-us/azure/virtual-machines/nc-series). With this in mind, while NNI can easily train hundreds of models over all hyperparameters for a model, in practice it may be beneficial to choose a subset of the hyperparameters that are deemed most important and to tune those. Too small of a hyperparameter search space may restrict our exploration, but too large may also lead to random noise in the data being exploited by a specific combination of hyperparameters.   \n",
    "\n",
    "For examples of scaling larger tuning workloads on clusters of machines, see [the notebooks](./README.md) that employ the [Azure Machine Learning service](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. References\n",
    "\n",
    "Recommenders Repo References\n",
    "* [SVD deep-dive notebook](../02_model/surprise_svd_deep_dive.ipynb)\n",
    "* [NCF deep-dive notebook](../02_model/ncf_deep_dive.ipynb)\n",
    "* [SVD + NNI model optimization](./nni_surprise_svd.ipynb)\n",
    "\n",
    "External References\n",
    "* [Surprise Docs | Matrix factorization algorithms](https://surprise.readthedocs.io/en/stable/matrix_factorization.html) \n",
    "* [NNI Docs | Neural Network Intelligence toolkit](https://github.com/Microsoft/nni)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python (recommenders_gpu)",
   "language": "python",
   "name": "recommenders_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
