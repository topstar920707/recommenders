{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.<br>\n",
    "Licensed under the MIT License.</i>\n",
    "<br><br>\n",
    "# Recommender Hyperparameter Tuning w/ AzureML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we show how to hyperparameter tune a recommender model by utilizing **Azure Machine Learning service*** ([AML or AzureML](https://azure.microsoft.com/en-us/services/machine-learning-service/)) in the context of movie recommendation. Note, to use AML, you will need Azure subscription.\n",
    "\n",
    "Here, we use [**wide-and-deep model**](https://ai.googleblog.com/2016/06/wide-deep-learning-better-together-with.html) from TensorFlow high-level Estimator API.\n",
    "\n",
    "We present an overall process of utilizing AML by demonstrating some key steps while avoiding showing too much details. This notebook includes many useful links for those details instead.\n",
    "  \n",
    "<br>  \n",
    "\n",
    "For more details about the **wide-and-deep** model:\n",
    "* [Wide-Deep Quickstart notebook](../00_quick_start/wide_deep_model_movielens.ipynb)\n",
    "* [Original paper](https://arxiv.org/abs/1606.07792)\n",
    "* [TensorFlow API doc](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNLinearCombinedRegressor)\n",
    "  \n",
    "Regarding **AuzreML**, please refer:\n",
    "* [Quickstart notebook](https://docs.microsoft.com/en-us/azure/machine-learning/service/quickstart-create-workspace-with-python)\n",
    "* [Hyperdrive](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters)\n",
    "* [Tensorflow model tuning with hyperdrive](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-train-tensorflow)\n",
    "\n",
    "> \\* When you web-search \"Azure Machine Learning\", you will most likely to see mixed results of Azure Machine Learning (we call it AML) and Azure Machine Learning **Studio**. Please note they are different services where AML's focuses are on ML model management, tracking and hyperparameter tuning, while the [ML Studio](https://studio.azureml.net/)'s is to provide a high-level tool for 'easy-to-use' experience of ML designing and experimentation based on GUI.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisite\n",
    "To run this example, you'll need to install [`azureml-sdk`](https://pypi.org/project/azureml-sdk/).\n",
    "If you are using a [Data Science Virtual Machine (DSVM)](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-configure-environment#dsvm) or [Azure Notebook](https://notebooks.azure.com/), `azureml-sdk` is already installed in it so you don't need to install the package.\n",
    "\n",
    "To install AML Python SDK*, run\n",
    "```\n",
    "pip install --upgrade azureml-sdk[notebooks]\n",
    "```\n",
    "\n",
    "More info about setting up AML environment can be found from [this link](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-configure-environment).\n",
    "\n",
    "> \\* AML has a Databricks sdk `azureml-sdk[databricks]` but it doesn't support hyperparameter tuning on Databricks for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AML Workspace Configuration\n",
    "AML workspace is the foundational block in the cloud that you use to experiment, train, and deploy machine learning models. We 1) setup a workspace from Azure portal and 2) create a config file manually. The instructions here are based on AML documents about [Quickstart with Azure portal](https://docs.microsoft.com/en-us/azure/machine-learning/service/quickstart-get-started) and [Quickstart with Python SDK](https://docs.microsoft.com/en-us/azure/machine-learning/service/quickstart-create-workspace-with-python) where you can find more details about the setup process with screen-shots.\n",
    "  \n",
    "<br>\n",
    "  \n",
    "#### Create a workspace\n",
    "1. Sign in to the [Azure portal](https://portal.azure.com) by using the credentials for the Azure subscription you use.\n",
    "2. Select **Create a resource** menu, search for **Machine Learning service workspace** select **Create** button.\n",
    "3. In the **ML service workspace** pane, configure your workspace with entering the *workspace name* and *resource group* (or **create new** resource group if you don't have one already), and select **Create**. It can take a few moments to create the workspace.\n",
    "  \n",
    "<br>\n",
    "  \n",
    "#### Make a configuration file\n",
    "To configure this notebook to communicate with your workspace easily, create a *.\\aml_config\\config.json* file with the following contents:\n",
    "```\n",
    "{\n",
    "    \"subscription_id\": \"<subscription-id>\",\n",
    "    \"resource_group\": \"<resource-group>\",\n",
    "    \"workspace_name\": \"<workspace-name>\"\n",
    "}\n",
    "```\n",
    "replacing `<subscription-id>`, `<resource-group>`, and `<workspace-name>` with the strings of your subscription id, resource group, and workspace name, respectively.\n",
    "\n",
    "Now let's see if everything is ready!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure ML SDK Version: 1.0.2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "import sklearn.preprocessing\n",
    "\n",
    "import azureml as aml\n",
    "import azureml.widgets\n",
    "import azureml.train.dnn\n",
    "import azureml.train.hyperdrive as hd\n",
    "\n",
    "from reco_utils.dataset import movielens\n",
    "from reco_utils.dataset.python_splitters import python_random_split\n",
    "\n",
    "print(\"Azure ML SDK Version:\", aml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the config file in: C:\\Users\\jumin\\git\\Recommenders\\notebooks\\04_model_select_and_optimize\\aml_config\\config.json\n",
      "AML workspace name:  junmin-aml-workspace\n"
     ]
    }
   ],
   "source": [
    "# Connect to a workspace\n",
    "ws = aml.core.Workspace.from_config()\n",
    "print(\"AML workspace name: \", ws.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the following cells, we\n",
    "1. Create a *remote compute target* (gpu-cluster) if it does not exist already,\n",
    "2. Mount a *data store* and upload the training set, and\n",
    "3. Run a hyperparameter tuning experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Remote Compute Target\n",
    "\n",
    "We create a gpu cluster for our remote compute target. The script will load the cluster if it already exists. You can see [this document](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-set-up-training-targets) to learn more about setting up a *compute target*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute target\n",
      "{'allocationState': 'Steady', 'allocationStateTransitionTime': '2019-01-23T06:02:14.017000+00:00', 'creationTime': '2019-01-23T03:32:15.957797+00:00', 'currentNodeCount': 8, 'errors': None, 'modifiedTime': '2019-01-23T03:32:53.879336+00:00', 'nodeStateCounts': {'idleNodeCount': 7, 'leavingNodeCount': 0, 'preemptedNodeCount': 0, 'preparingNodeCount': 1, 'runningNodeCount': 0, 'unusableNodeCount': 0}, 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 1, 'maxNodeCount': 8, 'nodeIdleTimeBeforeScaleDown': 'PT120S'}, 'targetNodeCount': 8, 'vmPriority': 'LowPriority', 'vmSize': 'STANDARD_NC6'}\n"
     ]
    }
   ],
   "source": [
    "CLUSTER_NAME = 'gpu-cluster-8'\n",
    "\n",
    "try:\n",
    "    compute_target = aml.core.compute.ComputeTarget(workspace=ws, name=CLUSTER_NAME)\n",
    "    print(\"Found existing compute target\")\n",
    "except aml.core.compute_target.ComputeTargetException:\n",
    "    print(\"Creating a new compute target...\")\n",
    "    compute_config = aml.core.compute.AmlCompute.provisioning_configuration(\n",
    "        vm_size='STANDARD_NC6',\n",
    "        vm_priority='lowpriority',\n",
    "        min_nodes=1,\n",
    "        max_nodes=8\n",
    "    )\n",
    "    # create the cluster\n",
    "    compute_target = aml.core.compute.ComputeTarget.create(ws, CLUSTER_NAME, compute_config)\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "# Use the 'status' property to get a detailed status for the current cluster. \n",
    "print(compute_target.status.serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Dataset\n",
    "1. Download data and split into training, evaluation, and testing sets\n",
    "2. Upload training and evaluation sets to the workspace's default **blob storage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommend top k items\n",
    "TOP_K = 10\n",
    "\n",
    "# Select Movielens data size: 100k, 1m, 10m, or 20m\n",
    "MOVIELENS_DATA_SIZE = '1m'\n",
    "\n",
    "USER_COL = 'UserId'\n",
    "ITEM_COL = 'MovieId'\n",
    "RATING_COL = 'Rating'\n",
    "ITEM_FEAT_COL = 'Genres'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserId</th>\n",
       "      <th>MovieId</th>\n",
       "      <th>Rating</th>\n",
       "      <th>_Genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1193</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>1193</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>1193</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>1193</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserId  MovieId  Rating _Genres\n",
       "0       1     1193     5.0   Drama\n",
       "1       2     1193     5.0   Drama\n",
       "2      12     1193     4.0   Drama\n",
       "3      15     1193     4.0   Drama\n",
       "4      17     1193     5.0   Drama"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = movielens.load_pandas_df(\n",
    "    size=MOVIELENS_DATA_SIZE,\n",
    "    header=[USER_COL, ITEM_COL, RATING_COL],\n",
    "    genres_col='_Genres'\n",
    ")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genres: ['Action' 'Adventure' 'Animation' \"Children's\" 'Comedy' 'Crime'\n",
      " 'Documentary' 'Drama' 'Fantasy' 'Film-Noir' 'Horror' 'Musical' 'Mystery'\n",
      " 'Romance' 'Sci-Fi' 'Thriller' 'War' 'Western']\n"
     ]
    }
   ],
   "source": [
    "# Encode 'genres' into int array (multi-hot representation) to use as item features\n",
    "genres_encoder = sklearn.preprocessing.MultiLabelBinarizer()\n",
    "data[ITEM_FEAT_COL] = genres_encoder.fit_transform(\n",
    "    data['_Genres'].apply(lambda s: s.split(\"|\"))\n",
    ").tolist()\n",
    "print(\"Genres:\", genres_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation set for the parameter tuning should be separated from the final test set\n",
    "train, _ = python_random_split(\n",
    "    data.drop('_Genres', axis=1),\n",
    "    ratio=0.75,\n",
    "    seed=123\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_2c016774a6644a20a476076213035161"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = os.path.join('.', 'data')\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "TRAIN_FILE_NAME = \"movielens_\" + MOVIELENS_DATA_SIZE + \"_train.pkl\"\n",
    "train.to_pickle(os.path.join(DATA_DIR, TRAIN_FILE_NAME))\n",
    "\n",
    "# Note, all the files under DATA_DIR will be uploaded to the data store\n",
    "ds = ws.get_default_datastore()\n",
    "ds.upload(\n",
    "    src_dir=DATA_DIR,\n",
    "    target_path='data',\n",
    "    overwrite=True,\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare training script. All the script in the folder will be uploaded\n",
    "\n",
    "We also prepare a training script [wide_deep_training.py](../../reco_utils/aml/wide_deep_training.py) for the hyperparameter tuning, which will log our target metrics such as [RMSE](https://en.wikipedia.org/wiki/Root-mean-square_deviation) and/or [NDCG](https://en.wikipedia.org/wiki/Discounted_cumulative_gain) to AML experiment so that we can track the metrics and optimize the primary metric via **hyperdrive**.\n",
    "\n",
    "```\n",
    "TODO - maybe attach a code snippet here for description\n",
    "1. logging part\n",
    "\n",
    "2. wide and deep model\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPT_DIR = os.path.join('.', 'aml_scripts')\n",
    "dest_dir = os.path.join(SCRIPT_DIR, 'reco_utils')\n",
    "try:\n",
    "    shutil.copytree(os.path.join('..', '..', 'reco_utils'), dest_dir)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "ENTRY_SCRIPT_NAME = 'reco_utils/aml/wide_deep_training.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a search space for the hyperparameters. All the parameter values will be passed to the training script where they are parsed by `argparse`, e.g.:\n",
    "```\n",
    "TODO code snippet for argparse\n",
    "```\n",
    "    \n",
    "AML hyperdrive provides some very useful searching strategies including `RandomParameterSampling`, `GridParameterSampling`, and `BayesianParameterSampling`. Details about each approach are beyond the scope of this notebook and you can find them from [Azure doc](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters). Here, we use the random sampling for simplicity. \n",
    "\n",
    "> Note: Currently, this repo accepts either 'rmse' or 'mae' for `METRICS` as implemented in [tf_utils.py](../../reco_utils/common/tf_utils.py), but you can define any custom metrics and utilize it along with AML hyperdrive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = \"movielens_\" + MOVIELENS_DATA_SIZE + \"_wide_deep_model\"\n",
    "METRICS = ['mae', 'ndcg@10']  # Put your primary metric at the first place\n",
    "\n",
    "script_params = {\n",
    "    '--datastore': ds.as_mount(),\n",
    "    '--train-datapath': \"data/\" + TRAIN_FILE_NAME,\n",
    "    '--user-col': USER_COL,\n",
    "    '--item-col': ITEM_COL,\n",
    "    '--item-feat-col': ITEM_FEAT_COL,\n",
    "    '--rating-col': RATING_COL,\n",
    "    '--metrics': METRICS,\n",
    "    '--model-type': 'wide_deep',\n",
    "    '--epochs': 50,\n",
    "}\n",
    "\n",
    "# hyperparameters search space\n",
    "hyper_params = {\n",
    "    '--batch-size': hd.choice(32, 64, 128, 256),\n",
    "    # Wide model hyperparameters\n",
    "    '--linear-optimizer': hd.choice('Ftrl', 'SGD'),\n",
    "    '--linear-optimizer-lr': hd.uniform(0.0005, 0.1),\n",
    "    '--l1-reg': hd.uniform(0.0, 0.1),\n",
    "    # Deep model hyperparameters\n",
    "    '--dnn-optimizer': hd.choice('Adagrad', 'Adam'),\n",
    "    '--dnn-optimizer-lr': hd.uniform(0.0005, 0.1),\n",
    "    '--dnn-user-embedding-dim': hd.choice(8, 32, 128),\n",
    "    '--dnn-item-embedding-dim': hd.choice(4, 16, 64),\n",
    "    '--dnn-hidden-units': hd.choice(\n",
    "        \"256,256,256,128\",\n",
    "        \"256,128\",\n",
    "        \"256,64,256\",\n",
    "        \"512,128,32\"\n",
    "    ),\n",
    "    '--dnn-batch-norm': hd.choice(True, False),\n",
    "    '--dropout': hd.uniform(0.0, 0.5),\n",
    "}\n",
    "\n",
    "# Note, BayesianParameterSampling only support choice, uniform, and quniform. Also list type doesn't support\n",
    "ps = hd.RandomParameterSampling(hyper_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `azureml.train.dnn.TensorFlow`, a custom AML `Estimator` class which utilizes a preset docker image in the cluster (see more information from [here](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-train-tensorflow)).\n",
    "\n",
    "Once you submit the experiment, you can see the progress from the notebook by using `azureml.widgets.RunDetails`. You can directly check the details from the Azure portal as well. To get the link, run `run.get_portal_url()`.\n",
    "\n",
    "> Since we will do hyperparameter tuning, we create a `HyperDriveRunConfig` and pass it to the experiment object. If you already know what hyperparameters to use and still want to utilize AML for other purposes (e.g. model management), you can set the hyperparameter values directly to `script_params` and run the experiment, `run = exp.submit(est)`, instead.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://mlworkspace.azure.ai/portal/subscriptions/03909a66-bef8-4d52-8e9a-a346604e0902/resourceGroups/junmin-aml/providers/Microsoft.MachineLearningServices/workspaces/junmin-aml-workspace/experiments/movielens_1m_wide_deep_model/runs/movielens_1m_wide_deep_model_1548223573070\n"
     ]
    }
   ],
   "source": [
    "est = azureml.train.dnn.TensorFlow(\n",
    "    source_directory=SCRIPT_DIR,\n",
    "    entry_script=ENTRY_SCRIPT_NAME,\n",
    "    script_params=script_params,\n",
    "    compute_target=compute_target,\n",
    "    use_gpu=True,\n",
    "    conda_packages=['pandas', 'scikit-learn'],\n",
    ")\n",
    "\n",
    "# early termnination policy\n",
    "policy = hd.BanditPolicy(evaluation_interval=1, slack_factor=0.1, delay_evaluation=3)\n",
    "\n",
    "hd_config = hd.HyperDriveRunConfig(\n",
    "    estimator=est, \n",
    "    hyperparameter_sampling=ps,\n",
    "    policy=policy,  # Not supported for BayesianParameterSampling\n",
    "    primary_metric_name=METRICS[0],\n",
    "    primary_metric_goal=hd.PrimaryMetricGoal.MINIMIZE, \n",
    "    max_total_runs=100,\n",
    "    max_concurrent_runs=8\n",
    ")\n",
    "\n",
    "# Create an experiment to track the runs in the workspace\n",
    "exp = aml.core.Experiment(workspace=ws, name=EXP_NAME)\n",
    "\n",
    "run = exp.submit(config=hd_config)\n",
    "print(run.get_portal_url())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f251391077aa47f2889355cb7477f041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_HyperDriveWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'NOTSE…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing interactive authentication. Please follow the instructions on the terminal.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note, we have launched a browser for you to login. For old experience with device code, use \"az login --use-device-code\"\n"
     ]
    }
   ],
   "source": [
    "azureml.widgets.RunDetails(run).show()\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get bestrun and printout metrics!!!\n",
    "best_run = run.get_best_run_by_primary_metric()\n",
    "\n",
    "best_run_metrics = best_run.get_metrics()\n",
    "parameter_values = best_run.get_details()['runDefinition']['Arguments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['--datastore', '$AZUREML_DATAREFERENCE_workspaceblobstore', '--train-datapath', 'data/movielens_1m_train.pkl', '--user-col', 'UserId', '--item-col', 'MovieId', '--item-feat-col', 'Genres', '--rating-col', 'Rating', '--metrics', 'mae', 'ndcg@10', '--model-type', 'wide_deep', '--epochs', '50', '--batch-size', '128', '--dnn-batch-norm', 'True', '--dnn-hidden-units', '256,64,256', '--dnn-item-embedding-dim', '4', '--dnn-optimizer', 'Adam', '--dnn-optimizer-lr', '0.02039596884309', '--dnn-user-embedding-dim', '128', '--dropout', '0.289914344143064', '--l1-reg', '0.0101626002122412', '--linear-optimizer', 'Ftrl', '--linear-optimizer-lr', '0.0369276616034676']\n",
      "mae = 0.7174221578052258\n",
      "ndcg@10 = 0.03484969394127618\n"
     ]
    }
   ],
   "source": [
    "print(parameter_values)\n",
    "\n",
    "for m in METRICS:\n",
    "    print(m, best_run_metrics[m], sep=' = ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.cancel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "https://github.com/MtDersvan/tf_playground/blob/master/wide_and_deep_tutorial/wide_and_deep_export_r1.3.ipynb\n",
    "\n",
    "* [Fine-tune natural language processing models using Azure Machine Learning service](https://azure.microsoft.com/en-us/blog/fine-tune-natural-language-processing-models-using-azure-machine-learning-service/)\n",
    "* [Training, hyperparameter tune, and deploy with TensorFlow](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/training-with-deep-learning/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml",
   "language": "python",
   "name": "aml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
