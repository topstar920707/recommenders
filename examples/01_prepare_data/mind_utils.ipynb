{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Mind Utils Generation\n",
    "\n",
    "Many news recommendation methods ultilize word embeddings, news vertical embeddings, news subvertical embeddings and user id embedding. Therefore, it is necessary to generate a word dictionary, a vertical dictionary, a subvertical dictionary and a userid dictionary to convert words, news verticals, subvericals and user ids from strings to indexes. To use the pretrain word embedding, a embedding matrix is generated as the intial weight of the word embedding layer.\n",
    "\n",
    "\n",
    "This notebook gives examples about how to generate\n",
    "* word_dict.pkl: convert the words in news titles into indexes.\n",
    "* word_dict_all.pkl: convert the words in news titles and abstracts into indexes.\n",
    "* embedding.npy: pretrained word embedding matrix of words in word_dict.pkl\n",
    "* embedding_all.npy: pretrained embedding matrix of words in word_dict_all.pkl\n",
    "* vert_dict.pkl: convert news verticals into indexes.\n",
    "* subvert_dict.pkl: convert news subverticals into indexes.\n",
    "* uid2index.pkl: convert user ids into indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.6.11 | packaged by conda-forge | (default, Aug  5 2020, 20:09:42) \n",
      "[GCC 7.5.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import os\n",
    "import papermill as pm\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "\n",
    "from tempfile import TemporaryDirectory\n",
    "from reco_utils.dataset.mind import (download_mind,\n",
    "                                     extract_mind,\n",
    "                                     _download_and_extract_globe)\n",
    "from reco_utils.dataset.download_utils import unzip_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mind_type=\"small\"\n",
    "# word_embedding_dim should be in [50, 100, 200, 300]\n",
    "word_embedding_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51.7k/51.7k [00:01<00:00, 40.2kKB/s]\n",
      "100%|██████████| 30.2k/30.2k [00:00<00:00, 35.8kKB/s]\n"
     ]
    }
   ],
   "source": [
    "tmpdir = TemporaryDirectory()\n",
    "data_path = tmpdir.name\n",
    "train_zip, valid_zip = download_mind(size=mind_type, dest_path=data_path)\n",
    "unzip_file(train_zip, os.path.join(data_path, 'train'))\n",
    "unzip_file(valid_zip, os.path.join(data_path, 'valid'))\n",
    "output_path = os.path.join(data_path, 'utils')\n",
    "os.makedirs(output_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare utils of news\n",
    "\n",
    "* word dictionary\n",
    "* vertical dictionary\n",
    "* subvetical dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_table(os.path.join(data_path, 'train', 'news.tsv'),\n",
    "                     names=['newid', 'vertical', 'subvertical', 'title',\n",
    "                            'abstract', 'url', 'entities in title', 'entities in abstract'],\n",
    "                     usecols = ['vertical', 'subvertical', 'title', 'abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vertical</th>\n",
       "      <th>subvertical</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lifestyle</td>\n",
       "      <td>lifestyleroyals</td>\n",
       "      <td>The Brands Queen Elizabeth, Prince Charles, an...</td>\n",
       "      <td>Shop the notebooks, jackets, and more that the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>health</td>\n",
       "      <td>weightloss</td>\n",
       "      <td>50 Worst Habits For Belly Fat</td>\n",
       "      <td>These seemingly harmless habits are holding yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>news</td>\n",
       "      <td>newsworld</td>\n",
       "      <td>The Cost of Trump's Aid Freeze in the Trenches...</td>\n",
       "      <td>Lt. Ivan Molchanets peeked over a parapet of s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>health</td>\n",
       "      <td>voices</td>\n",
       "      <td>I Was An NBA Wife. Here's How It Affected My M...</td>\n",
       "      <td>I felt like I was a fraud, and being an NBA wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>health</td>\n",
       "      <td>medical</td>\n",
       "      <td>How to Get Rid of Skin Tags, According to a De...</td>\n",
       "      <td>They seem harmless, but there's a very good re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    vertical      subvertical  \\\n",
       "0  lifestyle  lifestyleroyals   \n",
       "1     health       weightloss   \n",
       "2       news        newsworld   \n",
       "3     health           voices   \n",
       "4     health          medical   \n",
       "\n",
       "                                               title  \\\n",
       "0  The Brands Queen Elizabeth, Prince Charles, an...   \n",
       "1                      50 Worst Habits For Belly Fat   \n",
       "2  The Cost of Trump's Aid Freeze in the Trenches...   \n",
       "3  I Was An NBA Wife. Here's How It Affected My M...   \n",
       "4  How to Get Rid of Skin Tags, According to a De...   \n",
       "\n",
       "                                            abstract  \n",
       "0  Shop the notebooks, jackets, and more that the...  \n",
       "1  These seemingly harmless habits are holding yo...  \n",
       "2  Lt. Ivan Molchanets peeked over a parapet of s...  \n",
       "3  I felt like I was a fraud, and being an NBA wi...  \n",
       "4  They seem harmless, but there's a very good re...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_vertical = news.vertical.drop_duplicates().reset_index(drop=True)\n",
    "vert_dict_inv = news_vertical.to_dict()\n",
    "vert_dict = {v: k+1 for k, v in vert_dict_inv.items()}\n",
    "\n",
    "news_subvertical = news.subvertical.drop_duplicates().reset_index(drop=True)\n",
    "subvert_dict_inv = news_subvertical.to_dict()\n",
    "subvert_dict = {v: k+1 for k, v in vert_dict_inv.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize(sent):\n",
    "    pat = re.compile(r'[\\w]+|[.,!?;|]')\n",
    "    if isinstance(sent, str):\n",
    "        return pat.findall(sent.lower())\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "news.title = news.title.apply(word_tokenize)\n",
    "news.abstract = news.abstract.apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51282/51282 [00:27<00:00, 1854.28it/s]\n"
     ]
    }
   ],
   "source": [
    "word_cnt = Counter()\n",
    "word_cnt_all = Counter()\n",
    "\n",
    "for i in tqdm(range(len(news))):\n",
    "    word_cnt.update(news.loc[i]['title'])\n",
    "    word_cnt_all.update(news.loc[i]['title'])\n",
    "    word_cnt_all.update(news.loc[i]['abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {k: v+1 for k, v in zip(word_cnt, range(len(word_cnt)))}\n",
    "word_dict_all = {k: v+1 for k, v in zip(word_cnt_all, range(len(word_cnt_all)))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(output_path, 'vert_dict.pkl'), 'wb') as f:\n",
    "    pickle.dump(vert_dict, f)\n",
    "    \n",
    "with open(os.path.join(output_path, 'subvert_dict.pkl'), 'wb') as f:\n",
    "    pickle.dump(subvert_dict, f)\n",
    "\n",
    "with open(os.path.join(output_path, 'word_dict.pkl'), 'wb') as f:\n",
    "    pickle.dump(word_dict, f)\n",
    "    \n",
    "with open(os.path.join(output_path, 'word_dict_all.pkl'), 'wb') as f:\n",
    "    pickle.dump(word_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare embedding matrixs\n",
    "* embedding.npy\n",
    "* embedding_all.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 842k/842k [09:21<00:00, 1.50kKB/s]  \n"
     ]
    }
   ],
   "source": [
    "glove_path = _download_and_extract_globe(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_matrix(path_emb, word_dict, word_embedding_dim):\n",
    "    embedding_matrix = np.zeros((len(word_dict)+1, word_embedding_dim))\n",
    "    exist_word=[]\n",
    "\n",
    "    with open(os.path.join(path_emb, f\"glove.6B.{word_embedding_dim}d.txt\"),'rb') as f:\n",
    "        for l in tqdm(f):\n",
    "            l=l.split()\n",
    "            word = l[0].decode()\n",
    "            if len(word) != 0:\n",
    "                wordvec = [float(x) for x in l[1:]]\n",
    "                if word in word_dict:\n",
    "                    index = word_dict[word]\n",
    "                    embedding_matrix[index]=np.array(wordvec)\n",
    "                    exist_word.append(word)\n",
    "                    \n",
    "    return embedding_matrix, exist_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:29, 13535.85it/s]\n",
      "400000it [00:29, 13444.48it/s]\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix, exist_word = load_matrix(glove_path, word_dict, word_embedding_dim)\n",
    "embedding_all_matrix, exist_all_word = load_matrix(glove_path, word_dict_all, word_embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(output_path, 'embedding.npy'), embedding_matrix)\n",
    "np.save(os.path.join(output_path, 'embedding_all.npy'), embedding_all_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare uid2index.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156965it [00:00, 455714.84it/s]\n"
     ]
    }
   ],
   "source": [
    "uid2index = {}\n",
    "\n",
    "with open(os.path.join(data_path, 'train', 'behaviors.tsv'), 'r') as f:\n",
    "    for l in tqdm(f):\n",
    "        uid = l.strip('\\n').split('\\t')[1]\n",
    "        if uid not in uid2index:\n",
    "            uid2index[uid] = len(uid2index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(output_path, 'uid2index.pkl'), 'wb') as f:\n",
    "    pickle.dump(uid2index, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils_state = {\n",
    "    'vert_num': len(vert_dict),\n",
    "    'subvert_num': len(subvert_dict),\n",
    "    'word_num': len(word_dict),\n",
    "    'word_num_all': len(word_dict_all),\n",
    "    'embedding_exist_num': len(exist_word),\n",
    "    'embedding_exist_num_all': len(exist_all_word),\n",
    "    'uid2index': len(uid2index)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/reco_gpu/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.1). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "application/papermill.record+json": {
       "utils_state": {
        "embedding_exist_num": 29081,
        "embedding_exist_num_all": 48422,
        "subvert_num": 17,
        "uid2index": 50000,
        "vert_num": 17,
        "word_num": 31029,
        "word_num_all": 55028
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pm.record(\"utils_state\", utils_state)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python [conda env:reco_gpu]",
   "language": "python",
   "name": "conda-env-reco_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
