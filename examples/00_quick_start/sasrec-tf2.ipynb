{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SASRec in Tensorflow 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.7.7 (default, Mar 26 2020, 15:48:22) \n",
      "[GCC 7.3.0]\n",
      "Tensorflow version: 2.3.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import scrapbook as sb\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Tensorflow version: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# disable GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, \"/recsys_data/RecSys/recommenders-tf2/myfork/recommenders/recommenders/models/sasrec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sampler import WarpSampler\n",
    "from model import SASREC\n",
    "from model_ssept import SSEPT\n",
    "from util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combined_dataset(u, seq, pos, neg, seq_max_len):\n",
    "    inputs = {}\n",
    "    seq = tf.keras.preprocessing.sequence.pad_sequences(seq,\n",
    "            padding='pre',\n",
    "            truncating='pre',\n",
    "            maxlen=seq_max_len)\n",
    "    pos = tf.keras.preprocessing.sequence.pad_sequences(pos,\n",
    "            padding='pre',\n",
    "            truncating='pre',\n",
    "            maxlen=seq_max_len)\n",
    "    neg = tf.keras.preprocessing.sequence.pad_sequences(neg,\n",
    "            padding='pre',\n",
    "            truncating='pre',\n",
    "            maxlen=seq_max_len)\n",
    "\n",
    "    inputs['users'] = np.expand_dims(np.array(u), axis=-1)\n",
    "    inputs['input_seq'] = seq\n",
    "    inputs['positive'] = pos\n",
    "    inputs['negative'] = neg\n",
    "\n",
    "    target = np.concatenate([np.repeat(1, seq.shape[0] * seq.shape[1]), \n",
    "                             np.repeat(0, seq.shape[0] * seq.shape[1])], axis=0)\n",
    "    target = np.expand_dims(target, axis=-1)\n",
    "    return inputs, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_file = '../../recommenders/models/sasrec/config/sas_rec.yaml'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "batch_size = 128\n",
    "RANDOM_SEED = 100  # Set None for non-deterministic result\n",
    "\n",
    "data_path = os.path.join(\"..\", \"..\", \"tests\", \"resources\", \"deeprec\", \"slirec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/recsys_data/RecSys/SASRec-tf2/data/\"\n",
    "dataset = \"ae\"\n",
    "batch_size = 128\n",
    "num_epochs = 5\n",
    "lr = 0.001\n",
    "\n",
    "maxlen = 50\n",
    "num_blocks = 2\n",
    "hidden_units = 100\n",
    "num_heads = 1\n",
    "dropout_rate = 0.1\n",
    "l2_emb = 0.0\n",
    "num_neg_test = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/recsys_data/RecSys/SASRec-tf2/data/ae.txt\n",
      "63114 Users and 85930 items\n",
      "average sequence length: 13.04\n"
     ]
    }
   ],
   "source": [
    "inp_file = os.path.join(data_dir, dataset + \".txt\")\n",
    "print(inp_file)\n",
    "\n",
    "dataset = data_partition(inp_file)\n",
    "[user_train, user_valid, user_test, usernum, itemnum] = dataset\n",
    "num_steps = int(len(user_train) / batch_size)\n",
    "cc = 0.0\n",
    "for u in user_train:\n",
    "    cc += len(user_train[u])\n",
    "print('%g Users and %g items' % (usernum, itemnum))\n",
    "print('average sequence length: %.2f' % (cc / len(user_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SASREC(item_num=itemnum,\n",
    "               seq_max_len=maxlen,\n",
    "               num_blocks=num_blocks,\n",
    "               embedding_dim=hidden_units,\n",
    "               attention_dim=hidden_units,\n",
    "               attention_num_heads=num_heads,\n",
    "               dropout_rate=dropout_rate,\n",
    "            #    conv_dims = kwargs.get(\"conv_dims\", [100, 100])\n",
    "               l2_reg=l2_emb,\n",
    "               num_neg_test=num_neg_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr,\n",
    "                                        beta_1=0.9,\n",
    "                                        beta_2=0.999,\n",
    "                                        epsilon=1e-7)\n",
    "\n",
    "def loss_function(pos_logits, neg_logits, istarget):\n",
    "    pos_logits = pos_logits[:,0]\n",
    "    neg_logits = neg_logits[:,0]\n",
    "\n",
    "    # for logits\n",
    "    loss = tf.reduce_sum(\n",
    "        - tf.math.log(tf.math.sigmoid(pos_logits) + 1e-24) * istarget -\n",
    "        tf.math.log(1 - tf.math.sigmoid(neg_logits) + 1e-24) * istarget\n",
    "    ) / tf.reduce_sum(istarget)\n",
    "\n",
    "    # for probabilities\n",
    "    # loss = tf.reduce_sum(\n",
    "    #         - tf.math.log(pos_logits + 1e-24) * istarget -\n",
    "    #         tf.math.log(1 - neg_logits + 1e-24) * istarget\n",
    "    # ) / tf.reduce_sum(istarget)\n",
    "    reg_loss = tf.compat.v1.losses.get_regularization_loss()\n",
    "    # reg_losses = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    # loss += sum(reg_losses)\n",
    "    loss += reg_loss\n",
    "    return loss\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')\n",
    "\n",
    "train_step_signature = [\n",
    "    {'users': tf.TensorSpec(shape=(None, 1), dtype=tf.int64),\n",
    "     'input_seq': tf.TensorSpec(shape=(None, maxlen), dtype=tf.int64),\n",
    "     'positive': tf.TensorSpec(shape=(None, maxlen), dtype=tf.int64),\n",
    "     'negative': tf.TensorSpec(shape=(None, maxlen), dtype=tf.int64)},\n",
    "    tf.TensorSpec(shape=(None, 1), dtype=tf.int64)\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "    with tf.GradientTape() as tape:\n",
    "        pos_logits, neg_logits, loss_mask = model(inp, training=True)\n",
    "        loss = loss_function(pos_logits, neg_logits, loss_mask)\n",
    "        # loss = loss_function_(tar, predictions)\n",
    "        # loss = model.loss_function(*predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    # train_accuracy(accuracy_function(tar, predictions))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = WarpSampler(user_train, usernum, itemnum, batch_size=batch_size, maxlen=maxlen, n_workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                          | 0/493 [00:00<?, ?b/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda/envs/py37_default/lib/python3.7/site-packages/tensorflow/python/ops/linalg/linear_operator_lower_triangular.py:158: calling LinearOperator.__init__ (from tensorflow.python.ops.linalg.linear_operator) with graph_parents is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Do not pass `graph_parents`.  They will  no longer be used.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                 | 2/493 [00:00<00:30, 16.04b/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 2, time: 64.906325340271, valid (NDCG@10: 0.3211998443237736, HR@10: 0.5113)\n",
      "epoch: 2, time: 64.906325340271,  test (NDCG@10: 0.2711126682237444, HR@10: 0.4452)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                 | 2/493 [00:00<00:29, 16.70b/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 4, time: 126.3671600818634, valid (NDCG@10: 0.3402122173401346, HR@10: 0.5383)\n",
      "epoch: 4, time: 126.3671600818634,  test (NDCG@10: 0.2973869443338476, HR@10: 0.4787)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 5, test (NDCG@10: 0.3014105123529007, HR@10: 0.4875)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "T = 0.0\n",
    "t0 = time.time()\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "\n",
    "    step_loss = []\n",
    "    train_loss.reset_states()\n",
    "    for step in tqdm(range(num_steps), total=num_steps, ncols=70, leave=False, unit='b'):\n",
    "\n",
    "        u, seq, pos, neg = sampler.next_batch()\n",
    "\n",
    "        inputs, target = create_combined_dataset(u, seq, pos, neg, maxlen)\n",
    "\n",
    "        loss = train_step(inputs, target)\n",
    "        step_loss.append(loss)\n",
    "\n",
    "#     print(f\"Epoch: {epoch}, Loss: {np.mean(step_loss):.3f}, {train_loss.result():.3f}\")\n",
    "        \n",
    "    if epoch % 2 == 0:\n",
    "        t1 = time.time() - t0\n",
    "        T += t1\n",
    "        print('Evaluating...')\n",
    "        t_test = evaluate(model, dataset, maxlen, num_neg_test)\n",
    "        t_valid = evaluate_valid(model, dataset, maxlen, num_neg_test)\n",
    "        print(f\"\\nepoch: {epoch}, time: {T}, valid (NDCG@10: {t_valid[0]}, HR@10: {t_valid[1]})\")\n",
    "        print(f\"epoch: {epoch}, time: {T},  test (NDCG@10: {t_test[0]}, HR@10: {t_test[1]})\")\n",
    "\n",
    "#         f.write(str(t_valid) + ' ' + str(t_test) + '\\n')\n",
    "#         f.flush()\n",
    "        t0 = time.time()\n",
    "\n",
    "t_test = evaluate(model, dataset, maxlen, num_neg_test)\n",
    "print(f\"\\nepoch: {epoch}, test (NDCG@10: {t_test[0]}, HR@10: {t_test[1]})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ndcg@10': 0.3014105123529007, 'Hit@10': 0.4875}\n"
     ]
    }
   ],
   "source": [
    "res_syn = {\"ndcg@10\": t_test[0], \"Hit@10\": t_test[1]}\n",
    "print(res_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/scrapbook.scrap.json+json": {
       "data": 0.3014105123529007,
       "encoder": "json",
       "name": "ndcg",
       "version": 1
      }
     },
     "metadata": {
      "scrapbook": {
       "data": true,
       "display": false,
       "name": "ndcg"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/scrapbook.scrap.json+json": {
       "data": 0.4875,
       "encoder": "json",
       "name": "recall",
       "version": 1
      }
     },
     "metadata": {
      "scrapbook": {
       "data": true,
       "display": false,
       "name": "recall"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Record results with papermill for tests - ignore this cell\n",
    "# sb.glue(\"res_syn\", res_syn)\n",
    "\n",
    "sb.glue(\"ndcg@10\", t_test[0])\n",
    "sb.glue(\"Hit@10\", t_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
